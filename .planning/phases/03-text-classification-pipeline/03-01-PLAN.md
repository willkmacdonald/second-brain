---
phase: 03-text-classification-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/pyproject.toml
  - backend/src/second_brain/config.py
  - backend/src/second_brain/models/documents.py
  - backend/src/second_brain/tools/classification.py
  - backend/src/second_brain/agents/orchestrator.py
  - backend/src/second_brain/agents/classifier.py
  - backend/src/second_brain/agents/workflow.py
  - backend/src/second_brain/main.py
autonomous: true
requirements: [ORCH-01, ORCH-02, ORCH-06, CLAS-01, CLAS-02, CLAS-03, CLAS-07]

must_haves:
  truths:
    - "A text capture sent via AG-UI POST is routed by Orchestrator to Classifier without user interaction"
    - "Classifier calls classify_and_file tool with bucket, confidence, raw_text, and title"
    - "When confidence >= 0.6, both Inbox and target bucket container receive new documents"
    - "When confidence < 0.6, Inbox receives a document marked low_confidence and the best bucket still gets a record"
    - "Inbox document contains full classificationMeta with bucket, confidence, allScores, agentChain, classifiedAt"
    - "Bucket record and Inbox record are bi-directionally linked via filedRecordId and inboxRecordId"
    - "Classifier responds with 'Filed -> {Bucket} ({confidence})' confirmation string"
  artifacts:
    - path: "backend/src/second_brain/models/documents.py"
      provides: "ClassificationMeta Pydantic model, InboxDocument with filedRecordId, status fields"
      contains: "ClassificationMeta"
    - path: "backend/src/second_brain/tools/classification.py"
      provides: "ClassificationTools with classify_and_file tool"
      contains: "classify_and_file"
    - path: "backend/src/second_brain/agents/orchestrator.py"
      provides: "Orchestrator agent creation"
      contains: "create_orchestrator_agent"
    - path: "backend/src/second_brain/agents/classifier.py"
      provides: "Classifier agent creation with few-shot examples"
      contains: "create_classifier_agent"
    - path: "backend/src/second_brain/agents/workflow.py"
      provides: "HandoffBuilder workflow wiring"
      contains: "create_capture_workflow"
    - path: "backend/src/second_brain/main.py"
      provides: "Workflow agent registered at /api/ag-ui replacing echo agent"
      contains: "create_capture_workflow"
  key_links:
    - from: "backend/src/second_brain/main.py"
      to: "backend/src/second_brain/agents/workflow.py"
      via: "create_capture_workflow call in lifespan"
      pattern: "create_capture_workflow"
    - from: "backend/src/second_brain/agents/workflow.py"
      to: "backend/src/second_brain/agents/orchestrator.py"
      via: "HandoffBuilder with_start_agent"
      pattern: "with_start_agent.*orchestrator"
    - from: "backend/src/second_brain/agents/workflow.py"
      to: "backend/src/second_brain/agents/classifier.py"
      via: "HandoffBuilder add_handoff"
      pattern: "add_handoff.*classifier"
    - from: "backend/src/second_brain/agents/classifier.py"
      to: "backend/src/second_brain/tools/classification.py"
      via: "classify_and_file tool binding"
      pattern: "classification_tools.classify_and_file"
    - from: "backend/src/second_brain/tools/classification.py"
      to: "backend/src/second_brain/db/cosmos.py"
      via: "CosmosManager container writes"
      pattern: "self._manager.get_container"
---

<objective>
Build the complete backend classification pipeline: Orchestrator and Classifier agents wired via HandoffBuilder, with a classify_and_file tool that classifies text into one of four buckets (People, Projects, Ideas, Admin), files to Cosmos DB, and logs to Inbox with full trace metadata.

Purpose: Replace the Phase 1 echo agent with a real multi-agent pipeline that automatically classifies and files captured text -- the core intelligence of the Second Brain.
Output: Working AG-UI endpoint at POST /api/ag-ui that routes text through Orchestrator -> Classifier -> Cosmos DB and returns a confirmation.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-text-classification-pipeline/03-CONTEXT.md
@.planning/phases/03-text-classification-pipeline/03-RESEARCH.md
@.planning/phases/01-backend-foundation/01-02-SUMMARY.md
@backend/src/second_brain/main.py
@backend/src/second_brain/agents/echo.py
@backend/src/second_brain/models/documents.py
@backend/src/second_brain/tools/cosmos_crud.py
@backend/src/second_brain/config.py
@backend/src/second_brain/db/cosmos.py
@backend/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document model updates, classification tool, and config</name>
  <files>
    backend/src/second_brain/models/documents.py
    backend/src/second_brain/tools/classification.py
    backend/src/second_brain/config.py
    backend/pyproject.toml
  </files>
  <action>
**1. Update document models** (`backend/src/second_brain/models/documents.py`):

- Add `ClassificationMeta` Pydantic model with fields (all camelCase per Phase 1 convention):
  - `bucket: str` -- "People", "Projects", "Ideas", or "Admin"
  - `confidence: float` -- 0.0-1.0
  - `allScores: dict[str, float]` -- all four bucket scores, e.g. {"People": 0.1, "Projects": 0.85, "Ideas": 0.03, "Admin": 0.02} (per locked CONTEXT.md decision: ALL four classification scores stored)
  - `classifiedBy: str` -- agent name, e.g. "Classifier"
  - `agentChain: list[str]` -- e.g. ["Orchestrator", "Classifier"]
  - `classifiedAt: datetime` -- ISO timestamp
- Change `BaseDocument.classificationMeta` type from `dict | None` to `ClassificationMeta | None`
- Add to `InboxDocument`:
  - `filedRecordId: str | None = None` -- ID of the record filed in the target bucket container (bi-directional link per CONTEXT.md)
  - `status: str = "classified"` -- "classified", "low_confidence", or "unclassified"
  - `title: str | None = None` -- extracted title from the capture
- Add to each bucket document model (PeopleDocument, ProjectsDocument, IdeasDocument, AdminDocument):
  - `inboxRecordId: str | None = None` -- ID of the corresponding Inbox record (bi-directional link per CONTEXT.md)
- Keep the N815 ruff ignore for this file (camelCase fields)

**2. Create classification tool** (`backend/src/second_brain/tools/classification.py`):

- Create `ClassificationTools` class following the same pattern as `CosmosCrudTools` (class-based tool binding to CosmosManager)
- Constructor takes `CosmosManager` and `classification_threshold: float` (default 0.6)
- Implement `classify_and_file` as `@tool` method with parameters:
  - `bucket: Annotated[str, "Classification bucket: People, Projects, Ideas, or Admin"]` -- use str not Literal (Agent Framework tools need simple types for JSON schema generation)
  - `confidence: Annotated[float, "Confidence score 0.0-1.0 for the primary bucket"]`
  - `people_score: Annotated[float, "Score 0.0-1.0 for People bucket"]`
  - `projects_score: Annotated[float, "Score 0.0-1.0 for Projects bucket"]`
  - `ideas_score: Annotated[float, "Score 0.0-1.0 for Ideas bucket"]`
  - `admin_score: Annotated[float, "Score 0.0-1.0 for Admin bucket"]`
  - `raw_text: Annotated[str, "The original captured text"]`
  - `title: Annotated[str, "Brief title extracted from the text (3-6 words)"]`
- Tool logic:
  - Validate bucket is one of "People", "Projects", "Ideas", "Admin". If not, return error string.
  - Clamp confidence to 0.0-1.0
  - Build `allScores` dict from the four score parameters
  - Build `ClassificationMeta` with all fields including `classifiedAt=datetime.now(UTC)`
  - Generate IDs for both inbox and bucket records upfront (use `str(uuid4())`) so bi-directional links can be set
  - Create `InboxDocument` with: rawText, classificationMeta, source="text", title, filedRecordId=bucket_doc_id, status="classified" if confidence >= threshold else "low_confidence"
  - Write Inbox document to Cosmos DB
  - Create bucket document with: rawText, classificationMeta, title/name, inboxRecordId=inbox_doc_id
  - Write bucket document to Cosmos DB
  - For confidence < threshold: still file to best bucket (per CONTEXT.md decision "Below 0.6: still file to best bucket, but mark Inbox record as low_confidence")
  - Return confirmation string: `"Filed → {bucket} ({confidence:.2f})"` (use actual arrow character → per CONTEXT.md)
  - Log the classification at INFO level
- Also implement `mark_as_junk` as `@tool` method:
  - Parameters: `raw_text: Annotated[str, "The original captured text"]`
  - Creates InboxDocument with rawText, status="unclassified", source="text", no classificationMeta
  - Returns "Capture logged as unclassified"
  - This handles junk/nonsense input per CONTEXT.md: "filed to Inbox only with status unclassified"

**3. Update config** (`backend/src/second_brain/config.py`):

- Add `classification_threshold: float = 0.6` to Settings class (per CONTEXT.md: "Configurable via environment variable: CLASSIFICATION_THRESHOLD=0.6")

**4. Update pyproject.toml** (`backend/pyproject.toml`):

- Add `"agent-framework-orchestrations"` to the dependencies list (required for HandoffBuilder)
- Add N815 ruff per-file ignore for `"src/second_brain/tools/classification.py"` (camelCase field access in ClassificationMeta)
  </action>
  <verify>
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && uv pip install -e ".[dev,test]" --prerelease=allow` succeeds (agent-framework-orchestrations installs)
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff check src/second_brain/models/documents.py src/second_brain/tools/classification.py src/second_brain/config.py` passes with no errors
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.models.documents import ClassificationMeta, InboxDocument; print('Models OK')"` succeeds
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.tools.classification import ClassificationTools; print('Tools OK')"` succeeds
  </verify>
  <done>
- ClassificationMeta Pydantic model exists with bucket, confidence, allScores, classifiedBy, agentChain, classifiedAt
- InboxDocument has filedRecordId, status, and title fields
- All bucket documents have inboxRecordId field
- ClassificationTools.classify_and_file tool validates inputs, writes to Inbox + target bucket, returns "Filed → {bucket} ({confidence})"
- ClassificationTools.mark_as_junk tool writes minimal Inbox record with status "unclassified"
- CLASSIFICATION_THRESHOLD setting available in config
- agent-framework-orchestrations installed
  </done>
</task>

<task type="auto">
  <name>Task 2: Orchestrator, Classifier, workflow agents, and main.py wiring</name>
  <files>
    backend/src/second_brain/agents/orchestrator.py
    backend/src/second_brain/agents/classifier.py
    backend/src/second_brain/agents/workflow.py
    backend/src/second_brain/main.py
  </files>
  <action>
**1. Create Orchestrator agent** (`backend/src/second_brain/agents/orchestrator.py`):

- Create `create_orchestrator_agent(chat_client: AzureOpenAIChatClient) -> Agent` function
- Instructions: Route all input to the Classifier. NEVER respond directly. ALWAYS hand off. Minimal -- the Orchestrator adds no value-add text.
- Description: "Routes user input to the appropriate specialist agent"
- No tools -- the HandoffBuilder auto-generates handoff tools
- Follow the code example from 03-RESEARCH.md Pattern 1

**2. Create Classifier agent** (`backend/src/second_brain/agents/classifier.py`):

- Create `create_classifier_agent(chat_client: AzureOpenAIChatClient, classification_tools: ClassificationTools) -> Agent` function
- Instructions MUST include (per locked CONTEXT.md decisions):
  - Four bucket definitions matching CONTEXT.md exactly:
    - People: Relationships, interactions, social context
    - Projects: Multi-step endeavors with a goal
    - Ideas: Thoughts to revisit later, reflections, emotional processing, no immediate action
    - Admin: One-off tasks, errands, logistics, time-sensitive items
  - Multi-bucket rule: "primary intent wins" -- determine if capture is ABOUT the person or ABOUT the project
  - 5-10 few-shot examples covering each bucket and edge cases (Claude's discretion on exact examples)
  - Confidence calibration guidance: 0.8-1.0 for clear, 0.6-0.79 for ambiguous, below 0.6 for multi-bucket equally
  - Title extraction: "Extract a brief title (3-6 words) from the text"
  - Junk detection: "If the input is gibberish, accidental, or nonsensical, call mark_as_junk instead"
  - After filing: respond with ONLY the confirmation returned by the tool
  - MUST provide all four bucket scores in the tool call (not just primary confidence)
- Tools: `[classification_tools.classify_and_file, classification_tools.mark_as_junk]`
- Description: "Classifies text into People/Projects/Ideas/Admin and files to Cosmos DB"

**3. Create workflow** (`backend/src/second_brain/agents/workflow.py`):

- Create `create_capture_workflow(orchestrator: Agent, classifier: Agent) -> WorkflowAgent` function
- Use `HandoffBuilder` from `agent_framework.orchestrations`:
  - name="capture_pipeline"
  - participants=[orchestrator, classifier]
  - with_start_agent(orchestrator)
  - add_handoff(orchestrator, [classifier])
  - with_autonomous_mode(agents=[orchestrator], prompts={orchestrator.name: "Route this input to the Classifier."})
  - .build()
- Call `workflow.as_agent(name="SecondBrainPipeline")` -- CRITICAL: AG-UI endpoint requires an agent, not a Workflow
- Return the WorkflowAgent
- Do NOT make the Classifier autonomous -- it needs to be interactive for Phase 4 HITL

**4. Update main.py** (`backend/src/second_brain/main.py`):

- Replace the echo agent with the capture workflow:
  - Remove `from second_brain.agents.echo import create_echo_agent` import
  - Add imports for orchestrator, classifier, workflow, and ClassificationTools
  - In the lifespan, after cosmos_manager initialization:
    - Create a SHARED `AzureOpenAIChatClient` (use sync `DefaultAzureCredential` from `azure.identity`, NOT `azure.identity.aio` -- per Phase 1 decision that AzureOpenAIChatClient expects TokenCredential not AsyncTokenCredential)
    - Create `ClassificationTools(cosmos_manager, classification_threshold=settings.classification_threshold)` -- pass None-safe (if cosmos_manager is None, tools will fail gracefully on use)
    - Create orchestrator via `create_orchestrator_agent(chat_client)`
    - Create classifier via `create_classifier_agent(chat_client, classification_tools)`
    - Create workflow agent via `create_capture_workflow(orchestrator, classifier)`
    - Register with `add_agent_framework_fastapi_endpoint(app, workflow_agent, "/api/ag-ui")`
  - Keep the echo.py file for reference (do not delete)
  - The shared chat_client replaces the per-agent client pattern from echo.py -- this follows the anti-pattern guidance from research ("Creating a new AzureOpenAIChatClient per agent" is listed as an anti-pattern)
  - Handle the case where cosmos_manager is None: create ClassificationTools with None cosmos_manager. Tools will error at runtime if called without Cosmos, but server starts.
  </action>
  <verify>
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff check src/second_brain/agents/ src/second_brain/main.py` passes with no errors
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff format --check src/second_brain/agents/ src/second_brain/main.py` passes
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.agents.orchestrator import create_orchestrator_agent; print('Orchestrator OK')"` succeeds
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.agents.classifier import create_classifier_agent; print('Classifier OK')"` succeeds
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.agents.workflow import create_capture_workflow; print('Workflow OK')"` succeeds
- `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.main import app; print('Main OK')"` succeeds (no import errors)
- Existing tests still pass: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -x -q` (some may need mock updates for new imports -- fix if broken)
  </verify>
  <done>
- Orchestrator agent created with routing-only instructions, no tools
- Classifier agent created with bucket definitions, few-shot examples, confidence calibration, junk detection, and classify_and_file + mark_as_junk tools
- HandoffBuilder workflow wires Orchestrator -> Classifier with autonomous Orchestrator
- main.py lifespan creates shared chat_client, ClassificationTools, both agents, workflow, and registers at /api/ag-ui
- Echo agent kept for reference but no longer used
- Server imports cleanly with no errors
  </done>
</task>

</tasks>

<verification>
1. All 8 files exist and pass ruff linting: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff check src/second_brain/ && ruff format --check src/second_brain/`
2. All imports resolve: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.main import app; print('All imports OK')"`
3. agent-framework-orchestrations is installed: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from agent_framework.orchestrations import HandoffBuilder; print('HandoffBuilder available')"`
4. ClassificationMeta model validates correctly: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.models.documents import ClassificationMeta; m = ClassificationMeta(bucket='Projects', confidence=0.85, allScores={'People':0.05,'Projects':0.85,'Ideas':0.05,'Admin':0.05}, classifiedBy='Classifier', agentChain=['Orchestrator','Classifier']); print(m.model_dump_json())"`
</verification>

<success_criteria>
- The backend pipeline is fully wired: AG-UI POST -> Orchestrator -> Classifier -> classify_and_file tool -> Cosmos DB (Inbox + bucket)
- Inbox documents contain full ClassificationMeta with all four scores, agent chain, and timestamps
- Bi-directional linking between Inbox and bucket records via filedRecordId/inboxRecordId
- Junk input handled via mark_as_junk tool (Inbox only, status "unclassified")
- Low-confidence captures still filed but marked "low_confidence" in Inbox
- CLASSIFICATION_THRESHOLD configurable via environment variable
- Server starts cleanly with no import errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-text-classification-pipeline/03-01-SUMMARY.md`
</output>
