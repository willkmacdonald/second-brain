---
phase: 05-voice-capture
plan: 03
type: execute
wave: 3
depends_on:
  - 05-02
files_modified:
  - mobile/app.json
  - mobile/app/capture/voice.tsx
  - mobile/lib/ag-ui-client.ts
  - mobile/lib/types.ts
  - mobile/app/(tabs)/index.tsx
autonomous: false
requirements:
  - CAPT-03
  - CAPT-04

must_haves:
  truths:
    - "User can tap Voice button on main screen and navigate to voice capture screen"
    - "User can tap to start recording and tap again to stop (toggle pattern)"
    - "Recording shows elapsed timer and pulsing red indicator"
    - "On stop, audio is uploaded to backend and user sees step progression (Perception -> Orchestrator -> Classifier -> Filed)"
    - "User sees classification result (e.g., 'Filed -> Projects (0.85)') after voice capture"
    - "Short recordings (< 1 second) are discarded silently"
    - "Mic permission denied shows toast notification"
    - "After filing, stays in voice mode (ready to record again)"
    - "Voice captures go through same HITL flow as text (misunderstood, pending, etc.)"
  artifacts:
    - path: "mobile/app/capture/voice.tsx"
      provides: "Voice recording screen with timer, recording indicator, upload, and SSE streaming"
      min_lines: 100
    - path: "mobile/lib/ag-ui-client.ts"
      provides: "sendVoiceCapture function for multipart audio upload + SSE"
      contains: "sendVoiceCapture"
    - path: "mobile/lib/types.ts"
      provides: "SendVoiceCaptureOptions type"
      contains: "SendVoiceCaptureOptions"
    - path: "mobile/app.json"
      provides: "expo-audio plugin with microphone permission"
      contains: "expo-audio"
    - path: "mobile/app/(tabs)/index.tsx"
      provides: "Voice button navigates to /capture/voice"
      contains: "/capture/voice"
  key_links:
    - from: "mobile/app/capture/voice.tsx"
      to: "mobile/lib/ag-ui-client.ts"
      via: "sendVoiceCapture() call after recording stops"
      pattern: "sendVoiceCapture"
    - from: "mobile/lib/ag-ui-client.ts"
      to: "POST /api/voice-capture"
      via: "fetch() multipart upload then EventSource for SSE"
      pattern: "voice-capture"
    - from: "mobile/app/(tabs)/index.tsx"
      to: "mobile/app/capture/voice.tsx"
      via: "router.push('/capture/voice')"
      pattern: "capture/voice"
---

<objective>
Build the mobile voice recording screen with expo-audio, multipart upload to the backend, and real-time SSE streaming showing the full agent chain (Perception -> Orchestrator -> Classifier).

Purpose: This is the user-facing voice capture experience. User taps to record, taps to stop, audio is sent to the backend, and the classification result appears with the same step-dot UX as text capture.

Output: Working voice capture screen, sendVoiceCapture client function, enabled Voice button on main screen.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-voice-capture/05-RESEARCH.md
@.planning/phases/05-voice-capture/05-CONTEXT.md
@.planning/phases/05-voice-capture/05-01-SUMMARY.md
@.planning/phases/05-voice-capture/05-02-SUMMARY.md
@mobile/app/capture/text.tsx
@mobile/lib/ag-ui-client.ts
@mobile/lib/types.ts
@mobile/app/(tabs)/index.tsx
@mobile/app.json
@mobile/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install expo-audio, add plugin config, create sendVoiceCapture client, and voice capture screen</name>
  <files>
    mobile/app.json
    mobile/lib/types.ts
    mobile/lib/ag-ui-client.ts
    mobile/app/capture/voice.tsx
  </files>
  <action>
**Install expo-audio:**
```bash
cd mobile && npx expo install expo-audio
```

**app.json** -- Add expo-audio plugin for microphone permissions:
Add to the `plugins` array:
```json
["expo-audio", { "microphonePermission": "Allow Second Brain to access your microphone for voice capture." }]
```

**lib/types.ts** -- Add SendVoiceCaptureOptions type:
```typescript
export interface SendVoiceCaptureOptions {
  audioUri: string;
  apiKey: string;
  callbacks: StreamingCallbacks;
}
```

**lib/ag-ui-client.ts** -- Add sendVoiceCapture function:

The voice capture flow is different from text: it needs a multipart file upload FIRST, then reads the SSE stream from the SAME response. However, the backend returns a StreamingResponse from a POST with multipart data. The react-native-sse EventSource library sends the request and reads the SSE stream.

BUT -- EventSource sends JSON body, not multipart. For voice capture, we need a TWO-PHASE approach:
1. Upload audio via `fetch()` with FormData (multipart) to `/api/voice-capture`
2. The response IS the SSE stream (StreamingResponse from FastAPI)

The challenge: react-native-sse EventSource doesn't support multipart uploads. So we need to handle SSE parsing manually from a fetch response, OR use a different approach.

SOLUTION: Use a two-step approach within a single function:
- Step 1: POST multipart to `/api/voice-capture` using `fetch()` -- this returns a streaming response
- Step 2: Read the response body as a stream and parse SSE events manually

Actually, the simplest approach: since FastAPI returns `StreamingResponse` with `text/event-stream` content type from the multipart POST, we can use the Fetch API with a ReadableStream reader to parse SSE events.

Implement `sendVoiceCapture` as:
```typescript
export function sendVoiceCapture({
  audioUri,
  apiKey,
  callbacks,
}: SendVoiceCaptureOptions): () => void {
  let aborted = false;
  const abortController = new AbortController();

  (async () => {
    try {
      // Build multipart form data
      const formData = new FormData();
      formData.append('file', {
        uri: audioUri,
        type: 'audio/m4a',
        name: 'voice-capture.m4a',
      } as any);

      // POST multipart -- response is SSE stream
      const response = await fetch(`${API_BASE_URL}/api/voice-capture`, {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${apiKey}`,
          // Do NOT set Content-Type -- fetch sets it with multipart boundary
        },
        body: formData,
        signal: abortController.signal,
      });

      if (!response.ok) {
        callbacks.onError(`Upload failed: ${response.status}`);
        return;
      }

      // Read SSE stream from response body
      const reader = response.body?.getReader();
      if (!reader) {
        callbacks.onError('No response stream');
        return;
      }

      const decoder = new TextDecoder();
      let buffer = '';
      let result = '';
      let hitlTriggered = false;

      while (!aborted) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });

        // Parse SSE events from buffer (events separated by \n\n)
        const events = buffer.split('\n\n');
        buffer = events.pop() || ''; // Keep incomplete event in buffer

        for (const eventStr of events) {
          if (!eventStr.trim()) continue;
          // Extract data line from SSE format: "data: {...}\n"
          const dataMatch = eventStr.match(/^data:\s*(.+)$/m);
          if (!dataMatch) continue;

          try {
            const parsed = JSON.parse(dataMatch[1]);
            // Same event handling as attachCallbacks
            switch (parsed.type) {
              case 'STEP_STARTED':
                callbacks.onStepStart?.(parsed.stepName ?? 'Unknown');
                break;
              case 'STEP_FINISHED':
                callbacks.onStepFinish?.(parsed.stepName ?? 'Unknown');
                break;
              case 'TEXT_MESSAGE_CONTENT':
                if (parsed.delta) {
                  result += parsed.delta;
                  callbacks.onTextDelta?.(parsed.delta);
                }
                break;
              case 'CUSTOM':
                if (parsed.name === 'MISUNDERSTOOD' && parsed.value?.inboxItemId) {
                  hitlTriggered = true;
                  callbacks.onMisunderstood?.(
                    parsed.value.threadId ?? '',
                    parsed.value.questionText ?? '',
                    parsed.value.inboxItemId,
                  );
                }
                if (parsed.name === 'UNRESOLVED' && parsed.value?.inboxItemId) {
                  callbacks.onUnresolved?.(parsed.value.inboxItemId);
                }
                break;
              case 'RUN_FINISHED':
                if (!hitlTriggered) {
                  callbacks.onComplete(result);
                }
                break;
              case 'RUN_ERROR':
                callbacks.onError('Run failed');
                break;
            }
          } catch {
            // Ignore malformed JSON
          }
        }
      }
    } catch (err: any) {
      if (!aborted) {
        callbacks.onError(err?.message || 'Voice capture failed');
      }
    }
  })();

  return () => {
    aborted = true;
    abortController.abort();
  };
}
```

Note: This uses fetch + ReadableStream for SSE because react-native-sse EventSource doesn't support multipart uploads. The SSE parsing logic mirrors the attachCallbacks logic for consistency. HITL_REQUIRED is NOT handled here because voice captures go through the same workflow as text (which uses the existing low-confidence/misunderstood/junk detection) -- per CONTEXT.md: "Voice captures go through the same HITL flow as text".

Actually, wait -- voice captures DO go through the same HITL flow. We need to handle HITL_REQUIRED too. Add it to the switch:
```typescript
case 'CUSTOM':
  if (parsed.name === 'HITL_REQUIRED' && parsed.value?.threadId) {
    hitlTriggered = true;
    callbacks.onHITLRequired?.(
      parsed.value.threadId,
      parsed.value.questionText || result,
      parsed.value.inboxItemId,
    );
  }
  // ... rest of MISUNDERSTOOD/UNRESOLVED handling
```

**app/capture/voice.tsx** -- Create the voice capture screen:

Model after text.tsx but with recording UI instead of text input.

Key imports:
```typescript
import { useAudioRecorder, AudioModule, RecordingPresets, useAudioRecorderState } from 'expo-audio';
```

Note: `setAudioModeAsync` may need to be imported from expo-audio. Check the expo-audio API -- in SDK 54, the function is `AudioModule.setAudioModeAsync()` (on the AudioModule namespace, not a standalone export). Verify during implementation.

State:
- Same as text.tsx: toast, currentStep, completedSteps, streamedText, showSteps, hitlQuestion, hitlThreadId, hitlInboxItemId, hitlTopBuckets, isResolving, followUpRound, agentQuestion, misunderstoodInboxItemId, isReclassifying
- New: `isRecording` (derived from recorderState.isRecording), `permissionGranted` (boolean)
- The `thought` state from text.tsx is replaced by the recording flow

AGENT_STEPS for voice: `["Perception", "Orchestrator", "Classifier"]` (includes Perception as the first step)

Recording flow:
1. On mount: request mic permission via `AudioModule.requestRecordingPermissionsAsync()`. If denied, show toast "Mic permission required" and disable record button.
2. Set audio mode: `AudioModule.setAudioModeAsync({ allowsRecording: true, playsInSilentMode: true })`
3. Tap record button: `await audioRecorder.prepareToRecordAsync(); audioRecorder.record();`
4. While recording: show elapsed timer (from `recorderState.durationMillis`) and pulsing red indicator
5. Tap stop: `await audioRecorder.stop();`
6. Check duration: if `recorderState.durationMillis < 1000`, discard silently (reset state, no upload). Per CONTEXT.md: "Very short recordings (< 1 second): discard silently, treat as accidental tap"
7. Get URI: `audioRecorder.uri`
8. Call `sendVoiceCapture({ audioUri: uri, apiKey: API_KEY, callbacks: {...} })`
9. Show step dots (Perception -> Orchestrator -> Classifier)
10. On complete: show classification result toast, stay in voice mode (per CONTEXT.md: "After filing, stay in voice mode")

UI layout (per CONTEXT.md decisions):
- No text input -- recording mode is the primary UI
- Center: large circular record button (red when recording, gray when idle)
- Above button: elapsed timer (MM:SS format) -- visible only during recording
- Pulsing red ring animation around record button during recording (use Animated API with loop)
- Below: feedback area with step dots and streamed text (same as text.tsx)
- Header: title "Voice" with toast display (same pattern as text.tsx)

HITL handling: Same as text.tsx for misunderstood flow. If agent returns MISUNDERSTOOD, show the agentQuestion bubble and a TextInput for follow-up text reply (reuse the same pattern). If HITL_REQUIRED (low confidence), show bucket buttons. Voice captures are indistinguishable from text in the HITL flow.

Per CONTEXT.md: "Inbox shows transcribed text as item content -- voice and text captures look identical in the inbox list (no voice badge)". This is handled by the backend (transcribed text becomes rawText in the inbox document) -- no mobile changes needed for inbox display.

Per CONTEXT.md: "Transcription failure (Whisper error, network): toast error + stay in voice mode for immediate retry". On error callback, show error toast and reset to recording-ready state (don't navigate away).

Pulsing animation: Use `Animated.loop(Animated.sequence([Animated.timing(opacity, {toValue: 0.3, duration: 800}), Animated.timing(opacity, {toValue: 1, duration: 800})]))` for the recording indicator. Start on record, stop on stop.

Timer formatting: `const formatDuration = (ms: number) => { const s = Math.floor(ms / 1000); const m = Math.floor(s / 60); return \`\${m}:\${(s % 60).toString().padStart(2, '0')}\`; }`

Styling: Follow the dark theme pattern from text.tsx (#0f0f23 background, #1a1a2e surfaces, #ffffff text). Record button: large circle (80px diameter), #ff3b30 when recording, #4a4a6e when idle.
  </action>
  <verify>
- `cd mobile && npx expo install expo-audio` installs successfully
- `mobile/app.json` includes expo-audio plugin with microphonePermission
- `mobile/lib/types.ts` includes SendVoiceCaptureOptions interface
- `mobile/lib/ag-ui-client.ts` exports sendVoiceCapture function
- `mobile/app/capture/voice.tsx` exists with recording UI
- No TypeScript errors: `cd mobile && npx tsc --noEmit` (or verify via IDE)
  </verify>
  <done>
expo-audio installed and configured. sendVoiceCapture function uploads audio as multipart and reads SSE response stream. Voice capture screen has recording toggle, timer, pulsing indicator, step dots, and classification result display. HITL flows (misunderstood, low-confidence) work identically to text.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enable Voice button on main capture screen</name>
  <files>
    mobile/app/(tabs)/index.tsx
  </files>
  <action>
**app/(tabs)/index.tsx** -- Enable the Voice button:

1. Change the Voice CaptureButton from `disabled` with `showComingSoon` to enabled with navigation:
   - Remove `disabled` prop
   - Change `onPress` from `showComingSoon` to `() => router.push("/capture/voice")`

The button is already in the correct position (first button in the stack). Just enable it and wire the navigation.

Before:
```tsx
<CaptureButton
  label="Voice"
  icon={"ðŸŽ™ï¸"}
  onPress={showComingSoon}
  disabled
/>
```

After:
```tsx
<CaptureButton
  label="Voice"
  icon={"ðŸŽ™ï¸"}
  onPress={() => router.push("/capture/voice")}
/>
```

This is the minimal change needed. The `router` import from expo-router is already present.
  </action>
  <verify>
- Voice button in `mobile/app/(tabs)/index.tsx` no longer has `disabled` prop
- `grep -q "capture/voice" mobile/app/\(tabs\)/index.tsx` confirms navigation target
  </verify>
  <done>
Voice button on main capture screen is enabled and navigates to /capture/voice.
  </done>
</task>

</tasks>

<verification>
1. expo-audio installed and plugin configured in app.json
2. Voice button navigates to voice capture screen
3. Voice capture screen shows recording UI with timer and pulsing indicator
4. Tapping record starts recording, tapping again stops and uploads
5. Step dots show Perception -> Orchestrator -> Classifier progression
6. Classification result appears as toast
7. Short recordings (< 1 second) are discarded silently
8. Error states show toast and stay in voice mode for retry
9. After filing, screen stays in voice mode (ready to record again)
</verification>

<success_criteria>
- User can tap Voice, record a thought, and see it classified
- Recording shows timer and visual feedback
- SSE stream shows 3-step agent chain (Perception, Orchestrator, Classifier)
- Classification result displayed (e.g., "Filed -> Projects (0.85)")
- Same HITL flows as text (misunderstood, pending) work for voice
- Short recordings discarded, errors handled with retry UX
- Voice button on main screen is enabled and navigates correctly
</success_criteria>

<output>
After completion, create `.planning/phases/05-voice-capture/05-03-SUMMARY.md`
</output>
