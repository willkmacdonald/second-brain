---
phase: 05-voice-capture
plan: 03
type: execute
wave: 3
depends_on:
  - 05-02
files_modified:
  - mobile/app.json
  - mobile/lib/ag-ui-client.ts
  - mobile/lib/types.ts
  - mobile/app/(tabs)/index.tsx
autonomous: false
requirements:
  - CAPT-03
  - CAPT-04

must_haves:
  truths:
    - "User can tap Voice button on main capture screen and the screen switches to recording mode in-place (no navigation)"
    - "Text input area is hidden during recording mode, replaced by timer and pulsing indicator"
    - "User can tap to start recording and tap again to stop (toggle pattern)"
    - "Recording shows elapsed timer and pulsing red indicator"
    - "On stop, audio is uploaded to backend and user sees step progression (Perception -> Orchestrator -> Classifier -> Filed)"
    - "User sees classification result (e.g., 'Filed -> Projects (0.85)') after voice capture"
    - "Short recordings (< 1 second) are discarded silently"
    - "Mic permission denied shows toast notification"
    - "After filing, stays in voice mode (ready to record again, not reset to text)"
    - "Voice captures go through same HITL flow as text (misunderstood, pending, etc.)"
  artifacts:
    - path: "mobile/app/(tabs)/index.tsx"
      provides: "Capture screen with mode: 'text' | 'voice' state, recording UI replaces text input in-place"
      min_lines: 50
    - path: "mobile/lib/ag-ui-client.ts"
      provides: "sendVoiceCapture function for multipart audio upload + SSE"
      contains: "sendVoiceCapture"
    - path: "mobile/lib/types.ts"
      provides: "SendVoiceCaptureOptions type"
      contains: "SendVoiceCaptureOptions"
    - path: "mobile/app.json"
      provides: "expo-audio plugin with microphone permission"
      contains: "expo-audio"
  key_links:
    - from: "mobile/app/(tabs)/index.tsx"
      to: "mobile/lib/ag-ui-client.ts"
      via: "sendVoiceCapture() call after recording stops"
      pattern: "sendVoiceCapture"
    - from: "mobile/lib/ag-ui-client.ts"
      to: "POST /api/voice-capture"
      via: "fetch() multipart upload then ReadableStream for SSE"
      pattern: "voice-capture"
    - from: "mobile/app/(tabs)/index.tsx"
      to: "expo-audio"
      via: "useAudioRecorder hook for recording lifecycle"
      pattern: "useAudioRecorder"
---

<objective>
Build the mobile voice recording experience with expo-audio, multipart upload to the backend, and real-time SSE streaming showing the full agent chain (Perception -> Orchestrator -> Classifier).

Purpose: This is the user-facing voice capture experience. Voice recording happens in-place on the main capture screen (per CONTEXT.md: "Voice button switches to recording mode in-place"). The text input is hidden during recording, replaced with timer and pulsing indicator. After classification, the screen stays in voice mode for rapid-fire captures.

Output: In-place voice recording mode on the capture screen, sendVoiceCapture client function, expo-audio configuration.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-voice-capture/05-RESEARCH.md
@.planning/phases/05-voice-capture/05-CONTEXT.md
@.planning/phases/05-voice-capture/05-01-SUMMARY.md
@.planning/phases/05-voice-capture/05-02-SUMMARY.md
@mobile/app/capture/text.tsx
@mobile/lib/ag-ui-client.ts
@mobile/lib/types.ts
@mobile/app/(tabs)/index.tsx
@mobile/app.json
@mobile/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install expo-audio, add plugin config, create sendVoiceCapture client, and add voice recording mode to main capture screen</name>
  <files>
    mobile/app.json
    mobile/lib/types.ts
    mobile/lib/ag-ui-client.ts
    mobile/app/(tabs)/index.tsx
  </files>
  <action>
**Install expo-audio:**
```bash
cd mobile && npx expo install expo-audio
```

**app.json** -- Add expo-audio plugin for microphone permissions:
Add to the `plugins` array:
```json
["expo-audio", { "microphonePermission": "Allow Second Brain to access your microphone for voice capture." }]
```

**lib/types.ts** -- Add SendVoiceCaptureOptions type:
```typescript
export interface SendVoiceCaptureOptions {
  audioUri: string;
  apiKey: string;
  callbacks: StreamingCallbacks;
}
```

**lib/ag-ui-client.ts** -- Add sendVoiceCapture function:

The voice capture flow needs a multipart file upload, then reads the SSE stream from the SAME response. Since react-native-sse EventSource doesn't support multipart uploads, use fetch + ReadableStream for SSE parsing.

Implement `sendVoiceCapture` as:
```typescript
export function sendVoiceCapture({
  audioUri,
  apiKey,
  callbacks,
}: SendVoiceCaptureOptions): () => void {
  let aborted = false;
  const abortController = new AbortController();

  (async () => {
    try {
      // Build multipart form data
      const formData = new FormData();
      formData.append('file', {
        uri: audioUri,
        type: 'audio/m4a',
        name: 'voice-capture.m4a',
      } as any);

      // POST multipart -- response is SSE stream
      const response = await fetch(`${API_BASE_URL}/api/voice-capture`, {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${apiKey}`,
          // Do NOT set Content-Type -- fetch sets it with multipart boundary
        },
        body: formData,
        signal: abortController.signal,
      });

      if (!response.ok) {
        callbacks.onError(`Upload failed: ${response.status}`);
        return;
      }

      // Read SSE stream from response body
      const reader = response.body?.getReader();
      if (!reader) {
        callbacks.onError('No response stream');
        return;
      }

      const decoder = new TextDecoder();
      let buffer = '';
      let result = '';
      let hitlTriggered = false;

      while (!aborted) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });

        // Parse SSE events from buffer (events separated by \n\n)
        const events = buffer.split('\n\n');
        buffer = events.pop() || ''; // Keep incomplete event in buffer

        for (const eventStr of events) {
          if (!eventStr.trim()) continue;
          const dataMatch = eventStr.match(/^data:\s*(.+)$/m);
          if (!dataMatch) continue;

          try {
            const parsed = JSON.parse(dataMatch[1]);
            switch (parsed.type) {
              case 'STEP_STARTED':
                callbacks.onStepStart?.(parsed.stepName ?? 'Unknown');
                break;
              case 'STEP_FINISHED':
                callbacks.onStepFinish?.(parsed.stepName ?? 'Unknown');
                break;
              case 'TEXT_MESSAGE_CONTENT':
                if (parsed.delta) {
                  result += parsed.delta;
                  callbacks.onTextDelta?.(parsed.delta);
                }
                break;
              case 'CUSTOM':
                if (parsed.name === 'HITL_REQUIRED' && parsed.value?.threadId) {
                  hitlTriggered = true;
                  callbacks.onHITLRequired?.(
                    parsed.value.threadId,
                    parsed.value.questionText || result,
                    parsed.value.inboxItemId,
                  );
                }
                if (parsed.name === 'MISUNDERSTOOD' && parsed.value?.inboxItemId) {
                  hitlTriggered = true;
                  callbacks.onMisunderstood?.(
                    parsed.value.threadId ?? '',
                    parsed.value.questionText ?? '',
                    parsed.value.inboxItemId,
                  );
                }
                if (parsed.name === 'UNRESOLVED' && parsed.value?.inboxItemId) {
                  callbacks.onUnresolved?.(parsed.value.inboxItemId);
                }
                break;
              case 'RUN_FINISHED':
                if (!hitlTriggered) {
                  callbacks.onComplete(result);
                }
                break;
              case 'RUN_ERROR':
                callbacks.onError('Run failed');
                break;
            }
          } catch {
            // Ignore malformed JSON
          }
        }
      }
    } catch (err: any) {
      if (!aborted) {
        callbacks.onError(err?.message || 'Voice capture failed');
      }
    }
  })();

  return () => {
    aborted = true;
    abortController.abort();
  };
}
```

Note: Uses fetch + ReadableStream for SSE because react-native-sse EventSource doesn't support multipart uploads. The SSE parsing logic mirrors the attachCallbacks logic for consistency. HITL_REQUIRED, MISUNDERSTOOD, and UNRESOLVED events are all handled because voice captures go through the same HITL flow as text (per CONTEXT.md).

**app/(tabs)/index.tsx** -- Add voice recording mode IN-PLACE (CRITICAL: per CONTEXT.md "Voice button switches to recording mode in-place" and "Hide text input area during recording"):

Do NOT create a separate voice.tsx route. Instead, add a `mode: 'text' | 'voice'` state to the existing main capture screen.

Key changes:

1. **Add state:** `const [mode, setMode] = useState<'text' | 'voice'>('text');`
   Plus voice-specific state: `isRecording`, `permissionGranted`, recording state variables, toast, currentStep, completedSteps, streamedText, showSteps, and all HITL states (same as text.tsx).

2. **Voice button behavior:** Instead of `router.push('/capture/voice')`, the Voice button sets `mode` to `'voice'`:
   ```tsx
   <CaptureButton
     label="Voice"
     icon={"ðŸŽ™ï¸"}
     onPress={() => setMode('voice')}
     active={mode === 'voice'}
   />
   ```
   Remove `disabled` prop. Similarly, Text button sets `mode` to `'text'` and navigates:
   ```tsx
   <CaptureButton
     label="Text"
     icon={"âœï¸"}
     onPress={() => router.push("/capture/text")}
   />
   ```
   The Text button continues to navigate to the text capture screen as before. Only Voice mode is in-place.

3. **Voice mode UI (rendered when mode === 'voice'):**
   Below the button stack (NOT a separate screen), render the voice recording UI:
   - Request mic permission on entering voice mode (useEffect on mode === 'voice')
   - Center: large circular record button (80px diameter, #ff3b30 when recording, #4a4a6e when idle)
   - Above button: elapsed timer (MM:SS format) visible only during recording
   - Pulsing red ring animation around record button during recording (Animated API with loop)
   - Below: feedback area with AgentSteps (Perception, Orchestrator, Classifier), streamed text, HITL UI

4. **Recording flow:**
   - Import from expo-audio: `useAudioRecorder, AudioModule, useAudioRecorderState`
   - On entering voice mode: `AudioModule.requestRecordingPermissionsAsync()`. If denied, show toast "Mic permission required".
   - Set audio mode: `AudioModule.setAudioModeAsync({ allowsRecording: true, playsInSilentMode: true })`
   - Tap record: `await audioRecorder.prepareToRecordAsync(); audioRecorder.record();`
   - While recording: show elapsed timer from `recorderState.durationMillis` and pulsing indicator
   - Tap stop: `await audioRecorder.stop();`
   - Check duration: if `recorderState.durationMillis < 1000`, discard silently (per CONTEXT.md)
   - Get URI: `audioRecorder.uri`
   - Call `sendVoiceCapture({ audioUri: uri, apiKey: API_KEY, callbacks: {...} })`
   - Show step dots (Perception -> Orchestrator -> Classifier)

5. **After filing, stay in voice mode** (per CONTEXT.md: "After filing, stay in voice mode (don't reset to text)"):
   - On complete callback: show classification toast, reset recording state, but do NOT change `mode` back to 'text'
   - User stays ready to record again immediately

6. **HITL handling:** Same as text.tsx. If MISUNDERSTOOD, show agentQuestion bubble + TextInput for follow-up. If HITL_REQUIRED, show bucket buttons. All identical to text flow.

7. **Error handling:**
   - Mic permission denied: toast "Mic permission required"
   - Transcription failure: toast error + stay in voice mode for retry (per CONTEXT.md)
   - Short recordings: discard silently

8. **Imports to add:**
   ```typescript
   import { useState, useEffect, useRef, useCallback } from "react";
   import { Animated, TextInput, Pressable, Text, ScrollView } from "react-native";
   import { useAudioRecorder, AudioModule, useAudioRecorderState } from 'expo-audio';
   import * as Haptics from "expo-haptics";
   import { sendVoiceCapture, sendClarification, sendFollowUp } from "../../lib/ag-ui-client";
   import { API_KEY } from "../../constants/config";
   import { AgentSteps } from "../../components/AgentSteps";
   ```

AGENT_STEPS for voice: `["Perception", "Orchestrator", "Classifier"]` (includes Perception first).

Pulsing animation: Use `Animated.loop(Animated.sequence([Animated.timing(opacity, {toValue: 0.3, duration: 800}), Animated.timing(opacity, {toValue: 1, duration: 800})]))` for the recording indicator.

Timer formatting: `const formatDuration = (ms: number) => { const s = Math.floor(ms / 1000); const m = Math.floor(s / 60); return \`${m}:${(s % 60).toString().padStart(2, '0')}\`; }`

Styling: Follow dark theme from text.tsx (#0f0f23 background, #1a1a2e surfaces, #ffffff text). Record button: large circle (80px), #ff3b30 when recording, #4a4a6e when idle.

NOTE: The CaptureButton component may need an `active` prop to visually indicate which mode is selected (subtle highlight on the Voice button when in voice mode). If it doesn't have one, add a simple active style (e.g., border highlight). This is Claude's discretion on exact styling.
  </action>
  <verify>
- `cd mobile && npx expo install expo-audio` installs successfully
- `mobile/app.json` includes expo-audio plugin with microphonePermission
- `mobile/lib/types.ts` includes SendVoiceCaptureOptions interface
- `mobile/lib/ag-ui-client.ts` exports sendVoiceCapture function
- `mobile/app/(tabs)/index.tsx` has `mode` state with 'text' | 'voice' values
- `grep -q "sendVoiceCapture" mobile/app/\(tabs\)/index.tsx` confirms voice capture wiring
- No `mobile/app/capture/voice.tsx` file created (voice is in-place, NOT a separate route)
- No TypeScript errors: `cd mobile && npx tsc --noEmit` (or verify via IDE)
  </verify>
  <done>
expo-audio installed and configured. sendVoiceCapture function uploads audio as multipart and reads SSE response stream. Voice recording mode added in-place to main capture screen with recording toggle, timer, pulsing indicator, step dots, and classification result display. Text input hidden during voice mode. HITL flows identical to text. No separate voice.tsx route created. After filing, stays in voice mode.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify voice recording mode works in-place on capture screen</name>
  <files>mobile/app/(tabs)/index.tsx</files>
  <action>
**What was built:** Voice recording mode integrated into the main capture screen in-place. Tapping Voice button switches to recording UI (hides text input, shows record button with timer). Tapping record starts recording, tapping again stops and uploads. SSE stream shows Perception -> Orchestrator -> Classifier steps. Classification result appears as toast. Screen stays in voice mode after filing.

**How to verify:**
1. Open the app on device/simulator
2. On the main capture screen, tap the Voice button
3. Verify the screen switches to voice recording mode IN-PLACE (no navigation to a new screen)
4. Verify text input is NOT visible during voice mode
5. Verify a large record button is shown with idle state (gray)
6. Tap the record button -- verify it turns red with pulsing animation and timer appears
7. Speak a test capture (e.g., "Schedule meeting with Sarah about the project")
8. Tap record button again to stop
9. Verify step dots appear: Perception -> Orchestrator -> Classifier
10. Verify classification result toast appears (e.g., "Filed -> People (0.85)")
11. Verify the screen stays in voice mode (ready to record again, not reset to text)
12. Test error case: deny mic permission, verify toast "Mic permission required"
13. Test short recording: tap record, immediately tap stop (< 1 second) -- verify it's discarded silently

**Resume signal:** Type "approved" or describe issues
  </action>
  <verify>User confirms all 13 verification steps pass on device/simulator</verify>
  <done>Voice recording mode works in-place on capture screen with all CONTEXT.md decisions honored: in-place mode switch, hidden text input, toggle record, timer, pulsing indicator, step dots, classification result, stays in voice mode.</done>
</task>

</tasks>

<verification>
1. expo-audio installed and plugin configured in app.json
2. Voice button on main capture screen switches to voice mode in-place (NO navigation)
3. Text input is hidden during voice recording mode
4. Voice recording UI shows timer and pulsing red indicator
5. Tapping record starts recording, tapping again stops and uploads
6. Step dots show Perception -> Orchestrator -> Classifier progression
7. Classification result appears as toast
8. Short recordings (< 1 second) are discarded silently
9. Error states show toast and stay in voice mode for retry
10. After filing, screen stays in voice mode (ready to record again)
</verification>

<success_criteria>
- User can tap Voice on capture screen and switch to recording mode in-place (per CONTEXT.md)
- Text input area is hidden during recording (per CONTEXT.md)
- Recording shows timer and visual feedback
- SSE stream shows 3-step agent chain (Perception, Orchestrator, Classifier)
- Classification result displayed (e.g., "Filed -> Projects (0.85)")
- Same HITL flows as text (misunderstood, pending) work for voice
- Short recordings discarded, errors handled with retry UX
- After filing, stays in voice mode (per CONTEXT.md)
</success_criteria>

<output>
After completion, create `.planning/phases/05-voice-capture/05-03-SUMMARY.md`
</output>
