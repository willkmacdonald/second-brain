---
phase: 05-voice-capture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/second_brain/config.py
  - backend/src/second_brain/db/blob_storage.py
  - backend/src/second_brain/tools/transcription.py
  - backend/src/second_brain/agents/perception.py
  - backend/pyproject.toml
autonomous: true
requirements:
  - INFRA-03
  - ORCH-03

user_setup:
  - service: azure-blob-storage
    why: "Voice recordings stored in Azure Blob Storage"
    env_vars:
      - name: BLOB_STORAGE_URL
        source: "Azure Portal -> Storage account -> Properties -> Primary endpoint (blob)"
    dashboard_config:
      - task: "Create a storage account (or reuse existing)"
        location: "Azure Portal -> Storage accounts -> Create"
      - task: "Create a container named 'voice-recordings'"
        location: "Azure Portal -> Storage account -> Containers -> + Container"
      - task: "Assign 'Storage Blob Data Contributor' role to your Azure AD identity"
        location: "Azure Portal -> Storage account -> Access Control (IAM) -> Add role assignment"
  - service: azure-openai-whisper
    why: "Audio transcription via Whisper model"
    env_vars:
      - name: AZURE_OPENAI_WHISPER_DEPLOYMENT_NAME
        source: "Azure Portal -> Azure OpenAI resource -> Model deployments"
    dashboard_config:
      - task: "Deploy a Whisper model (whisper) in your Azure OpenAI resource"
        location: "Azure Portal -> Azure OpenAI -> Model deployments -> Deploy model -> whisper"

must_haves:
  truths:
    - "BlobStorageManager can upload an audio file and return its URL"
    - "transcribe_audio tool takes audio bytes, calls Whisper, returns transcribed text"
    - "Perception Agent exists as a tool-based agent with transcribe_audio"
    - "Settings include blob_storage_url and azure_openai_whisper_deployment_name"
  artifacts:
    - path: "backend/src/second_brain/db/blob_storage.py"
      provides: "BlobStorageManager for Azure Blob Storage uploads"
      min_lines: 30
    - path: "backend/src/second_brain/tools/transcription.py"
      provides: "transcribe_audio tool wrapping Whisper API"
      min_lines: 20
    - path: "backend/src/second_brain/agents/perception.py"
      provides: "Perception Agent with transcription tool"
      min_lines: 15
    - path: "backend/src/second_brain/config.py"
      provides: "New settings for blob storage and Whisper deployment"
      contains: "blob_storage_url"
  key_links:
    - from: "backend/src/second_brain/tools/transcription.py"
      to: "Azure OpenAI Whisper API"
      via: "openai.AzureOpenAI client.audio.transcriptions.create()"
      pattern: "audio.transcriptions.create"
    - from: "backend/src/second_brain/db/blob_storage.py"
      to: "Azure Blob Storage"
      via: "azure.storage.blob.aio BlobClient"
      pattern: "upload_blob"
    - from: "backend/src/second_brain/agents/perception.py"
      to: "backend/src/second_brain/tools/transcription.py"
      via: "tool binding"
      pattern: "transcribe_audio"
---

<objective>
Create the backend infrastructure for voice capture: Azure Blob Storage manager for audio file storage, Whisper transcription tool, and a Perception Agent that orchestrates transcription.

Purpose: These components are the foundation that the voice-capture endpoint (Plan 02) will wire together. The Perception Agent is the new specialist in the agent chain that converts audio to text.

Output: BlobStorageManager, transcribe_audio tool, Perception Agent, updated Settings.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-voice-capture/05-RESEARCH.md
@.planning/phases/05-voice-capture/05-CONTEXT.md
@backend/src/second_brain/config.py
@backend/src/second_brain/db/cosmos.py
@backend/src/second_brain/tools/classification.py
@backend/src/second_brain/agents/classifier.py
@backend/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BlobStorageManager, transcription tool, and Settings updates</name>
  <files>
    backend/src/second_brain/config.py
    backend/src/second_brain/db/blob_storage.py
    backend/src/second_brain/tools/transcription.py
    backend/pyproject.toml
  </files>
  <action>
**config.py** -- Add two new settings fields:
- `blob_storage_url: str = ""` (e.g., "https://mystorageaccount.blob.core.windows.net")
- `azure_openai_whisper_deployment_name: str = "whisper"`

**pyproject.toml** -- Add dependencies to `[project.dependencies]`:
- `"azure-storage-blob"` (for blob uploads)
- `"python-multipart"` (required by FastAPI for UploadFile)

**db/blob_storage.py** -- Create BlobStorageManager:
- Uses `azure.storage.blob.aio.BlobServiceClient` with `DefaultAzureCredential` (async)
- Container name: `"voice-recordings"` (constant)
- `async def initialize(self)` -- creates BlobServiceClient from account_url + credential
- `async def upload_audio(self, audio_bytes: bytes, filename: str, user_id: str = "will") -> str` -- uploads to `{user_id}/{uuid4()}.m4a`, returns the blob URL
- `async def delete_audio(self, blob_url: str) -> None` -- deletes blob by URL (for cleanup after transcription, per CONTEXT.md decision: "Delete voice recordings after successful transcription")
- `async def close(self)` -- closes the BlobServiceClient
- Follow the same singleton pattern as CosmosManager (init in lifespan, store on app.state)
- Use `from azure.identity.aio import DefaultAzureCredential` (async credential, same as Key Vault in main.py)

**tools/transcription.py** -- Create transcription tool:
- Module-level function `transcribe_audio(audio_bytes: bytes, filename: str, deployment_name: str, endpoint: str) -> str`
- Creates a SYNC `openai.AzureOpenAI` client with `azure_ad_token_provider` from `azure.identity` `DefaultAzureCredential` + `get_bearer_token_provider` (same pattern as chat client but for Whisper)
- Calls `client.audio.transcriptions.create(file=(filename, audio_bytes), model=deployment_name)`
- Returns `result.text`
- Wrap in `asyncio.to_thread()` at the call site (not inside this function) since the OpenAI sync client blocks. The function itself is sync â€” caller wraps in `asyncio.to_thread()`.
- Include type hints, docstring, and error handling with logging

NOTE: Do NOT use AzureOpenAIChatClient for Whisper -- use the openai.AzureOpenAI client directly (the Agent Framework chat client doesn't expose audio.transcriptions).
  </action>
  <verify>
- `ruff check backend/src/second_brain/config.py backend/src/second_brain/db/blob_storage.py backend/src/second_brain/tools/transcription.py` passes
- `ruff format --check backend/src/second_brain/` passes
- `python3 -c "from second_brain.config import get_settings; s = get_settings(); print(s.blob_storage_url, s.azure_openai_whisper_deployment_name)"` prints defaults
- `python3 -c "from second_brain.db.blob_storage import BlobStorageManager; print('OK')"` imports without error
- `python3 -c "from second_brain.tools.transcription import transcribe_audio; print('OK')"` imports without error
  </verify>
  <done>
BlobStorageManager class exists with upload_audio and delete_audio methods. transcribe_audio function exists wrapping Whisper API. Settings includes blob_storage_url and azure_openai_whisper_deployment_name with defaults.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Perception Agent</name>
  <files>
    backend/src/second_brain/agents/perception.py
  </files>
  <action>
**agents/perception.py** -- Create the Perception Agent:
- `def create_perception_agent(chat_client: AzureOpenAIChatClient) -> Agent`
- The Perception Agent is a simple tool-based agent. Its ONLY purpose is to transcribe audio and return the text.
- Instructions: "You are the Perception Agent. When given audio input, use the transcribe_audio tool to convert speech to text. Return ONLY the transcribed text, with no commentary or modification."
- Description: "Transcribes audio input to text via Whisper"
- Name: "Perception"
- Follow the exact same pattern as `create_classifier_agent` and `create_orchestrator_agent` (use `chat_client.as_agent()`).

NOTE: Unlike the Classifier which has tools bound via ClassificationTools, the Perception Agent's transcription will be called DIRECTLY by the voice-capture endpoint (not as an agent tool call). The Perception Agent exists primarily for the agent chain visibility (step dots in UI) and architectural consistency. The actual Whisper transcription happens in the endpoint before the workflow runs. The Perception Agent will be a participant in the HandoffBuilder workflow but its "work" (transcription) is performed outside the agent framework because Whisper requires file bytes, not text.

Approach: The Perception Agent is a thin agent that receives the transcribed text and passes it to the Orchestrator. In practice, the endpoint does: (1) upload audio, (2) transcribe via Whisper, (3) emit StepStarted/StepFinished for "Perception" manually in the SSE stream, (4) feed the transcribed text into the existing Orchestrator -> Classifier workflow. The Perception Agent is NOT added to the HandoffBuilder -- instead, the endpoint emits synthetic step events for it. This avoids the complexity of passing audio bytes through the agent framework.

Implementation: Create the agent module for future use but the current integration will use synthetic step events in the endpoint (Plan 02).
  </action>
  <verify>
- `ruff check backend/src/second_brain/agents/perception.py` passes
- `python3 -c "from second_brain.agents.perception import create_perception_agent; print('OK')"` imports without error
  </verify>
  <done>
Perception Agent module exists with create_perception_agent function following established agent patterns.
  </done>
</task>

</tasks>

<verification>
1. All new Python files pass `ruff check` and `ruff format --check`
2. All new modules import without errors
3. Settings includes blob_storage_url and azure_openai_whisper_deployment_name
4. pyproject.toml includes azure-storage-blob and python-multipart dependencies
5. Existing tests still pass: `cd backend && python3 -m pytest tests/ -x`
</verification>

<success_criteria>
- BlobStorageManager provides async upload/delete for audio files
- transcribe_audio function wraps Whisper API with proper auth
- Perception Agent exists as a named agent for the capture pipeline
- Settings updated with voice capture configuration
- Dependencies added to pyproject.toml
- All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/05-voice-capture/05-01-SUMMARY.md`
</output>
