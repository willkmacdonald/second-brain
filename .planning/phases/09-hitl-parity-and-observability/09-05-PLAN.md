---
phase: 09-hitl-parity-and-observability
plan: 05
type: execute
wave: 2
depends_on: [09-04]
files_modified:
  - backend/src/second_brain/api/capture.py
  - mobile/lib/ag-ui-client.ts
  - mobile/lib/types.ts
  - mobile/app/capture/text.tsx
  - mobile/app/(tabs)/index.tsx
autonomous: true
requirements: [HITL-02]
gap_closure: true

must_haves:
  truths:
    - "Follow-up clarification screen defaults to voice input with text as secondary option"
    - "Voice follow-up uploads audio, transcribes it, then reclassifies on the same Foundry thread"
    - "Text follow-up still works as a fallback when user taps the text toggle"
  artifacts:
    - path: "backend/src/second_brain/api/capture.py"
      provides: "POST /api/capture/follow-up/voice endpoint accepting multipart audio"
      contains: "follow_up_voice"
    - path: "mobile/lib/ag-ui-client.ts"
      provides: "sendFollowUpVoice function for audio follow-up"
      contains: "sendFollowUpVoice"
    - path: "mobile/lib/types.ts"
      provides: "SendFollowUpVoiceOptions interface"
      contains: "SendFollowUpVoiceOptions"
  key_links:
    - from: "mobile/app/capture/text.tsx"
      to: "mobile/lib/ag-ui-client.ts"
      via: "sendFollowUpVoice call"
      pattern: "sendFollowUpVoice"
    - from: "mobile/app/(tabs)/index.tsx"
      to: "mobile/lib/ag-ui-client.ts"
      via: "sendFollowUpVoice call"
      pattern: "sendFollowUpVoice"
    - from: "backend/src/second_brain/api/capture.py"
      to: "backend/src/second_brain/streaming/adapter.py"
      via: "stream_follow_up_capture after transcription"
      pattern: "stream_follow_up_capture"
    - from: "backend/src/second_brain/api/capture.py"
      to: "_stream_with_reconciliation (updated in 09-04 to handle LOW_CONFIDENCE)"
      via: "voice follow-up wraps stream with reconciliation"
      pattern: "_stream_with_reconciliation"
---

<objective>
Add voice recording as the default follow-up input mode for misunderstood captures, with text as a fallback.

Purpose: The conversation/clarification flow currently only supports text replies. The user wants voice as the primary input mode for follow-up clarifications, consistent with the app's voice-first capture philosophy. The backend needs a new endpoint that accepts audio uploads, transcribes via gpt-4o-transcribe, then feeds the transcript into the existing follow-up reclassification stream.

Dependency: This plan depends on 09-04 which updates `_stream_with_reconciliation` to handle both CLASSIFIED and LOW_CONFIDENCE events. The voice follow-up endpoint wraps its stream with `_stream_with_reconciliation`, so it needs the LOW_CONFIDENCE handling to be in place.

Output: New backend voice follow-up endpoint, mobile voice recorder on follow-up screens, text toggle as secondary option.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-hitl-parity-and-observability/09-03-SUMMARY.md
@.planning/phases/09-hitl-parity-and-observability/09-04-SUMMARY.md
@backend/src/second_brain/api/capture.py
@backend/src/second_brain/streaming/adapter.py
@backend/src/second_brain/tools/transcription.py
@backend/src/second_brain/db/blob_storage.py
@mobile/lib/ag-ui-client.ts
@mobile/lib/types.ts
@mobile/app/capture/text.tsx
@mobile/app/(tabs)/index.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add POST /api/capture/follow-up/voice endpoint</name>
  <files>backend/src/second_brain/api/capture.py</files>
  <action>
    Add a new endpoint `POST /api/capture/follow-up/voice` in `capture.py` that:

    1. Accepts multipart form data with fields:
       - `file: UploadFile` (the audio recording)
       - `inbox_item_id: str` (form field, not JSON -- use `Form(...)` from FastAPI)
       - `follow_up_round: int = 1` (form field with default)

    2. Looks up the original inbox item from Cosmos to get `foundryThreadId` (same pattern as the existing text `follow_up` endpoint).

    3. Reads the audio bytes from the upload and uploads to Blob Storage for audit trail:
       ```python
       blob_manager = getattr(request.app.state, "blob_manager", None)
       if blob_manager is None:
           raise HTTPException(status_code=503, detail="Blob storage not configured.")

       audio_bytes = await file.read()
       blob_url = await blob_manager.upload_audio(
           audio_bytes=audio_bytes,
           filename=file.filename or "follow-up.m4a",
       )
       ```

    4. Transcribes the audio using the in-memory `audio_bytes` directly -- do NOT re-download from blob storage. The bytes are already in memory from step 3:
       ```python
       openai_client = request.app.state.openai_client
       if openai_client is None:
           raise HTTPException(status_code=503, detail="Transcription not configured.")

       transcript = await openai_client.audio.transcriptions.create(
           model="gpt-4o-transcribe",
           file=("recording.m4a", audio_bytes, "audio/m4a"),
       )
       follow_up_text = transcript.text
       ```

       IMPORTANT: Do NOT use `request.app.state.credential` or construct a `BlobClient` to download the audio. The `audio_bytes` are already in memory from `await file.read()`. Using the blob_manager's internal credential or constructing BlobClient manually is unnecessary and fragile. Transcribe directly from the in-memory bytes.

       NOTE: We transcribe in the endpoint (not via agent tool) because this is a follow-up -- the agent only needs the text for reclassification, not the audio. This avoids an extra agent round-trip.

    5. Stream the follow-up reclassification using `stream_follow_up_capture` with the transcribed text (same as text follow-up).

    6. Wrap the generator with `_stream_with_reconciliation` for orphan handling (same as text follow-up). NOTE: 09-04 has already updated `_stream_with_reconciliation` to handle both CLASSIFIED and LOW_CONFIDENCE events, so this will correctly reconcile orphan docs regardless of which event type the follow-up produces.

    7. Clean up the blob after streaming completes (same pattern as `capture_voice` -- use a wrapper generator with `finally` block calling `blob_manager.delete_audio`).

    The endpoint function signature:
    ```python
    @router.post("/api/capture/follow-up/voice")
    async def follow_up_voice(
        request: Request,
        file: UploadFile = File(...),
        inbox_item_id: str = Form(...),
        follow_up_round: int = Form(1),
    ) -> StreamingResponse:
    ```

    Import `Form` from fastapi at the top of the file (add to existing import line).

    Do NOT modify any existing endpoints. Do NOT modify adapter.py or sse.py. Do NOT change the existing follow-up endpoint.
  </action>
  <verify>
    `grep -n "follow_up_voice" backend/src/second_brain/api/capture.py` shows the endpoint function.
    `grep -n "Form" backend/src/second_brain/api/capture.py` shows the Form import.
    `grep -n "/api/capture/follow-up/voice" backend/src/second_brain/api/capture.py` shows the route.
    `grep -n "_stream_with_reconciliation" backend/src/second_brain/api/capture.py` shows it wrapping the voice follow-up stream.
    `grep -n "BlobClient" backend/src/second_brain/api/capture.py` returns NO matches (must NOT construct BlobClient manually).
    `grep -n "credential" backend/src/second_brain/api/capture.py` returns NO matches (must NOT access app.state.credential).
    The existing `follow_up` endpoint at `/api/capture/follow-up` is unchanged.
  </verify>
  <done>
    New POST /api/capture/follow-up/voice endpoint accepts multipart audio + inbox_item_id, transcribes via gpt-4o-transcribe using in-memory audio bytes (no blob re-download), then streams follow-up reclassification with reconciliation (which handles both CLASSIFIED and LOW_CONFIDENCE per 09-04). Blob uploaded for audit trail and cleaned up after stream completes. Existing text follow-up endpoint unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add voice follow-up client and voice-first UI on follow-up screens</name>
  <files>
    mobile/lib/types.ts
    mobile/lib/ag-ui-client.ts
    mobile/app/capture/text.tsx
    mobile/app/(tabs)/index.tsx
  </files>
  <action>
    **types.ts:**
    Add a new interface:
    ```typescript
    export interface SendFollowUpVoiceOptions {
      audioUri: string;
      inboxItemId: string;
      followUpRound: number;
      apiKey: string;
      callbacks: StreamingCallbacks;
    }
    ```

    **ag-ui-client.ts:**
    Add a new `sendFollowUpVoice` export function (modeled on `sendVoiceCapture` + `sendFollowUp`):
    ```typescript
    export function sendFollowUpVoice({
      audioUri,
      inboxItemId,
      followUpRound,
      apiKey,
      callbacks,
    }: SendFollowUpVoiceOptions): () => void {
      const formData = new FormData();
      formData.append("file", {
        uri: audioUri,
        type: "audio/m4a",
        name: "follow-up.m4a",
      } as any);
      formData.append("inbox_item_id", inboxItemId);
      formData.append("follow_up_round", String(followUpRound));

      const es = new EventSource<AGUIEventType>(
        `${API_BASE_URL}/api/capture/follow-up/voice`,
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${apiKey}`,
          },
          body: formData,
          pollingInterval: 0,
        },
      );

      return attachCallbacks(es, callbacks);
    }
    ```
    Import `SendFollowUpVoiceOptions` from types.ts.

    **index.tsx (voice capture screen -- already has voice recording infrastructure):**
    This screen already has the audio recorder and misunderstood follow-up handling. When `agentQuestion` is set (misunderstood state), it currently shows a text input + Reply button. Change this to:

    1. Add a new state `followUpMode: "voice" | "text"` initialized to `"voice"`.
    2. When `agentQuestion` is set, show:
       - A voice record button (reuse the same record button pattern from the main capture) as the DEFAULT
       - A small "Type instead" text toggle below that switches to `followUpMode: "text"`
       - When in text mode, show the existing text input + Reply button, plus a "Record instead" toggle
    3. When the user records and stops in follow-up mode:
       - Check duration >= 1 second (same as main capture)
       - Call `sendFollowUpVoice` with the audio URI, misunderstoodInboxItemId, followUpRound
       - Use the following SSE callbacks (must match `handleFollowUpSubmit` exactly): `onMisunderstood`, `onUnresolved`, `onComplete`, `onError`, `onLowConfidence`
    4. Create a new `handleVoiceFollowUp` function that mirrors `handleFollowUpSubmit` but uses `sendFollowUpVoice` instead of `sendFollowUp`. The callbacks object MUST include ALL of these callbacks:
       - `onMisunderstood`: updates agentQuestion, increments followUpRound (same as text follow-up)
       - `onUnresolved`: resets state with unresolved toast (same as text follow-up)
       - `onComplete`: resets state with success toast (same as text follow-up)
       - `onError`: resets state with error toast (same as text follow-up)
       - `onLowConfidence`: resets state with filed toast, auto-accepts the pending result (same as text follow-up -- added by 09-04)

    Import `sendFollowUpVoice` from ag-ui-client.ts. Import `RecordingPresets` and audio modules (already imported in index.tsx).

    **text.tsx (text capture screen -- does NOT have audio recording):**
    This screen needs to gain voice recording capability for follow-up mode. When `agentQuestion` is set:

    1. Add new state declarations (explicit useState calls):
       ```typescript
       const [followUpMode, setFollowUpMode] = useState<"voice" | "text">("voice");
       const [isFollowUpRecording, setIsFollowUpRecording] = useState(false);
       ```

    2. Add audio recording setup with ALL required imports. At the top of the file, add to imports:
       ```typescript
       import { useAudioRecorder, AudioModule, useAudioRecorderState, RecordingPresets } from "expo-audio";
       import { Animated } from "react-native";
       ```
       NOTE: `Animated` is imported from `react-native` -- add it to the existing `react-native` import destructure (which currently imports View, TextInput, Pressable, Text, StyleSheet, ScrollView).

    3. Inside the component body (after existing state declarations), add audio recorder setup:
       ```typescript
       const audioRecorder = useAudioRecorder(RecordingPresets.HIGH_QUALITY);
       const recorderState = useAudioRecorderState(audioRecorder, 100);
       ```

    4. Add a `useEffect` for mic permission when entering voice follow-up mode:
       ```typescript
       useEffect(() => {
         if (followUpMode === "voice" && agentQuestion) {
           AudioModule.requestRecordingPermissionsAsync();
         }
       }, [followUpMode, agentQuestion]);
       ```

    5. When `agentQuestion` is shown (the agent question bubble area), below it render:
       - If `followUpMode === "voice"`: A smaller record button (round, 60x60, same red active style), duration timer when recording, "Type instead" toggle
       - If `followUpMode === "text"`: The existing TextInput + header Reply button, "Record instead" toggle

    6. Create `handleVoiceFollowUpSubmit` that:
       - Stops recording via `audioRecorder.stop()`
       - Checks `recorderState.durationMillis >= 1000` (1 second minimum)
       - Gets the audio URI from `audioRecorder.uri`
       - Calls `sendFollowUpVoice({ audioUri, inboxItemId: misunderstoodInboxItemId, followUpRound, apiKey: API_KEY!, callbacks })`
       - The callbacks object MUST include ALL of these callbacks (matching `handleFollowUpSubmit` exactly):
         - `onMisunderstood`: updates agentQuestion, increments followUpRound
         - `onUnresolved`: resets state with unresolved toast
         - `onComplete`: resets state with success toast
         - `onError`: resets state with error toast
         - `onLowConfidence`: resets state with filed toast, auto-accepts the pending result (added by 09-04)
       - Sets `setIsReclassifying(true)`, `setShowSteps(true)`, resets step state

    7. The existing text follow-up path (`handleFollowUpSubmit`) remains unchanged and is used when `followUpMode === "text"`.

    Import `sendFollowUpVoice` from ag-ui-client.ts (add to existing import from `../../lib/ag-ui-client`).

    Style the follow-up voice recorder to be compact (not full-screen like the main capture). The record button should be 60x60 (not 80x80), centered below the agent question bubble. Use a simple toggle link text like "Type instead" / "Record instead" in #4a90d9 color, fontSize 13.

    Add these styles to the StyleSheet.create block:
    ```typescript
    followUpRecordButton: {
      width: 60,
      height: 60,
      borderRadius: 30,
      backgroundColor: "#333",
      alignItems: "center",
      justifyContent: "center",
      alignSelf: "center",
      marginTop: 12,
    },
    followUpRecordButtonActive: {
      backgroundColor: "#e53935",
    },
    followUpToggle: {
      color: "#4a90d9",
      fontSize: 13,
      textAlign: "center",
      marginTop: 8,
    },
    followUpDuration: {
      color: "#999",
      fontSize: 13,
      textAlign: "center",
      marginTop: 4,
    },
    ```
  </action>
  <verify>
    `grep -n "sendFollowUpVoice" mobile/lib/ag-ui-client.ts` shows the function export.
    `grep -n "SendFollowUpVoiceOptions" mobile/lib/types.ts` shows the interface.
    `grep -n "sendFollowUpVoice" mobile/app/capture/text.tsx` shows it imported and used.
    `grep -n "sendFollowUpVoice" mobile/app/(tabs)/index.tsx` shows it imported and used.
    `grep -n "followUpMode" mobile/app/capture/text.tsx` shows voice/text toggle state.
    `grep -n "followUpMode" mobile/app/(tabs)/index.tsx` shows voice/text toggle state.
    `grep -n "useAudioRecorder" mobile/app/capture/text.tsx` shows audio recorder setup.
    `grep -n "isFollowUpRecording" mobile/app/capture/text.tsx` shows explicit useState declaration.
    `grep -n "Animated" mobile/app/capture/text.tsx` shows Animated import from react-native.
    `grep -n "onLowConfidence" mobile/app/capture/text.tsx` shows onLowConfidence callback in handleVoiceFollowUpSubmit.
    `grep -n "onLowConfidence" mobile/app/(tabs)/index.tsx` shows onLowConfidence callback in handleVoiceFollowUp.
    `cd mobile && npx tsc --noEmit` passes with no type errors.
  </verify>
  <done>
    Mobile follow-up screens default to voice recording with "Type instead" toggle. Voice follow-up uploads audio to new backend endpoint which transcribes and reclassifies. Text follow-up still works as fallback. Both text.tsx and index.tsx support voice-first follow-up with all five SSE callbacks (onMisunderstood, onUnresolved, onComplete, onError, onLowConfidence). text.tsx has complete audio recording infrastructure (useAudioRecorder, AudioModule, permissions, explicit state declarations). TypeScript compiles cleanly.
  </done>
</task>

</tasks>

<verification>
- Backend: `grep -n "follow_up_voice" backend/src/second_brain/api/capture.py` shows the new endpoint
- Backend: `grep -n "BlobClient" backend/src/second_brain/api/capture.py` returns NO matches (no manual blob download)
- Backend: Existing text follow-up endpoint `/api/capture/follow-up` is unchanged
- Mobile: `npx tsc --noEmit` in mobile/ passes
- Mobile: `grep -r "sendFollowUpVoice" mobile/` shows usage in ag-ui-client.ts, text.tsx, and index.tsx
- Mobile: `grep -r "followUpMode" mobile/` shows voice/text toggle in both capture screens
- Mobile: Voice follow-up is the default mode (followUpMode initialized to "voice")
- Mobile: text.tsx has complete audio imports (useAudioRecorder, AudioModule, useAudioRecorderState, RecordingPresets, Animated)
- Mobile: `grep -r "onLowConfidence" mobile/app/capture/text.tsx mobile/app/(tabs)/index.tsx` shows onLowConfidence in voice follow-up callbacks on both screens
</verification>

<success_criteria>
1. New POST /api/capture/follow-up/voice endpoint transcribes audio from in-memory bytes (no blob re-download) and reclassifies
2. Mobile follow-up defaults to voice recording (not text input)
3. "Type instead" toggle switches to text input (existing flow)
4. "Record instead" toggle switches back to voice
5. Voice follow-up produces the same SSE events as text follow-up
6. Voice follow-up callbacks include all five: onMisunderstood, onUnresolved, onComplete, onError, onLowConfidence
7. Blob cleanup happens after stream completes
8. text.tsx has all audio recording infrastructure (recorder, state, permissions, Animated import)
9. TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/09-hitl-parity-and-observability/09-05-SUMMARY.md`
</output>
