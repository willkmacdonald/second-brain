---
phase: 09-hitl-parity-and-observability
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/second_brain/api/capture.py
  - mobile/lib/ag-ui-client.ts
  - mobile/lib/types.ts
  - mobile/app/capture/text.tsx
  - mobile/app/(tabs)/index.tsx
autonomous: true
requirements: [HITL-02]
gap_closure: true

must_haves:
  truths:
    - "Follow-up clarification screen defaults to voice input with text as secondary option"
    - "Voice follow-up uploads audio, transcribes it, then reclassifies on the same Foundry thread"
    - "Text follow-up still works as a fallback when user taps the text toggle"
  artifacts:
    - path: "backend/src/second_brain/api/capture.py"
      provides: "POST /api/capture/follow-up/voice endpoint accepting multipart audio"
      contains: "follow_up_voice"
    - path: "mobile/lib/ag-ui-client.ts"
      provides: "sendFollowUpVoice function for audio follow-up"
      contains: "sendFollowUpVoice"
    - path: "mobile/lib/types.ts"
      provides: "SendFollowUpVoiceOptions interface"
      contains: "SendFollowUpVoiceOptions"
  key_links:
    - from: "mobile/app/capture/text.tsx"
      to: "mobile/lib/ag-ui-client.ts"
      via: "sendFollowUpVoice call"
      pattern: "sendFollowUpVoice"
    - from: "mobile/app/(tabs)/index.tsx"
      to: "mobile/lib/ag-ui-client.ts"
      via: "sendFollowUpVoice call"
      pattern: "sendFollowUpVoice"
    - from: "backend/src/second_brain/api/capture.py"
      to: "backend/src/second_brain/streaming/adapter.py"
      via: "stream_follow_up_capture after transcription"
      pattern: "stream_follow_up_capture"
---

<objective>
Add voice recording as the default follow-up input mode for misunderstood captures, with text as a fallback.

Purpose: The conversation/clarification flow currently only supports text replies. The user wants voice as the primary input mode for follow-up clarifications, consistent with the app's voice-first capture philosophy. The backend needs a new endpoint that accepts audio uploads, transcribes via gpt-4o-transcribe, then feeds the transcript into the existing follow-up reclassification stream.

Output: New backend voice follow-up endpoint, mobile voice recorder on follow-up screens, text toggle as secondary option.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-hitl-parity-and-observability/09-03-SUMMARY.md
@backend/src/second_brain/api/capture.py
@backend/src/second_brain/streaming/adapter.py
@backend/src/second_brain/tools/transcription.py
@backend/src/second_brain/db/blob_storage.py
@mobile/lib/ag-ui-client.ts
@mobile/lib/types.ts
@mobile/app/capture/text.tsx
@mobile/app/(tabs)/index.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add POST /api/capture/follow-up/voice endpoint</name>
  <files>backend/src/second_brain/api/capture.py</files>
  <action>
    Add a new endpoint `POST /api/capture/follow-up/voice` in `capture.py` that:

    1. Accepts multipart form data with fields:
       - `file: UploadFile` (the audio recording)
       - `inbox_item_id: str` (form field, not JSON -- use `Form(...)` from FastAPI)
       - `follow_up_round: int = 1` (form field with default)

    2. Looks up the original inbox item from Cosmos to get `foundryThreadId` (same pattern as the existing text `follow_up` endpoint).

    3. Uploads audio to Blob Storage using `request.app.state.blob_manager.upload_audio()` (same as `capture_voice`).

    4. Transcribes the audio directly using the OpenAI client on app.state:
       ```python
       openai_client = request.app.state.openai_client
       audio_bytes = await file.read()
       blob_url = await blob_manager.upload_audio(audio_bytes, file.filename or "follow-up.m4a")

       # Download and transcribe (reuse the TranscriptionTools pattern)
       from azure.storage.blob.aio import BlobClient
       credential = request.app.state.credential
       blob_client_dl = BlobClient.from_blob_url(blob_url, credential=credential)
       try:
           downloader = await blob_client_dl.download_blob()
           audio_data = await downloader.readall()
       finally:
           await blob_client_dl.close()

       transcript = await openai_client.audio.transcriptions.create(
           model="gpt-4o-transcribe",
           file=("recording.m4a", audio_data, "audio/m4a"),
       )
       follow_up_text = transcript.text
       ```

       NOTE: We transcribe in the endpoint (not via agent tool) because this is a follow-up -- the agent only needs the text for reclassification, not the audio. This avoids an extra agent round-trip.

    5. Stream the follow-up reclassification using `stream_follow_up_capture` with the transcribed text (same as text follow-up).

    6. Wrap the generator with `_stream_with_reconciliation` for orphan handling (same as text follow-up).

    7. Clean up the blob after streaming completes (same pattern as `capture_voice` -- use a wrapper generator with `finally` block calling `blob_manager.delete_audio`).

    The endpoint function signature:
    ```python
    @router.post("/api/capture/follow-up/voice")
    async def follow_up_voice(
        request: Request,
        file: UploadFile = File(...),
        inbox_item_id: str = Form(...),
        follow_up_round: int = Form(1),
    ) -> StreamingResponse:
    ```

    Import `Form` from fastapi at the top of the file (add to existing import line).

    Do NOT modify any existing endpoints. Do NOT modify adapter.py or sse.py. Do NOT change the existing follow-up endpoint.
  </action>
  <verify>
    `grep -n "follow_up_voice" backend/src/second_brain/api/capture.py` shows the endpoint function.
    `grep -n "Form" backend/src/second_brain/api/capture.py` shows the Form import.
    `grep -n "/api/capture/follow-up/voice" backend/src/second_brain/api/capture.py` shows the route.
    The existing `follow_up` endpoint at `/api/capture/follow-up` is unchanged.
  </verify>
  <done>
    New POST /api/capture/follow-up/voice endpoint accepts multipart audio + inbox_item_id, transcribes via gpt-4o-transcribe, then streams follow-up reclassification with reconciliation. Blob cleaned up after stream completes. Existing text follow-up endpoint unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add voice follow-up client and voice-first UI on follow-up screens</name>
  <files>
    mobile/lib/types.ts
    mobile/lib/ag-ui-client.ts
    mobile/app/capture/text.tsx
    mobile/app/(tabs)/index.tsx
  </files>
  <action>
    **types.ts:**
    Add a new interface:
    ```typescript
    export interface SendFollowUpVoiceOptions {
      audioUri: string;
      inboxItemId: string;
      followUpRound: number;
      apiKey: string;
      callbacks: StreamingCallbacks;
    }
    ```

    **ag-ui-client.ts:**
    Add a new `sendFollowUpVoice` export function (modeled on `sendVoiceCapture` + `sendFollowUp`):
    ```typescript
    export function sendFollowUpVoice({
      audioUri,
      inboxItemId,
      followUpRound,
      apiKey,
      callbacks,
    }: SendFollowUpVoiceOptions): () => void {
      const formData = new FormData();
      formData.append("file", {
        uri: audioUri,
        type: "audio/m4a",
        name: "follow-up.m4a",
      } as any);
      formData.append("inbox_item_id", inboxItemId);
      formData.append("follow_up_round", String(followUpRound));

      const es = new EventSource<AGUIEventType>(
        `${API_BASE_URL}/api/capture/follow-up/voice`,
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${apiKey}`,
          },
          body: formData,
          pollingInterval: 0,
        },
      );

      return attachCallbacks(es, callbacks);
    }
    ```
    Import `SendFollowUpVoiceOptions` from types.ts.

    **index.tsx (voice capture screen -- already has voice recording infrastructure):**
    This screen already has the audio recorder and misunderstood follow-up handling. When `agentQuestion` is set (misunderstood state), it currently shows a text input + Reply button. Change this to:

    1. Add a new state `followUpMode: "voice" | "text"` initialized to `"voice"`.
    2. When `agentQuestion` is set, show:
       - A voice record button (reuse the same record button pattern from the main capture) as the DEFAULT
       - A small "Type instead" text toggle below that switches to `followUpMode: "text"`
       - When in text mode, show the existing text input + Reply button, plus a "Record instead" toggle
    3. When the user records and stops in follow-up mode:
       - Check duration >= 1 second (same as main capture)
       - Call `sendFollowUpVoice` with the audio URI, misunderstoodInboxItemId, followUpRound
       - Use the same SSE callbacks as the existing `handleFollowUpSubmit` (onMisunderstood, onComplete, onError, etc.)
    4. Create a new `handleVoiceFollowUp` function that mirrors `handleFollowUpSubmit` but uses `sendFollowUpVoice` instead of `sendFollowUp`.

    Import `sendFollowUpVoice` from ag-ui-client.ts. Import `RecordingPresets` and audio modules (already imported in index.tsx).

    **text.tsx (text capture screen -- does NOT have audio recording):**
    This screen needs to gain voice recording capability for follow-up mode. When `agentQuestion` is set:

    1. Add state: `followUpMode: "voice" | "text"` initialized to `"voice"`.
    2. Add audio recording setup:
       - Import `useAudioRecorder, AudioModule, useAudioRecorderState, RecordingPresets` from `expo-audio`
       - Import `Animated` from `react-native`
       - Set up the recorder: `const audioRecorder = useAudioRecorder(RecordingPresets.HIGH_QUALITY);` and `const recorderState = useAudioRecorderState(audioRecorder, 100);`
       - Add `isFollowUpRecording` state for tracking recording state
       - Request mic permission when entering voice follow-up mode (useEffect on followUpMode === "voice" && agentQuestion)
    3. When `agentQuestion` is shown (the agent question bubble area), below it render:
       - If `followUpMode === "voice"`: A smaller record button (round, 60x60, same red active style), duration timer when recording, "Type instead" toggle
       - If `followUpMode === "text"`: The existing TextInput + header Reply button, "Record instead" toggle
    4. Create `handleVoiceFollowUpSubmit` that:
       - Stops recording
       - Checks duration >= 1 second
       - Calls `sendFollowUpVoice({ audioUri, inboxItemId: misunderstoodInboxItemId, followUpRound, apiKey, callbacks })` with the same callbacks as `handleFollowUpSubmit`
       - Sets isReclassifying, shows steps, etc.
    5. The existing text follow-up path (`handleFollowUpSubmit`) remains unchanged and is used when `followUpMode === "text"`.

    Import `sendFollowUpVoice` from ag-ui-client.ts.

    Style the follow-up voice recorder to be compact (not full-screen like the main capture). The record button should be 60x60 (not 80x80), centered below the agent question bubble. Use a simple toggle link text like "Type instead" / "Record instead" in #4a90d9 color, fontSize 13.
  </action>
  <verify>
    `grep -n "sendFollowUpVoice" mobile/lib/ag-ui-client.ts` shows the function export.
    `grep -n "SendFollowUpVoiceOptions" mobile/lib/types.ts` shows the interface.
    `grep -n "sendFollowUpVoice" mobile/app/capture/text.tsx` shows it imported and used.
    `grep -n "sendFollowUpVoice" mobile/app/(tabs)/index.tsx` shows it imported and used.
    `grep -n "followUpMode" mobile/app/capture/text.tsx` shows voice/text toggle state.
    `grep -n "followUpMode" mobile/app/(tabs)/index.tsx` shows voice/text toggle state.
    `cd mobile && npx tsc --noEmit` passes with no type errors.
  </verify>
  <done>
    Mobile follow-up screens default to voice recording with "Type instead" toggle. Voice follow-up uploads audio to new backend endpoint which transcribes and reclassifies. Text follow-up still works as fallback. Both text.tsx and index.tsx support voice-first follow-up. TypeScript compiles cleanly.
  </done>
</task>

</tasks>

<verification>
- Backend: `grep -n "follow_up_voice" backend/src/second_brain/api/capture.py` shows the new endpoint
- Backend: Existing text follow-up endpoint `/api/capture/follow-up` is unchanged
- Mobile: `npx tsc --noEmit` in mobile/ passes
- Mobile: `grep -r "sendFollowUpVoice" mobile/` shows usage in ag-ui-client.ts, text.tsx, and index.tsx
- Mobile: `grep -r "followUpMode" mobile/` shows voice/text toggle in both capture screens
- Mobile: Voice follow-up is the default mode (followUpMode initialized to "voice")
</verification>

<success_criteria>
1. New POST /api/capture/follow-up/voice endpoint transcribes audio and reclassifies
2. Mobile follow-up defaults to voice recording (not text input)
3. "Type instead" toggle switches to text input (existing flow)
4. "Record instead" toggle switches back to voice
5. Voice follow-up produces the same SSE events as text follow-up
6. Blob cleanup happens after stream completes
7. TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/09-hitl-parity-and-observability/09-05-SUMMARY.md`
</output>
