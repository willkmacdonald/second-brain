---
phase: 04.3-agent-user-ux-with-unclear-item
plan: 09
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/second_brain/agents/workflow.py
autonomous: true
requirements: [CLAS-04, APPX-04]
gap_closure: true

must_haves:
  truths:
    - "Capture screen toast shows corrected confidence score (e.g., 0.75), not stale 0.00 from raw LLM args"
    - "CLASSIFIED custom event carries the corrected confidence from the tool return string, not from function_call.arguments"
    - "When tool return string says 'Filed -> People (0.75)', the toast shows 0.75 (not the pre-fallback 0.00)"
  artifacts:
    - path: "backend/src/second_brain/agents/workflow.py"
      provides: "clean_result and CLASSIFIED event built from tool return string, not from detected_tool_args"
      contains: "_parse_classify_result"
  key_links:
    - from: "backend/src/second_brain/agents/workflow.py"
      to: "backend/src/second_brain/tools/classification.py"
      via: "Adapter parses tool return string (post-fallback values) instead of function_call.arguments (pre-fallback values)"
      pattern: "_parse_classify_result"
---

<objective>
Fix capture screen toast showing 0.00 confidence by parsing the tool's return string (which contains post-fallback corrected values) instead of using detected_tool_args (which contain the LLM's raw pre-fallback values).

Purpose: UAT Tests 1 and 2 fail because the adapter constructs clean_result text and CLASSIFIED event from function_call.arguments (captured BEFORE tool execution), but the confidence fallback (0.0 -> 0.75) happens inside classify_and_file DURING execution. The tool return string already contains the corrected values (e.g., "Filed -> People (0.75) | {uuid}").
Output: Updated workflow.py where clean_result and CLASSIFIED event use parsed return string values.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-RETEST2-UAT.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-07-SUMMARY.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-08-SUMMARY.md
@.planning/debug/capture-toast-zero-confidence.md
@backend/src/second_brain/agents/workflow.py
@backend/src/second_brain/tools/classification.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Parse tool return string for corrected confidence in clean_result and CLASSIFIED event</name>
  <files>backend/src/second_brain/agents/workflow.py</files>
  <action>
The root cause: `clean_result` (lines 379-400) and `CLASSIFIED` event (lines 425-435) both read confidence from `detected_tool_args` -- the LLM's raw function_call.arguments captured BEFORE tool execution. The fallback (0.0 -> 0.75) happens inside classify_and_file, so `detected_tool_args.get("confidence", 0.0)` returns the stale pre-fallback 0.0. The tool's RETURN STRING already contains the corrected values (e.g., "Filed -> People (0.75) | {uuid}").

**Fix approach:** Extract a classify_and_file function_result from the event stream (the same way we already extract the UUID for request_misunderstood and classify_and_file). Parse the return string to get corrected bucket, confidence, and status. Use parsed values for clean_result text and CLASSIFIED event instead of detected_tool_args.

**Step 1: Add a return string parser.**

Add a module-level helper after `_UUID_RE`:

```python
# Regex to parse classify_and_file return string.
# Matches: "Filed -> Bucket (0.XX) | uuid" or "Filed (needs review) -> Bucket (0.XX) | uuid"
_CLASSIFY_RESULT_RE = re.compile(
    r"Filed(?: \(needs review\))?\s*(?:->|\u2192)\s*(\w+)\s*\(([0-9.]+)\)"
)


def _parse_classify_result(result_str: str) -> dict[str, Any] | None:
    """Parse corrected bucket and confidence from classify_and_file return string.

    The return string contains POST-fallback values (e.g., confidence 0.75
    after the 0.0 -> 0.75 default was applied). These are the authoritative
    values -- not the LLM's raw function_call.arguments.

    Returns dict with 'bucket', 'confidence', 'needs_review' keys, or None.
    """
    match = _CLASSIFY_RESULT_RE.search(result_str)
    if not match:
        return None
    return {
        "bucket": match.group(1),
        "confidence": float(match.group(2)),
        "needs_review": "needs review" in result_str,
    }
```

**Step 2: Capture the classify_and_file return string in `_process_update`.**

Add a new tracker `classify_result_str` alongside `classified_inbox_id`. Update `_process_update` to accept and return it as a 5th element:

Update the method signature:
```python
def _process_update(
    self,
    update: AgentResponseUpdate,
    detected_tool: str | None,
    detected_tool_args: dict[str, Any],
    misunderstood_inbox_id: str | None,
    classified_inbox_id: str | None,
    classify_result_str: str | None,
) -> tuple[str | None, dict[str, Any], str | None, str | None, str | None]:
```

Inside the `if detected_tool == "classify_and_file" and classified_inbox_id is None:` block, after extracting the inbox ID, also capture the full result string:

```python
if detected_tool == "classify_and_file" and classified_inbox_id is None:
    iid = self._extract_inbox_id_from_result(update)
    if iid is not None:
        classified_inbox_id = iid
        logger.info("Extracted classified inbox_id: %s", iid)
    # Also capture the full result string for corrected values
    if classify_result_str is None:
        for content in update.contents or []:
            if getattr(content, "type", None) == "function_result":
                classify_result_str = str(getattr(content, "result", "") or "")
                break
```

Return the 5-tuple:
```python
return detected_tool, detected_tool_args, misunderstood_inbox_id, classified_inbox_id, classify_result_str
```

**Step 3: Update `_stream_updates` to track and destructure the 5th element.**

Initialize:
```python
classify_result_str: str | None = None
```

Update BOTH call sites to pass and destructure the 5-tuple:

Call site 1 (WorkflowEvent branch, around lines 325-335):
```python
result = self._process_update(
    update, detected_tool, detected_tool_args,
    misunderstood_inbox_id, classified_inbox_id, classify_result_str,
)
detected_tool = result[0]
detected_tool_args = result[1]
misunderstood_inbox_id = result[2]
classified_inbox_id = result[3]
classify_result_str = result[4]
```

Call site 2 (AgentResponseUpdate branch, around lines 355-363):
```python
detected_tool, detected_tool_args, misunderstood_inbox_id, classified_inbox_id, classify_result_str = (
    self._process_update(
        event, detected_tool, detected_tool_args,
        misunderstood_inbox_id, classified_inbox_id, classify_result_str,
    )
)
```

**Step 4: Use parsed return string for clean_result construction.**

Replace the `if detected_tool == "classify_and_file":` block in the clean_result construction (around line 382) with:

```python
if detected_tool == "classify_and_file":
    # Use parsed return string for corrected values (post-fallback)
    parsed = _parse_classify_result(classify_result_str or "")
    if parsed:
        bucket = parsed["bucket"]
        confidence = parsed["confidence"]
        if parsed["needs_review"]:
            clean_result = f"Filed (needs review) \u2192 {bucket} ({confidence:.2f})"
        else:
            clean_result = f"Filed \u2192 {bucket} ({confidence:.2f})"
    else:
        # Fallback to detected_tool_args if return string not parseable
        bucket = detected_tool_args.get("bucket", "?")
        confidence = detected_tool_args.get("confidence", 0.0)
        if confidence < self._classification_threshold:
            clean_result = f"Filed (needs review) \u2192 {bucket} ({confidence:.2f})"
        else:
            clean_result = f"Filed \u2192 {bucket} ({confidence:.2f})"
```

**Step 5: Use parsed return string for CLASSIFIED custom event.**

Replace the CLASSIFIED event emission (around line 427) with:

```python
elif detected_tool == "classify_and_file":
    logger.info("Classification completed via classify_and_file")
    if classified_inbox_id:
        # Use parsed return string for corrected values (post-fallback)
        parsed = _parse_classify_result(classify_result_str or "")
        if parsed:
            yield CustomEvent(
                name="CLASSIFIED",
                value={
                    "inboxItemId": classified_inbox_id,
                    "bucket": parsed["bucket"],
                    "confidence": parsed["confidence"],
                },
            )
        else:
            yield CustomEvent(
                name="CLASSIFIED",
                value={
                    "inboxItemId": classified_inbox_id,
                    "bucket": detected_tool_args.get("bucket", ""),
                    "confidence": detected_tool_args.get("confidence", 0.0),
                },
            )
```

The key insight: the classify_and_file return string ("Filed -> People (0.75) | {uuid}") is the authoritative source of truth. It contains values AFTER the 0.0 -> 0.75 fallback and score derivation. Using these parsed values means the toast and CLASSIFIED event always show what was actually stored in Cosmos DB.
  </action>
  <verify>
Run backend tests:
```bash
cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v
```

Verify the parser function exists and works:
```bash
cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "
from second_brain.agents.workflow import _parse_classify_result
# Test normal result
r1 = _parse_classify_result('Filed \u2192 People (0.75) | abc-123')
assert r1 is not None, 'Failed to parse normal result'
assert r1['bucket'] == 'People', f'Wrong bucket: {r1[\"bucket\"]}'
assert r1['confidence'] == 0.75, f'Wrong confidence: {r1[\"confidence\"]}'
assert r1['needs_review'] is False, 'Should not be needs_review'
# Test pending result
r2 = _parse_classify_result('Filed (needs review) \u2192 Admin (0.55) | def-456')
assert r2 is not None, 'Failed to parse pending result'
assert r2['bucket'] == 'Admin', f'Wrong bucket: {r2[\"bucket\"]}'
assert r2['confidence'] == 0.55, f'Wrong confidence: {r2[\"confidence\"]}'
assert r2['needs_review'] is True, 'Should be needs_review'
print('All parser tests passed')
"
```

Verify `_process_update` returns 5-tuple:
```bash
grep -n "classify_result_str" /Users/willmacdonald/Documents/Code/claude/second-brain/backend/src/second_brain/agents/workflow.py | head -10
```
  </verify>
  <done>
(1) `_parse_classify_result` helper extracts corrected bucket and confidence from classify_and_file return string.
(2) `_process_update` captures the full function_result string for classify_and_file and returns it as 5th element.
(3) clean_result text uses parsed return string values (post-fallback 0.75), not detected_tool_args (pre-fallback 0.00).
(4) CLASSIFIED custom event uses parsed return string values (post-fallback 0.75).
(5) Fallback to detected_tool_args if return string is not parseable (safety net).
(6) All existing backend tests pass.
  </done>
</task>

</tasks>

<verification>
- `python3 -m pytest tests/ -v` in backend directory: all tests pass
- `_parse_classify_result` correctly parses both "Filed -> Bucket (X.XX)" and "Filed (needs review) -> Bucket (X.XX)" formats
- clean_result text uses parsed confidence (post-fallback), not detected_tool_args (pre-fallback)
- CLASSIFIED event uses parsed confidence (post-fallback), not detected_tool_args (pre-fallback)
- When LLM sends confidence=0.0 but tool falls back to 0.75, toast shows 0.75
</verification>

<success_criteria>
- Capture screen toast shows corrected confidence score (e.g., 0.75), matching what is stored in Cosmos DB
- No more 0.00 confidence displayed in capture toast when the fallback corrected the value
- CLASSIFIED custom event carries corrected confidence for downstream consumers
- All existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-09-SUMMARY.md`
</output>
