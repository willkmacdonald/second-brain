---
phase: 04.3-agent-user-ux-with-unclear-item
plan: 08
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/second_brain/tools/classification.py
  - backend/src/second_brain/agents/workflow.py
  - backend/src/second_brain/main.py
autonomous: true
requirements: [CLAS-04, APPX-04]
gap_closure: true

must_haves:
  truths:
    - "Follow-up reply that successfully classifies updates the ORIGINAL inbox item (not creates a new one)"
    - "The orphaned inbox doc created by classify_and_file during follow-up is deleted after its metadata is copied to the original"
    - "The bucket doc created during follow-up has its inboxRecordId updated to point to the original inbox item"
    - "Follow-up reply that triggers request_misunderstood at round 1 updates the original item's clarificationText and deletes the orphan"
    - "No duplicate inbox items appear after follow-up classification"
  artifacts:
    - path: "backend/src/second_brain/main.py"
      provides: "Post-stream orphan reconciliation in follow_up_misunderstood endpoint"
      contains: "orphan_reconciliation"
    - path: "backend/src/second_brain/tools/classification.py"
      provides: "classify_and_file return string includes inbox_doc_id"
      contains: "inbox_doc_id"
    - path: "backend/src/second_brain/agents/workflow.py"
      provides: "CLASSIFIED custom event with orphan inbox ID"
      contains: "CLASSIFIED"
  key_links:
    - from: "backend/src/second_brain/agents/workflow.py"
      to: "backend/src/second_brain/main.py"
      via: "CLASSIFIED CustomEvent carries orphaned inbox_doc_id for follow-up endpoint reconciliation"
      pattern: "CustomEvent.*CLASSIFIED"
    - from: "backend/src/second_brain/main.py"
      to: "CosmosManager.upsert_item"
      via: "follow_up_misunderstood copies classification to original and deletes orphan"
      pattern: "upsert_item.*existing"
---

<objective>
Fix follow-up endpoint creating duplicate inbox items instead of updating the original. The workflow tools (classify_and_file, request_misunderstood) always create NEW docs with uuid4(). The follow-up endpoint must detect the outcome, copy metadata to the original inbox item, and clean up orphaned docs.

Purpose: UAT Tests 3, 4, and 7 fail because follow-up creates duplicates. The original misunderstood item stays stuck forever, and a new duplicate appears. This breaks the entire follow-up UX.
Output: Updated classify_and_file (include inbox_doc_id in return), updated workflow adapter (emit CLASSIFIED event), updated follow-up endpoint (post-stream orphan reconciliation).
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-RETEST-UAT.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-05-SUMMARY.md
@backend/src/second_brain/tools/classification.py
@backend/src/second_brain/agents/workflow.py
@backend/src/second_brain/main.py
@backend/src/second_brain/models/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Include inbox_doc_id in classify_and_file return string and emit CLASSIFIED event from adapter</name>
  <files>
    backend/src/second_brain/tools/classification.py
    backend/src/second_brain/agents/workflow.py
  </files>
  <action>
**Part A: Modify classify_and_file return string to include inbox_doc_id.**

In `backend/src/second_brain/tools/classification.py`, modify the return statements of `classify_and_file` to include the inbox_doc_id (matching the pattern used by `request_misunderstood` which returns "Misunderstood -> {uuid} | {text}"):

Change the two return lines (currently around lines 140-142):
```python
# Before:
if status == "pending":
    return f"Filed (needs review) \u2192 {bucket} ({confidence:.2f})"
return f"Filed \u2192 {bucket} ({confidence:.2f})"

# After:
if status == "pending":
    return f"Filed (needs review) \u2192 {bucket} ({confidence:.2f}) | {inbox_doc_id}"
return f"Filed \u2192 {bucket} ({confidence:.2f}) | {inbox_doc_id}"
```

The `| {inbox_doc_id}` suffix uses the same `|` separator convention as request_misunderstood. The UUID regex `_UUID_RE` in workflow.py will match it.

**Part B: Add CLASSIFIED custom event emission in the workflow adapter.**

In `backend/src/second_brain/agents/workflow.py`, in the `_stream_updates` method:

1. Add a `classified_inbox_id: str | None = None` tracker alongside the existing `misunderstood_inbox_id`.

2. In `_process_update`, after the existing `request_misunderstood` inbox_id extraction block, add a parallel block for `classify_and_file`:
```python
if detected_tool == "classify_and_file" and classified_inbox_id is None:
    iid = self._extract_inbox_id_from_result(update)
    if iid is not None:
        classified_inbox_id = iid
        logger.info("Extracted classified inbox_id: %s", iid)
```

Note: `_extract_inbox_id_from_result` already extracts UUIDs from function_result content. It will work for the updated return string.

3. Update `_process_update` return signature to include `classified_inbox_id`:
```python
def _process_update(
    self,
    update: AgentResponseUpdate,
    detected_tool: str | None,
    detected_tool_args: dict[str, Any],
    misunderstood_inbox_id: str | None,
    classified_inbox_id: str | None,
) -> tuple[str | None, dict[str, Any], str | None, str | None]:
```

And return the 4-tuple. Update all call sites in `_stream_updates` accordingly.

4. At the end of `_stream_updates` (in the custom event emission section), add a `CLASSIFIED` event emission after the existing `MISUNDERSTOOD` block:

In the existing `elif detected_tool in ("classify_and_file", "mark_as_junk"):` block, replace with:
```python
elif detected_tool == "classify_and_file":
    logger.info("Classification completed via classify_and_file")
    if classified_inbox_id:
        yield CustomEvent(
            name="CLASSIFIED",
            value={
                "inboxItemId": classified_inbox_id,
                "bucket": detected_tool_args.get("bucket", ""),
                "confidence": detected_tool_args.get("confidence", 0.0),
            },
        )
elif detected_tool == "mark_as_junk":
    logger.info("Classification completed via mark_as_junk")
```

5. Update the clean_result construction for classify_and_file to NOT include the ` | {uuid}` suffix in the text streamed to the client. The clean_result is constructed from `detected_tool_args` (not the return string), so it already won't include the UUID. Verify this is the case -- the current code at the end of `_stream_updates`:
```python
if detected_tool == "classify_and_file":
    bucket = detected_tool_args.get("bucket", "?")
    confidence = detected_tool_args.get("confidence", 0.0)
    ...
    clean_result = f"Filed \u2192 {bucket} ({confidence:.2f})"
```
This already constructs the text from args (not the tool return), so it won't include the UUID. Good -- no change needed here.
  </action>
  <verify>
Run `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v` -- all existing tests must pass.

Verify the return string includes inbox_doc_id:
```bash
grep -n "inbox_doc_id" backend/src/second_brain/tools/classification.py | head -5
```

Verify CLASSIFIED event is emitted:
```bash
grep -n "CLASSIFIED" backend/src/second_brain/agents/workflow.py
```
  </verify>
  <done>
(1) classify_and_file return string includes ` | {inbox_doc_id}` for ID extraction by the adapter.
(2) Workflow adapter extracts classified_inbox_id from function_result content.
(3) Workflow adapter emits CLASSIFIED CustomEvent with inboxItemId, bucket, and confidence.
(4) The clean result text streamed to the client does NOT include the UUID suffix.
(5) All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add post-stream orphan reconciliation to follow_up_misunderstood endpoint</name>
  <files>backend/src/second_brain/main.py</files>
  <action>
Rewrite the `generate()` inner function of `follow_up_misunderstood` to detect BOTH classification outcomes (CLASSIFIED and MISUNDERSTOOD) and reconcile orphaned documents with the original inbox item.

**Key pattern:** The `respond_to_hitl` endpoint already does this correctly -- it reads the existing inbox item, creates the new bucket doc, and then uses `upsert_item` to update the original. The follow-up endpoint should follow the same pattern but post-stream: let the workflow create the orphaned docs, then copy metadata to the original and clean up the orphans.

**Replace the `generate()` function body** (keeping the same SSE lifecycle events) with this logic:

```python
async def generate() -> AsyncGenerator[str, None]:
    encoder = EventEncoder()
    yield encoder.encode(RunStartedEvent(thread_id=thread_id, run_id=run_id))

    message_id_state: dict[str, str | None] = {"current": None}

    # Track outcomes from custom events
    classified_event_data: dict | None = None  # From CLASSIFIED event
    misunderstood_detected = False
    orphaned_inbox_item_id: str | None = None  # From MISUNDERSTOOD event

    async for item in stream:
        if isinstance(item, BaseEvent):
            if isinstance(item, CustomEvent):
                if item.name == "CLASSIFIED":
                    # Workflow successfully classified the combined text.
                    # Don't yield to client -- we'll handle reconciliation after stream.
                    classified_event_data = item.value if isinstance(item.value, dict) else {}
                    continue

                if item.name == "MISUNDERSTOOD":
                    if body.follow_up_round >= 2:
                        # Max rounds reached -- mark as unresolved
                        misunderstood_detected = True
                        orphaned_inbox_item_id = (
                            item.value.get("inboxItemId")
                            if isinstance(item.value, dict)
                            else None
                        )
                        continue
                    else:
                        # Round 1: still misunderstood. Intercept to update
                        # original item's clarificationText, then pass event
                        # to client for next follow-up round.
                        orphaned_inbox_item_id = (
                            item.value.get("inboxItemId")
                            if isinstance(item.value, dict)
                            else None
                        )
                        # Update original item's clarificationText
                        if cosmos_manager and orphaned_inbox_item_id:
                            try:
                                inbox_container = cosmos_manager.get_container("Inbox")
                                existing = await inbox_container.read_item(
                                    item=body.inbox_item_id, partition_key="will"
                                )
                                question_text = (
                                    item.value.get("questionText", "")
                                    if isinstance(item.value, dict)
                                    else ""
                                )
                                existing["clarificationText"] = question_text
                                existing["updatedAt"] = datetime.now(UTC).isoformat()
                                await inbox_container.upsert_item(body=existing)
                                logger.info(
                                    "Updated original %s clarificationText",
                                    body.inbox_item_id,
                                )
                                # Delete the orphaned new inbox doc
                                if orphaned_inbox_item_id != body.inbox_item_id:
                                    await inbox_container.delete_item(
                                        item=orphaned_inbox_item_id,
                                        partition_key="will",
                                    )
                                    logger.info(
                                        "Deleted orphan misunderstood inbox %s",
                                        orphaned_inbox_item_id,
                                    )
                            except Exception:
                                logger.warning(
                                    "Could not reconcile misunderstood round 1 for %s",
                                    body.inbox_item_id,
                                )
                        # Emit the MISUNDERSTOOD event to client (with ORIGINAL item ID)
                        yield encoder.encode(
                            CustomEvent(
                                name="MISUNDERSTOOD",
                                value={
                                    "threadId": thread_id,
                                    "inboxItemId": body.inbox_item_id,
                                    "questionText": (
                                        item.value.get("questionText", "")
                                        if isinstance(item.value, dict)
                                        else ""
                                    ),
                                },
                            )
                        )
                        continue

            yield encoder.encode(item)
        elif isinstance(item, AgentResponseUpdate):
            for event in _convert_update_to_events(item, message_id_state):
                yield encoder.encode(event)

    # Close open text message
    if message_id_state.get("current"):
        yield encoder.encode(
            TextMessageEndEvent(message_id=message_id_state["current"])
        )

    # --- Post-stream orphan reconciliation ---

    if classified_event_data and cosmos_manager:
        # Workflow created a new inbox doc + bucket doc via classify_and_file.
        # Copy classification metadata to original, delete orphaned inbox doc,
        # update bucket doc's inboxRecordId.
        orphan_inbox_id = classified_event_data.get("inboxItemId", "")
        bucket = classified_event_data.get("bucket", "")
        try:
            inbox_container = cosmos_manager.get_container("Inbox")

            # Read the orphaned inbox doc to get its classification metadata
            orphan_doc = await inbox_container.read_item(
                item=orphan_inbox_id, partition_key="will"
            )
            classification_meta = orphan_doc.get("classificationMeta")
            filed_record_id = orphan_doc.get("filedRecordId")
            title = orphan_doc.get("title")

            # Read the original inbox doc
            existing = await inbox_container.read_item(
                item=body.inbox_item_id, partition_key="will"
            )

            # Copy classification to original
            existing["classificationMeta"] = classification_meta
            existing["filedRecordId"] = filed_record_id
            existing["title"] = title
            existing["status"] = orphan_doc.get("status", "classified")
            existing["updatedAt"] = datetime.now(UTC).isoformat()
            await inbox_container.upsert_item(body=existing)
            logger.info(
                "Copied classification to original %s from orphan %s",
                body.inbox_item_id,
                orphan_inbox_id,
            )

            # Delete the orphaned inbox doc
            if orphan_inbox_id and orphan_inbox_id != body.inbox_item_id:
                await inbox_container.delete_item(
                    item=orphan_inbox_id, partition_key="will"
                )
                logger.info("Deleted orphan inbox doc %s", orphan_inbox_id)

            # Update the bucket doc's inboxRecordId to point to original
            if filed_record_id and bucket:
                try:
                    bucket_container = cosmos_manager.get_container(bucket)
                    bucket_doc = await bucket_container.read_item(
                        item=filed_record_id, partition_key="will"
                    )
                    bucket_doc["inboxRecordId"] = body.inbox_item_id
                    await bucket_container.upsert_item(body=bucket_doc)
                    logger.info(
                        "Updated bucket doc %s inboxRecordId to %s",
                        filed_record_id,
                        body.inbox_item_id,
                    )
                except Exception:
                    logger.warning(
                        "Could not update bucket doc %s inboxRecordId",
                        filed_record_id,
                    )

        except Exception:
            logger.exception(
                "Failed post-stream reconciliation for inbox %s",
                body.inbox_item_id,
            )

    elif misunderstood_detected:
        # Max rounds reached -- update original to unresolved + clean up orphan
        # (Same logic as existing code, kept here for completeness)
        if cosmos_manager:
            try:
                inbox_container = cosmos_manager.get_container("Inbox")
                existing = await inbox_container.read_item(
                    item=body.inbox_item_id, partition_key="will"
                )
                existing["status"] = "unresolved"
                existing["updatedAt"] = datetime.now(UTC).isoformat()
                await inbox_container.upsert_item(body=existing)
            except Exception:
                logger.warning(
                    "Could not update inbox item %s to unresolved",
                    body.inbox_item_id,
                )

            if (
                orphaned_inbox_item_id
                and orphaned_inbox_item_id != body.inbox_item_id
            ):
                try:
                    await inbox_container.delete_item(
                        item=orphaned_inbox_item_id, partition_key="will"
                    )
                    logger.info(
                        "Deleted orphaned misunderstood inbox item %s",
                        orphaned_inbox_item_id,
                    )
                except Exception:
                    logger.warning(
                        "Could not delete orphaned inbox item %s",
                        orphaned_inbox_item_id,
                    )

        yield encoder.encode(
            CustomEvent(
                name="UNRESOLVED",
                value={"inboxItemId": body.inbox_item_id},
            )
        )

    yield encoder.encode(RunFinishedEvent(thread_id=thread_id, run_id=run_id))
```

**IMPORTANT implementation notes:**

1. The CLASSIFIED event is NOT yielded to the client. It's only used internally for reconciliation. The client already receives the text message "Filed -> Bucket (0.XX)" via the SSE stream from the adapter's clean_result.

2. For the round 1 MISUNDERSTOOD case: the endpoint intercepts the event, updates the original item's clarificationText, deletes the orphan, then re-emits a MISUNDERSTOOD event to the client with the ORIGINAL inbox_item_id (not the orphan's). This ensures the client's subsequent follow-up uses the correct ID.

3. The bucket doc's inboxRecordId is updated to point to the original inbox item (not the orphaned one). This maintains the bi-directional link.

4. Error handling is non-fatal throughout -- if any reconciliation step fails, the endpoint logs a warning but still completes the SSE stream. The orphan is harmless (just extra data).
  </action>
  <verify>
Run `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v` -- all existing tests must pass.

Verify the reconciliation code is present:
```bash
grep -n "orphan_reconciliation\|Copied classification to original\|Deleted orphan inbox" backend/src/second_brain/main.py | head -5
```

Verify CLASSIFIED event handling:
```bash
grep -n "CLASSIFIED" backend/src/second_brain/main.py | head -5
```
  </verify>
  <done>
(1) When follow-up classifies successfully (CLASSIFIED event): original inbox item is updated with classification metadata, orphaned inbox doc is deleted, bucket doc inboxRecordId points to original.
(2) When follow-up triggers misunderstood at round 1: original item's clarificationText is updated, orphaned inbox doc is deleted, MISUNDERSTOOD event re-emitted with original item ID.
(3) When follow-up triggers misunderstood at round >= 2: original item marked unresolved, orphan deleted, UNRESOLVED event emitted.
(4) No duplicate inbox items created by follow-up flow.
(5) All existing tests pass.
  </done>
</task>

</tasks>

<verification>
- `python3 -m pytest tests/ -v` in backend directory: all tests pass
- classify_and_file return string includes ` | {inbox_doc_id}` UUID
- Workflow adapter emits CLASSIFIED CustomEvent with inboxItemId
- follow_up_misunderstood endpoint:
  - On CLASSIFIED: copies metadata to original, deletes orphan inbox, updates bucket doc link
  - On MISUNDERSTOOD round 1: updates original clarificationText, deletes orphan, re-emits with original ID
  - On MISUNDERSTOOD round >= 2: marks original unresolved, deletes orphan, emits UNRESOLVED
</verification>

<success_criteria>
- Follow-up reply that successfully classifies updates the original inbox item and deletes orphan
- Follow-up reply that triggers misunderstood updates original's question and deletes orphan
- No duplicate inbox items appear in Cosmos DB after follow-up flow
- Bi-directional links (inbox <-> bucket) point to correct original item
- All existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-08-SUMMARY.md`
</output>
