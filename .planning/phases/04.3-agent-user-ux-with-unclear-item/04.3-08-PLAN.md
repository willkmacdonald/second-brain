---
phase: 04.3-agent-user-ux-with-unclear-item
plan: 08
type: execute
wave: 2
depends_on: ["07"]
files_modified:
  - backend/src/second_brain/tools/classification.py
  - backend/src/second_brain/agents/workflow.py
  - backend/src/second_brain/main.py
autonomous: true
requirements: [CLAS-04, APPX-04]
gap_closure: true

must_haves:
  truths:
    - "Follow-up reply that successfully classifies updates the ORIGINAL inbox item (not creates a new one)"
    - "The orphaned inbox doc created by classify_and_file during follow-up is deleted after its metadata is copied to the original"
    - "The bucket doc created during follow-up has its inboxRecordId updated to point to the original inbox item"
    - "Follow-up reply that triggers request_misunderstood at round 1 updates the original item's clarificationText and deletes the orphan"
    - "No duplicate inbox items appear after follow-up classification"
  artifacts:
    - path: "backend/src/second_brain/main.py"
      provides: "Post-stream orphan reconciliation in follow_up_misunderstood endpoint"
      contains: "orphan_reconciliation"
    - path: "backend/src/second_brain/tools/classification.py"
      provides: "classify_and_file return string includes inbox_doc_id"
      contains: "inbox_doc_id"
    - path: "backend/src/second_brain/agents/workflow.py"
      provides: "CLASSIFIED custom event with orphan inbox ID"
      contains: "CLASSIFIED"
  key_links:
    - from: "backend/src/second_brain/agents/workflow.py"
      to: "backend/src/second_brain/main.py"
      via: "CLASSIFIED CustomEvent carries orphaned inbox_doc_id for follow-up endpoint reconciliation"
      pattern: "CustomEvent.*CLASSIFIED"
    - from: "backend/src/second_brain/main.py"
      to: "CosmosManager.upsert_item"
      via: "follow_up_misunderstood copies classification to original and deletes orphan"
      pattern: "upsert_item.*existing"
---

<objective>
Fix follow-up endpoint creating duplicate inbox items instead of updating the original. The workflow tools (classify_and_file, request_misunderstood) always create NEW docs with uuid4(). The follow-up endpoint must detect the outcome, copy metadata to the original inbox item, and clean up orphaned docs.

Purpose: UAT Tests 3, 4, and 7 fail because follow-up creates duplicates. The original misunderstood item stays stuck forever, and a new duplicate appears. This breaks the entire follow-up UX.
Output: Updated classify_and_file (include inbox_doc_id in return), updated workflow adapter (emit CLASSIFIED event), updated follow-up endpoint (post-stream orphan reconciliation).

NOTE: This plan depends on Plan 07 which modifies classify_and_file's score validation/fallback logic in the same file. Plan 07 must complete first.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-RETEST-UAT.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-05-SUMMARY.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-07-SUMMARY.md
@backend/src/second_brain/tools/classification.py
@backend/src/second_brain/agents/workflow.py
@backend/src/second_brain/main.py
@backend/src/second_brain/models/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1a: Include inbox_doc_id in classify_and_file return string</name>
  <files>backend/src/second_brain/tools/classification.py</files>
  <action>
**Modify classify_and_file return statements to include inbox_doc_id.**

In `backend/src/second_brain/tools/classification.py`, modify the two return statements at the end of `classify_and_file` to append ` | {inbox_doc_id}` (matching the pattern used by `request_misunderstood` which returns "Misunderstood -> {uuid} | {text}"):

Change the two return lines (currently around lines 140-142, but may have shifted after Plan 07's changes):
```python
# Before:
if status == "pending":
    return f"Filed (needs review) \u2192 {bucket} ({confidence:.2f})"
return f"Filed \u2192 {bucket} ({confidence:.2f})"

# After:
if status == "pending":
    return f"Filed (needs review) \u2192 {bucket} ({confidence:.2f}) | {inbox_doc_id}"
return f"Filed \u2192 {bucket} ({confidence:.2f}) | {inbox_doc_id}"
```

The `| {inbox_doc_id}` suffix uses the same `|` separator convention as request_misunderstood. The UUID regex `_UUID_RE` in workflow.py will match it.
  </action>
  <verify>
Run `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v` -- all existing tests must pass.

Verify the return string includes inbox_doc_id:
```bash
grep -n "inbox_doc_id" backend/src/second_brain/tools/classification.py | head -5
```
  </verify>
  <done>
classify_and_file return strings include ` | {inbox_doc_id}` UUID suffix for extraction by the workflow adapter.
  </done>
</task>

<task type="auto">
  <name>Task 1b: Emit CLASSIFIED event from workflow adapter with extracted inbox ID</name>
  <files>backend/src/second_brain/agents/workflow.py</files>
  <action>
Three changes to the workflow adapter in `backend/src/second_brain/agents/workflow.py`:

**Change 1: Add `classified_inbox_id` tracker and update `_process_update` signature.**

Update `_process_update` method (currently at line 215) to accept and return a 4th element:

```python
def _process_update(
    self,
    update: AgentResponseUpdate,
    detected_tool: str | None,
    detected_tool_args: dict[str, Any],
    misunderstood_inbox_id: str | None,
    classified_inbox_id: str | None,
) -> tuple[str | None, dict[str, Any], str | None, str | None]:
```

After the existing `request_misunderstood` inbox_id extraction block (lines 232-236), add a parallel block for `classify_and_file`:
```python
if detected_tool == "classify_and_file" and classified_inbox_id is None:
    iid = self._extract_inbox_id_from_result(update)
    if iid is not None:
        classified_inbox_id = iid
        logger.info("Extracted classified inbox_id: %s", iid)
```

Update the return statement to return the 4-tuple:
```python
return detected_tool, detected_tool_args, misunderstood_inbox_id, classified_inbox_id
```

**Change 2: Update BOTH call sites in `_stream_updates` to destructure the 4-tuple.**

In `_stream_updates`, initialize the new tracker alongside `misunderstood_inbox_id` (around line 267):
```python
classified_inbox_id: str | None = None
```

There are exactly two call sites that must be updated:

Call site 1 (inside the `WorkflowEvent` branch, around lines 316-324):
```python
# BEFORE (3-tuple):
result = self._process_update(
    update, detected_tool, detected_tool_args, misunderstood_inbox_id,
)
detected_tool = result[0]
detected_tool_args = result[1]
misunderstood_inbox_id = result[2]

# AFTER (4-tuple):
result = self._process_update(
    update, detected_tool, detected_tool_args,
    misunderstood_inbox_id, classified_inbox_id,
)
detected_tool = result[0]
detected_tool_args = result[1]
misunderstood_inbox_id = result[2]
classified_inbox_id = result[3]
```

Call site 2 (inside the `AgentResponseUpdate` branch, around lines 344-351):
```python
# BEFORE (3-tuple):
detected_tool, detected_tool_args, misunderstood_inbox_id = (
    self._process_update(
        event, detected_tool, detected_tool_args, misunderstood_inbox_id,
    )
)

# AFTER (4-tuple):
detected_tool, detected_tool_args, misunderstood_inbox_id, classified_inbox_id = (
    self._process_update(
        event, detected_tool, detected_tool_args,
        misunderstood_inbox_id, classified_inbox_id,
    )
)
```

**Change 3: Emit CLASSIFIED custom event at stream end.**

In the custom event emission section at the end of `_stream_updates` (around line 413), replace the existing `elif detected_tool in ("classify_and_file", "mark_as_junk"):` block with two separate branches:

```python
elif detected_tool == "classify_and_file":
    logger.info("Classification completed via classify_and_file")
    if classified_inbox_id:
        yield CustomEvent(
            name="CLASSIFIED",
            value={
                "inboxItemId": classified_inbox_id,
                "bucket": detected_tool_args.get("bucket", ""),
                "confidence": detected_tool_args.get("confidence", 0.0),
            },
        )
elif detected_tool == "mark_as_junk":
    logger.info("Classification completed via mark_as_junk")
```

The clean_result construction (around line 370) already builds text from `detected_tool_args` (not the tool return string), so it will NOT include the UUID suffix. No change needed there.
  </action>
  <verify>
Run `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v` -- all existing tests must pass.

Verify CLASSIFIED event is emitted:
```bash
grep -n "CLASSIFIED" backend/src/second_brain/agents/workflow.py
```

Verify both call sites destructure 4-tuple:
```bash
grep -n "classified_inbox_id" backend/src/second_brain/agents/workflow.py
```
  </verify>
  <done>
(1) `_process_update` accepts and returns `classified_inbox_id` as 4th element.
(2) Both call sites in `_stream_updates` updated: WorkflowEvent branch (around line 316) and AgentResponseUpdate branch (around line 344) both destructure the 4-tuple.
(3) CLASSIFIED CustomEvent emitted at stream end with inboxItemId, bucket, and confidence.
(4) Clean result text streamed to client does NOT include the UUID suffix.
(5) All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add post-stream orphan reconciliation to follow_up_misunderstood endpoint</name>
  <files>backend/src/second_brain/main.py</files>
  <action>
Rewrite the `generate()` inner function of `follow_up_misunderstood` to detect BOTH classification outcomes (CLASSIFIED and MISUNDERSTOOD) and reconcile orphaned documents with the original inbox item.

**Key pattern:** The `respond_to_hitl` endpoint already does this correctly -- it reads the existing inbox item, creates the new bucket doc, and then uses `upsert_item` to update the original. The follow-up endpoint should follow the same pattern but post-stream: let the workflow create the orphaned docs, then copy metadata to the original and clean up the orphans.

**Replace the `generate()` function body** (keeping the same SSE lifecycle events) with this logic:

```python
async def generate() -> AsyncGenerator[str, None]:
    encoder = EventEncoder()
    yield encoder.encode(RunStartedEvent(thread_id=thread_id, run_id=run_id))

    message_id_state: dict[str, str | None] = {"current": None}

    # Track outcomes from custom events
    classified_event_data: dict | None = None  # From CLASSIFIED event
    misunderstood_detected = False
    orphaned_inbox_item_id: str | None = None  # From MISUNDERSTOOD event

    async for item in stream:
        if isinstance(item, BaseEvent):
            if isinstance(item, CustomEvent):
                if item.name == "CLASSIFIED":
                    # Workflow successfully classified the combined text.
                    # Don't yield to client -- we'll handle reconciliation after stream.
                    classified_event_data = item.value if isinstance(item.value, dict) else {}
                    continue

                if item.name == "MISUNDERSTOOD":
                    if body.follow_up_round >= 2:
                        # Max rounds reached -- mark as unresolved
                        misunderstood_detected = True
                        orphaned_inbox_item_id = (
                            item.value.get("inboxItemId")
                            if isinstance(item.value, dict)
                            else None
                        )
                        continue
                    else:
                        # Round 1: still misunderstood. Intercept to update
                        # original item's clarificationText, then pass event
                        # to client for next follow-up round.
                        orphaned_inbox_item_id = (
                            item.value.get("inboxItemId")
                            if isinstance(item.value, dict)
                            else None
                        )
                        # Update original item's clarificationText
                        if cosmos_manager and orphaned_inbox_item_id:
                            try:
                                inbox_container = cosmos_manager.get_container("Inbox")
                                existing = await inbox_container.read_item(
                                    item=body.inbox_item_id, partition_key="will"
                                )
                                question_text = (
                                    item.value.get("questionText", "")
                                    if isinstance(item.value, dict)
                                    else ""
                                )
                                existing["clarificationText"] = question_text
                                existing["updatedAt"] = datetime.now(UTC).isoformat()
                                await inbox_container.upsert_item(body=existing)
                                logger.info(
                                    "Updated original %s clarificationText",
                                    body.inbox_item_id,
                                )
                                # Delete the orphaned new inbox doc
                                if orphaned_inbox_item_id != body.inbox_item_id:
                                    await inbox_container.delete_item(
                                        item=orphaned_inbox_item_id,
                                        partition_key="will",
                                    )
                                    logger.info(
                                        "Deleted orphan misunderstood inbox %s",
                                        orphaned_inbox_item_id,
                                    )
                            except Exception:
                                logger.warning(
                                    "Could not reconcile misunderstood round 1 for %s",
                                    body.inbox_item_id,
                                )
                        # Emit the MISUNDERSTOOD event to client (with ORIGINAL item ID)
                        yield encoder.encode(
                            CustomEvent(
                                name="MISUNDERSTOOD",
                                value={
                                    "threadId": thread_id,
                                    "inboxItemId": body.inbox_item_id,
                                    "questionText": (
                                        item.value.get("questionText", "")
                                        if isinstance(item.value, dict)
                                        else ""
                                    ),
                                },
                            )
                        )
                        continue

            yield encoder.encode(item)
        elif isinstance(item, AgentResponseUpdate):
            for event in _convert_update_to_events(item, message_id_state):
                yield encoder.encode(event)

    # Close open text message
    if message_id_state.get("current"):
        yield encoder.encode(
            TextMessageEndEvent(message_id=message_id_state["current"])
        )

    # --- Post-stream orphan reconciliation ---

    if classified_event_data and cosmos_manager:
        # Workflow created a new inbox doc + bucket doc via classify_and_file.
        # Copy classification metadata to original, delete orphaned inbox doc,
        # update bucket doc's inboxRecordId.
        orphan_inbox_id = classified_event_data.get("inboxItemId", "")
        bucket = classified_event_data.get("bucket", "")
        try:
            inbox_container = cosmos_manager.get_container("Inbox")

            # Read the orphaned inbox doc to get its classification metadata
            orphan_doc = await inbox_container.read_item(
                item=orphan_inbox_id, partition_key="will"
            )
            classification_meta = orphan_doc.get("classificationMeta")
            filed_record_id = orphan_doc.get("filedRecordId")
            title = orphan_doc.get("title")

            # Read the original inbox doc
            existing = await inbox_container.read_item(
                item=body.inbox_item_id, partition_key="will"
            )

            # Copy classification to original
            existing["classificationMeta"] = classification_meta
            existing["filedRecordId"] = filed_record_id
            existing["title"] = title
            existing["status"] = orphan_doc.get("status", "classified")
            existing["updatedAt"] = datetime.now(UTC).isoformat()
            await inbox_container.upsert_item(body=existing)
            logger.info(
                "Copied classification to original %s from orphan %s",
                body.inbox_item_id,
                orphan_inbox_id,
            )

            # Delete the orphaned inbox doc
            if orphan_inbox_id and orphan_inbox_id != body.inbox_item_id:
                await inbox_container.delete_item(
                    item=orphan_inbox_id, partition_key="will"
                )
                logger.info("Deleted orphan inbox doc %s", orphan_inbox_id)

            # Update the bucket doc's inboxRecordId to point to original
            if filed_record_id and bucket:
                try:
                    bucket_container = cosmos_manager.get_container(bucket)
                    bucket_doc = await bucket_container.read_item(
                        item=filed_record_id, partition_key="will"
                    )
                    bucket_doc["inboxRecordId"] = body.inbox_item_id
                    await bucket_container.upsert_item(body=bucket_doc)
                    logger.info(
                        "Updated bucket doc %s inboxRecordId to %s",
                        filed_record_id,
                        body.inbox_item_id,
                    )
                except Exception:
                    logger.warning(
                        "Could not update bucket doc %s inboxRecordId",
                        filed_record_id,
                    )

        except Exception:
            logger.exception(
                "Failed post-stream reconciliation for inbox %s",
                body.inbox_item_id,
            )

    elif misunderstood_detected:
        # Max rounds reached -- update original to unresolved + clean up orphan
        if cosmos_manager:
            try:
                inbox_container = cosmos_manager.get_container("Inbox")
                existing = await inbox_container.read_item(
                    item=body.inbox_item_id, partition_key="will"
                )
                existing["status"] = "unresolved"
                existing["updatedAt"] = datetime.now(UTC).isoformat()
                await inbox_container.upsert_item(body=existing)
            except Exception:
                logger.warning(
                    "Could not update inbox item %s to unresolved",
                    body.inbox_item_id,
                )

            if (
                orphaned_inbox_item_id
                and orphaned_inbox_item_id != body.inbox_item_id
            ):
                try:
                    await inbox_container.delete_item(
                        item=orphaned_inbox_item_id, partition_key="will"
                    )
                    logger.info(
                        "Deleted orphaned misunderstood inbox item %s",
                        orphaned_inbox_item_id,
                    )
                except Exception:
                    logger.warning(
                        "Could not delete orphaned inbox item %s",
                        orphaned_inbox_item_id,
                    )

        yield encoder.encode(
            CustomEvent(
                name="UNRESOLVED",
                value={"inboxItemId": body.inbox_item_id},
            )
        )

    yield encoder.encode(RunFinishedEvent(thread_id=thread_id, run_id=run_id))
```

**IMPORTANT implementation notes:**

1. The CLASSIFIED event is NOT yielded to the client. It's only used internally for reconciliation. The client already receives the text message "Filed -> Bucket (0.XX)" via the SSE stream from the adapter's clean_result.

2. For the round 1 MISUNDERSTOOD case: the endpoint intercepts the event, updates the original item's clarificationText, deletes the orphan, then re-emits a MISUNDERSTOOD event to the client with the ORIGINAL inbox_item_id (not the orphan's). This ensures the client's subsequent follow-up uses the correct ID.

3. The bucket doc's inboxRecordId is updated to point to the original inbox item (not the orphaned one). This maintains the bi-directional link.

4. Error handling is non-fatal throughout -- if any reconciliation step fails, the endpoint logs a warning but still completes the SSE stream. The orphan is harmless (just extra data).
  </action>
  <verify>
Run `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v` -- all existing tests must pass.

Verify all three reconciliation paths are present:

Path 1 - CLASSIFIED (successful follow-up classification):
```bash
grep -n "classified_event_data" backend/src/second_brain/main.py | head -5
```
Should show: tracking variable, CLASSIFIED event interception, post-stream reconciliation block.

Path 2 - MISUNDERSTOOD round 1 (still unclear, ask again):
```bash
grep -n "clarificationText\|round 1" backend/src/second_brain/main.py | head -5
```
Should show: original item clarificationText update, orphan deletion, re-emit with original ID.

Path 3 - MISUNDERSTOOD round >= 2 (max rounds, mark unresolved):
```bash
grep -n "unresolved\|UNRESOLVED" backend/src/second_brain/main.py | head -5
```
Should show: original item status set to "unresolved", orphan deletion, UNRESOLVED event emitted.

Verify orphan cleanup is present in all paths:
```bash
grep -c "delete_item" backend/src/second_brain/main.py
```
Should show at least 3 delete_item calls (one per reconciliation path, plus existing cascade delete in the DELETE endpoint).
  </verify>
  <done>
(1) When follow-up classifies successfully (CLASSIFIED event): original inbox item is updated with classification metadata, orphaned inbox doc is deleted, bucket doc inboxRecordId points to original.
(2) When follow-up triggers misunderstood at round 1: original item's clarificationText is updated, orphaned inbox doc is deleted, MISUNDERSTOOD event re-emitted with original item ID.
(3) When follow-up triggers misunderstood at round >= 2: original item marked unresolved, orphan deleted, UNRESOLVED event emitted.
(4) No duplicate inbox items created by follow-up flow.
(5) All existing tests pass.
  </done>
</task>

</tasks>

<verification>
- `python3 -m pytest tests/ -v` in backend directory: all tests pass
- classify_and_file return string includes ` | {inbox_doc_id}` UUID
- Workflow adapter emits CLASSIFIED CustomEvent with inboxItemId
- Both `_process_update` call sites destructure the 4-tuple correctly
- follow_up_misunderstood endpoint:
  - On CLASSIFIED: copies metadata to original, deletes orphan inbox, updates bucket doc link
  - On MISUNDERSTOOD round 1: updates original clarificationText, deletes orphan, re-emits with original ID
  - On MISUNDERSTOOD round >= 2: marks original unresolved, deletes orphan, emits UNRESOLVED
</verification>

<success_criteria>
- Follow-up reply that successfully classifies updates the original inbox item and deletes orphan
- Follow-up reply that triggers misunderstood updates original's question and deletes orphan
- No duplicate inbox items appear in Cosmos DB after follow-up flow
- Bi-directional links (inbox <-> bucket) point to correct original item
- All existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-08-SUMMARY.md`
</output>
