---
phase: 04.3-agent-user-ux-with-unclear-item
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/second_brain/tools/classification.py
  - backend/src/second_brain/agents/classifier.py
autonomous: true
requirements: [CLAS-04, APPX-04]
gap_closure: true

must_haves:
  truths:
    - "When LLM provides valid four bucket scores, they are used as-is in allScores"
    - "When four bucket scores are all 0.0 (Agent Framework stripped them), allScores are derived from confidence + bucket"
    - "When confidence itself is 0.0 but a valid bucket was chosen, a reasonable default confidence (0.75) is applied"
    - "Debug logging shows actual received argument values for every classify_and_file call"
  artifacts:
    - path: "backend/src/second_brain/tools/classification.py"
      provides: "Score validation and fallback logic in classify_and_file"
      contains: "_derive_scores_from_bucket"
  key_links:
    - from: "backend/src/second_brain/tools/classification.py"
      to: "backend/src/second_brain/models/documents.py"
      via: "ClassificationMeta.allScores receives validated non-zero scores"
      pattern: "all_scores.*People.*Projects.*Ideas.*Admin"
---

<objective>
Fix 0.00 confidence scores in classify_and_file tool. When the Agent Framework strips or defaults the four individual score parameters (people_score, projects_score, ideas_score, admin_score), all scores resolve to 0.0 and allScores is built from these zeros. Add debug logging and a validation/fallback that derives reasonable scores from confidence + bucket when the individual params are all 0.0.

Purpose: UAT Tests 2, 3, and 7 all show 0.00 confidence for every classification. Without meaningful scores, the UI shows broken confidence values and the threshold logic mis-routes items.
Output: Updated classification.py with score validation and fallback, updated classifier instructions.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-RETEST-UAT.md
@backend/src/second_brain/tools/classification.py
@backend/src/second_brain/agents/classifier.py
@backend/src/second_brain/models/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add debug logging and score validation/fallback to classify_and_file</name>
  <files>backend/src/second_brain/tools/classification.py</files>
  <action>
Two changes to `classify_and_file` in ClassificationTools:

**Change 1: Add debug logging of received arguments.**

Immediately after the method signature (before the bucket validation), add a debug log that dumps all received argument values:

```python
logger.debug(
    "classify_and_file called: bucket=%s confidence=%.4f "
    "people=%.4f projects=%.4f ideas=%.4f admin=%.4f raw_text=%s title=%s",
    bucket, confidence, people_score, projects_score, ideas_score, admin_score,
    raw_text[:80], title,
)
```

This will show in server logs what the Agent Framework actually passes to the tool.

**Change 2: Add score validation and fallback.**

Add a module-level helper function `_derive_scores_from_bucket` that takes `bucket: str` and `confidence: float` and returns a `dict[str, float]` with synthetic allScores:
- The primary bucket gets the confidence value
- The remaining (1.0 - confidence) is split evenly among the other 3 buckets
- Example: bucket="Admin", confidence=0.85 -> {"People": 0.05, "Projects": 0.05, "Ideas": 0.05, "Admin": 0.85}

In classify_and_file, AFTER the confidence clamping (line 72) and BEFORE building allScores (line 74-80):

1. Check if all four scores are 0.0:
   ```python
   scores_provided = not (
       people_score == 0.0
       and projects_score == 0.0
       and ideas_score == 0.0
       and admin_score == 0.0
   )
   ```

2. If confidence itself is also 0.0 but bucket is valid, apply a default:
   ```python
   if confidence == 0.0 and bucket in VALID_BUCKETS:
       confidence = 0.75
       logger.warning(
           "confidence was 0.0 with valid bucket '%s' -- defaulting to 0.75",
           bucket,
       )
   ```

3. Build allScores with validation:
   ```python
   if scores_provided:
       all_scores = {
           "People": people_score,
           "Projects": projects_score,
           "Ideas": ideas_score,
           "Admin": admin_score,
       }
   else:
       all_scores = _derive_scores_from_bucket(bucket, confidence)
       logger.warning(
           "All four scores were 0.0 -- derived from bucket=%s confidence=%.2f: %s",
           bucket, confidence, all_scores,
       )
   ```

This replaces the current lines 74-80 that build allScores directly from the param values.

**Also make the four score parameters optional** by giving them default values of 0.0 in the tool signature. They already effectively default to 0.0 when the Agent Framework strips them, but making them explicitly optional with defaults documents the behavior:

```python
people_score: Annotated[float, "Score 0.0-1.0 for People bucket"] = 0.0,
projects_score: Annotated[float, "Score 0.0-1.0 for Projects bucket"] = 0.0,
ideas_score: Annotated[float, "Score 0.0-1.0 for Ideas bucket"] = 0.0,
admin_score: Annotated[float, "Score 0.0-1.0 for Admin bucket"] = 0.0,
```

This way the tool still ACCEPTS scores if the LLM provides them, but gracefully handles the case when they're missing.
  </action>
  <verify>
Run `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v` -- all existing tests must pass. Then verify the fallback function with a quick test:

```python
python3 -c "
from second_brain.tools.classification import _derive_scores_from_bucket
scores = _derive_scores_from_bucket('Admin', 0.85)
assert scores['Admin'] == 0.85, f'Expected 0.85, got {scores[\"Admin\"]}'
assert abs(scores['People'] - 0.05) < 0.01, f'Expected ~0.05, got {scores[\"People\"]}'
print('Score derivation OK:', scores)
"
```
  </verify>
  <done>
(1) classify_and_file logs all received argument values at debug level.
(2) When all four score params are 0.0, allScores are derived from confidence + bucket with a warning log.
(3) When confidence is 0.0 with a valid bucket, it defaults to 0.75 with a warning log.
(4) All existing backend tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update classifier instructions to de-emphasize individual scores</name>
  <files>backend/src/second_brain/agents/classifier.py</files>
  <action>
The classifier instructions currently include rule 4: "You MUST provide ALL FOUR bucket scores (people_score, projects_score, ideas_score, admin_score) that sum roughly to 1.0". Since the scores now have a fallback and are optional, soften this instruction to reduce friction:

In the Rules section of the classifier instructions string, change rule 4 from:
```
"4. You MUST provide ALL FOUR bucket scores (people_score, "
"projects_score, ideas_score, admin_score) that sum roughly to 1.0\n"
```
to:
```
"4. Provide ALL FOUR bucket scores (people_score, projects_score, "
"ideas_score, admin_score) that sum roughly to 1.0 when possible. "
"If omitted, scores are derived from your confidence and chosen bucket\n"
```

This tells the LLM to still try providing scores, but doesn't make it a hard requirement that could cause failures.
  </action>
  <verify>
Verify the instruction string was updated correctly by checking the file contains "If omitted, scores are derived":

```bash
cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && grep -q "If omitted, scores are derived" src/second_brain/agents/classifier.py && echo "OK" || echo "MISSING"
```

Run tests: `python3 -m pytest tests/ -v`
  </verify>
  <done>
Classifier instructions updated to treat individual bucket scores as preferred but optional, matching the new fallback behavior in classify_and_file.
  </done>
</task>

</tasks>

<verification>
- `python3 -m pytest tests/ -v` in backend directory: all tests pass
- classify_and_file logs argument values at debug level
- When all four scores are 0.0, allScores derived from confidence + bucket (non-zero values)
- When confidence is also 0.0 with valid bucket, defaults to 0.75
- Classifier instructions de-emphasize strict score requirement
</verification>

<success_criteria>
- No more 0.00 allScores in Cosmos DB documents when the Agent Framework strips score parameters
- classify_and_file produces meaningful confidence values for all classifications
- Existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/04.3-agent-user-ux-with-unclear-item/04.3-07-SUMMARY.md`
</output>
