# Phase 04.1: Backend Deployment to Azure Container Apps - Research

**Researched:** 2026-02-23
**Domain:** Azure Container Apps, Docker containerization, CI/CD with GitHub Actions
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

#### Access and networking
- Public endpoint, secured by existing API key auth middleware (built in Phase 1)
- Custom domain with managed TLS certificate (user will provide domain when ready)
- HTTPS enforced -- redirect all HTTP to HTTPS
- Azure Container Apps free managed cert for TLS

#### Configuration and secrets
- Secrets (Cosmos connection string, OpenAI key, API key) pulled from Azure Key Vault
- Key Vault already provisioned -- configure access from Container App
- System-assigned managed identity on Container App for Key Vault authentication
- Non-secret config (COSMOS_DATABASE_NAME, API version, etc.) as plain environment variables on the Container App
- Existing Key Vault fetch in lifespan code should work with managed identity's DefaultAzureCredential

#### Container build and registry
- Azure Container Registry (ACR) -- already provisioned
- Managed identity pull from Container Apps to ACR
- Dockerfile uses uv for dependency management (multi-stage build, matches local dev tooling)
- Images tagged with short git commit SHA (immutable, traceable to source)

#### CI/CD and deployment flow
- GitHub Actions pipeline for automated deployment
- Trigger: push to main branch with path filter (backend/ changes only)
- OIDC (Workload Identity Federation) for GitHub Actions -> Azure authentication (no stored secrets)
- Requires one-time setup: Azure app registration + federated credential for the GitHub repo

### Claude's Discretion
- Container App scaling configuration (min/max replicas)
- Health check endpoint configuration
- Resource allocation (CPU/memory)
- Multi-stage Dockerfile optimization details
- GitHub Actions workflow structure and job naming

### Deferred Ideas (OUT OF SCOPE)
None -- discussion stayed within phase scope
</user_constraints>

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| INFRA-01 | AG-UI server running on Azure Container Apps | Full deployment pipeline researched: Dockerfile, Container App creation, CI/CD, secrets, networking |
</phase_requirements>

## Summary

This phase deploys the existing FastAPI backend to Azure Container Apps. The backend is a Python 3.12 FastAPI application using the Agent Framework AG-UI library, Cosmos DB, and Azure OpenAI. It currently runs locally on port 8003 with `uvicorn`. No Dockerfile, GitHub Actions workflows, or `.dockerignore` exist yet -- all must be created.

The deployment architecture has three major components: (1) a multi-stage Dockerfile using `uv` for fast, reproducible builds, (2) Azure Container Apps infrastructure with system-assigned managed identity for Key Vault and ACR access, and (3) a GitHub Actions CI/CD pipeline using OIDC (Workload Identity Federation) for secretless Azure authentication.

A critical finding is that Azure Container Apps natively supports **Key Vault secret references** -- secrets can be declared at the Container App level with `keyvaultref:` syntax and automatically pulled from Key Vault by the platform. This means the existing Python lifespan code that fetches the API key from Key Vault will continue to work (DefaultAzureCredential picks up managed identity), but the planner should also consider whether to use the platform-level secret reference for the API key instead. The recommendation is to keep the existing lifespan pattern unchanged for now, since it already works and handles the graceful fallback logic.

**Primary recommendation:** Build a multi-stage Dockerfile with `uv`, deploy to Container Apps with system-assigned managed identity, and wire up GitHub Actions with OIDC for push-to-main automated deployment.

## Standard Stack

### Core

| Tool/Service | Version | Purpose | Why Standard |
|-------------|---------|---------|--------------|
| Azure Container Apps | Current | Serverless container hosting | Managed Kubernetes without cluster overhead, built-in scaling, ingress, TLS |
| Azure Container Registry | Current | Docker image registry | Already provisioned, supports managed identity pull |
| Azure Key Vault | Current | Secrets management | Already provisioned, RBAC-based access with managed identity |
| GitHub Actions | Current | CI/CD pipeline | Native OIDC support for Azure, path-filtered triggers |
| `uv` | 0.5.x+ | Python dependency management | Already used locally, fast installs, lockfile-based reproducibility |
| `uvicorn` | (from deps) | ASGI server | Already the app's server, built into agent-framework dependencies |

### Supporting

| Tool | Purpose | When to Use |
|------|---------|-------------|
| `azure/login@v2` | GitHub Action for Azure auth | OIDC login in CI/CD pipeline |
| `azure/container-apps-deploy-action@v1` | GitHub Action for Container Apps deploy | Alternative to raw `az containerapp update` -- handles build+push+deploy |
| `docker/login-action@v2` | GitHub Action for registry login | ACR authentication in CI/CD |
| `docker/build-push-action@v4` | GitHub Action for Docker build | Multi-platform builds, layer caching |
| `ghcr.io/astral-sh/uv` | uv Docker image | Source for COPY --from in multi-stage build |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| `azure/container-apps-deploy-action@v1` | Raw `az containerapp update` in CLI step | Action is simpler but less control; raw CLI gives explicit image tag management. **Recommend raw CLI** for this project since we build/push separately and want explicit SHA tagging |
| Container Apps Key Vault references (platform-level) | Python lifespan Key Vault fetch (current code) | Platform refs are simpler but require restart to pick up changes; Python fetch gives graceful fallback. **Keep current Python approach** -- it already works |
| Service principal secret | OIDC (Workload Identity Federation) | OIDC is more secure (no stored secrets), matches user decision |

## Architecture Patterns

### Recommended Project Structure (new files)

```
backend/
  Dockerfile                    # Multi-stage uv build
  .dockerignore                 # Exclude .venv, .env, tests, etc.
.github/
  workflows/
    deploy-backend.yml          # CI/CD pipeline
```

### Pattern 1: Multi-Stage Dockerfile with uv

**What:** Two-stage Dockerfile -- builder stage installs dependencies with uv, runtime stage copies only the venv and app code.
**When to use:** All Python projects using uv for dependency management.

```dockerfile
# syntax=docker/dockerfile:1

# ============ Build Stage ============
FROM python:3.12-slim-bookworm AS builder

# Copy uv binary from official image (pinned version for reproducibility)
COPY --from=ghcr.io/astral-sh/uv:0.5.4 /uv /usr/local/bin/uv

WORKDIR /app

# uv environment variables for Docker builds
ENV UV_COMPILE_BYTECODE=1 \
    UV_LINK_MODE=copy \
    UV_PYTHON_DOWNLOADS=never \
    UV_PROJECT_ENVIRONMENT=/app/.venv

# Install dependencies FIRST for better layer caching
COPY pyproject.toml uv.lock .python-version ./
RUN uv sync --frozen --no-install-project --no-dev

# Copy source code and install the project itself
COPY src/ ./src/
RUN uv sync --frozen --no-dev --no-editable

# ============ Runtime Stage ============
FROM python:3.12-slim-bookworm AS runtime

# Security: non-root user
RUN groupadd -g 1001 app && \
    useradd -u 1001 -g app -m -d /app -s /bin/false app

WORKDIR /app

# Production environment
ENV PATH="/app/.venv/bin:$PATH" \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Copy only the virtual environment from builder
COPY --from=builder --chown=app:app /app/.venv /app/.venv
COPY --from=builder --chown=app:app /app/src /app/src

USER app

EXPOSE 8000

# Run with uvicorn
CMD ["uvicorn", "second_brain.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Source:** Astral uv Docker documentation, verified multi-stage patterns from community (digon.io, medium.com/@benitomartin)

**Key decisions in this Dockerfile:**
- `UV_COMPILE_BYTECODE=1`: Pre-compiles `.pyc` files for faster startup
- `UV_LINK_MODE=copy`: Copies files instead of hardlinks (required for multi-stage)
- `UV_PYTHON_DOWNLOADS=never`: Uses system Python, does not download standalone Python
- `--frozen`: Uses exact versions from `uv.lock` (no resolution at build time)
- `--no-dev`: Excludes dev dependencies (ruff, devui, pytest)
- `--no-editable`: Installs as a regular package, not editable
- Two-step sync: first `--no-install-project` (deps only, cacheable layer), then full sync
- Non-root user for security
- Port 8000 (standard, differs from local 8003)

### Pattern 2: Container App with Managed Identity for Key Vault

**What:** System-assigned managed identity on the Container App, granted "Key Vault Secrets User" role on the Key Vault.
**When to use:** Any Container App that needs to read secrets from Key Vault.

```bash
# 1. Create container app with system-assigned identity
az containerapp create \
  --name second-brain-api \
  --resource-group <RG> \
  --environment <ENV> \
  --image <ACR>.azurecr.io/second-brain-api:<SHA> \
  --ingress external --target-port 8000 \
  --system-assigned \
  --registry-server <ACR>.azurecr.io \
  --registry-identity system \
  --env-vars \
    COSMOS_ENDPOINT=https://<cosmos>.documents.azure.com:443/ \
    DATABASE_NAME=second-brain \
    AZURE_OPENAI_ENDPOINT=https://<openai>.openai.azure.com/ \
    AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4o \
    KEY_VAULT_URL=https://<kv>.vault.azure.net/ \
    ENABLE_INSTRUMENTATION=true \
    ENABLE_SENSITIVE_DATA=false \
  --min-replicas 0 --max-replicas 3

# 2. Get the managed identity principal ID
PRINCIPAL_ID=$(az containerapp identity show \
  --name second-brain-api --resource-group <RG> \
  --query principalId -o tsv)

# 3. Grant Key Vault Secrets User role
az role assignment create \
  --role "Key Vault Secrets User" \
  --assignee "$PRINCIPAL_ID" \
  --scope "/subscriptions/<SUB>/resourceGroups/<RG>/providers/Microsoft.KeyVault/vaults/<KV>"

# 4. Also grant Cosmos DB roles if using RBAC (not connection string)
# The app already uses DefaultAzureCredential for Cosmos -- same identity works
```

**Source:** Microsoft Learn -- Managed identities in Azure Container Apps, Key Vault Secrets User role assignment

### Pattern 3: GitHub Actions OIDC with Workload Identity Federation

**What:** GitHub Actions authenticates to Azure using OIDC tokens instead of stored secrets. Requires one-time setup of a Microsoft Entra app registration with a federated credential.
**When to use:** Any GitHub Actions -> Azure deployment pipeline.

**One-time setup (manual, before first deploy):**

```bash
# 1. Create app registration
APP_ID=$(az ad app create --display-name "github-second-brain-deploy" --query appId -o tsv)

# 2. Create service principal
az ad sp create --id $APP_ID

# 3. Add federated credential for GitHub Actions
az ad app federated-credential create --id $APP_ID --parameters '{
  "name": "github-main-branch",
  "issuer": "https://token.actions.githubusercontent.com",
  "subject": "repo:<OWNER>/second-brain:ref:refs/heads/main",
  "audiences": ["api://AzureADTokenExchange"]
}'

# 4. Assign Contributor role on the resource group
SP_ID=$(az ad sp show --id $APP_ID --query id -o tsv)
az role assignment create \
  --role Contributor \
  --assignee-object-id $SP_ID \
  --assignee-principal-type ServicePrincipal \
  --scope /subscriptions/<SUB>/resourceGroups/<RG>

# 5. Assign AcrPush role on ACR (for pushing images)
az role assignment create \
  --role AcrPush \
  --assignee-object-id $SP_ID \
  --assignee-principal-type ServicePrincipal \
  --scope /subscriptions/<SUB>/resourceGroups/<RG>/providers/Microsoft.ContainerRegistry/registries/<ACR>

# 6. Store these as GitHub repository variables (NOT secrets -- they're not secret):
#    AZURE_CLIENT_ID = $APP_ID
#    AZURE_TENANT_ID = <tenant>
#    AZURE_SUBSCRIPTION_ID = <sub>
```

**Source:** Microsoft Learn -- Workload Identity Federation, Configure a federated identity credential on an app

### Pattern 4: GitHub Actions Workflow Structure

**What:** CI/CD workflow that builds, pushes, and deploys on push to main.

```yaml
name: Deploy Backend to Azure Container Apps

on:
  push:
    branches: [main]
    paths: ['backend/**']

permissions:
  id-token: write   # Required for OIDC
  contents: read

env:
  ACR_NAME: <registry-name>
  CONTAINER_APP_NAME: second-brain-api
  RESOURCE_GROUP: <rg-name>
  IMAGE_NAME: second-brain-api

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Log in to Azure
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}

      - name: Log in to ACR
        run: az acr login --name ${{ env.ACR_NAME }}

      - name: Build and push image
        run: |
          IMAGE_TAG=${{ env.ACR_NAME }}.azurecr.io/${{ env.IMAGE_NAME }}:${{ github.sha }}
          docker build -t $IMAGE_TAG ./backend
          docker push $IMAGE_TAG

      - name: Deploy to Container Apps
        run: |
          az containerapp update \
            --name ${{ env.CONTAINER_APP_NAME }} \
            --resource-group ${{ env.RESOURCE_GROUP }} \
            --image ${{ env.ACR_NAME }}.azurecr.io/${{ env.IMAGE_NAME }}:${{ github.sha }}
```

**Source:** Microsoft Learn -- Deploy to Azure Container Apps with GitHub Actions, azure/login OIDC documentation

### Anti-Patterns to Avoid

- **Using `latest` tag for container images:** Always use commit SHA for traceability and to ensure new revisions are created. Container Apps may cache `latest` and not pull the new image.
- **Storing Azure credentials as GitHub Secrets:** Use OIDC (Workload Identity Federation) instead -- no secrets to rotate or leak.
- **Running as root in container:** Always create a non-root user in the Dockerfile.
- **Installing dev dependencies in production image:** Use `uv sync --no-dev` to keep the image small and secure.
- **Hardcoding configuration in Dockerfile:** Use environment variables set on the Container App, not baked into the image.
- **Using `load_dotenv()` in production without guard:** The existing `load_dotenv()` call is harmless (it's a no-op if no `.env` file exists), but environment variables should come from Container App config, not `.env` files.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| TLS certificate management | Custom cert provisioning | Container Apps managed certificate | Free, auto-renewing, zero config |
| Container orchestration | Kubernetes cluster | Azure Container Apps | Serverless, managed scaling, built-in ingress |
| CI/CD auth to Azure | Service principal with stored secret | OIDC Workload Identity Federation | No secrets to manage, short-lived tokens |
| Docker image caching | Custom cache logic | Docker layer caching (two-step uv sync) | Dependency layer cached when only code changes |
| Health monitoring | Custom health check service | Container Apps built-in probes | TCP probes auto-configured, HTTP probes for richer checks |
| Secret injection | Custom Key Vault client wrapper | DefaultAzureCredential (existing code) | Works with managed identity automatically, same code local and prod |

**Key insight:** Azure Container Apps handles TLS, scaling, health probes, and ingress routing. The Dockerfile just needs to produce a working container. The GitHub Actions workflow just needs to build, push, and update the image tag.

## Common Pitfalls

### Pitfall 1: Port Mismatch Between Container and Ingress
**What goes wrong:** The Container App ingress target port doesn't match the port the app listens on inside the container. Default health probes use the ingress target port, so mismatched ports cause probe failures and the app appears unhealthy.
**Why it happens:** Local dev uses port 8003, but the Dockerfile should expose 8000 (standard). If `--target-port` doesn't match the `CMD` port, the container restarts endlessly.
**How to avoid:** Ensure `--target-port 8000` matches the uvicorn `--port 8000` in the Dockerfile CMD. Document this mapping clearly.
**Warning signs:** Container keeps restarting, health probes failing, revision marked as unhealthy.

### Pitfall 2: System-Assigned Identity Not Available at Creation Time
**What goes wrong:** You try to assign Key Vault roles during `az containerapp create`, but the system-assigned identity doesn't exist until after creation completes.
**Why it happens:** System-assigned managed identity is created as part of the resource provisioning. Key Vault role assignment must happen after.
**How to avoid:** Use a two-step process: create the Container App first (with `--system-assigned`), then query the principal ID and assign roles. The app's first startup may fail Key Vault access -- this is expected and the existing graceful fallback in the lifespan handles it.
**Warning signs:** "Key Vault access denied" errors in first deployment logs.

### Pitfall 3: OIDC Federated Credential Subject Mismatch
**What goes wrong:** GitHub Actions OIDC login fails with "AADSTS70021: No matching federated identity record found."
**Why it happens:** The `subject` in the federated credential must exactly match the GitHub token claim. For branch-triggered workflows, it must be `repo:<owner>/<repo>:ref:refs/heads/main`. Case matters, and the repo name must match exactly.
**How to avoid:** Double-check the `subject` field. Use `ref:refs/heads/main` for push triggers on main branch. If the repo is in an org, include the org name.
**Warning signs:** Azure login step fails in GitHub Actions with AADSTS error.

### Pitfall 4: Missing `--prerelease=allow` Context for uv
**What goes wrong:** `uv sync --frozen` in Docker fails because the lock file references pre-release packages (agent-framework RC packages).
**Why it happens:** The pyproject.toml notes that `agent-framework-ag-ui` requires `--prerelease=allow`. However, `--frozen` uses the lock file as-is without re-resolving, so this should be fine. The risk is if the lock file is regenerated without the prerelease flag.
**How to avoid:** The lock file already has the resolved versions. `uv sync --frozen` does not re-resolve, so prerelease is not an issue at Docker build time. Just ensure the lock file is committed and up-to-date.
**Warning signs:** `uv sync` fails during Docker build with version resolution errors.

### Pitfall 5: DefaultAzureCredential Slow Probe in Container
**What goes wrong:** `DefaultAzureCredential` tries multiple credential sources (environment, managed identity, CLI, etc.) sequentially. In a container without all sources configured, it may take 10-30 seconds to fall through to managed identity.
**Why it happens:** DefaultAzureCredential probes sources in order. In a container, CLI and VS Code credentials don't exist, causing timeout-based fallback.
**How to avoid:** This is generally not a problem for Container Apps because `IDENTITY_ENDPOINT` and `IDENTITY_HEADER` environment variables are automatically set, causing DefaultAzureCredential to find managed identity quickly. No code change needed.
**Warning signs:** Slow startup, timeout warnings in logs during credential acquisition.

### Pitfall 6: Custom Domain DNS Validation Order
**What goes wrong:** The managed certificate fails to provision because DNS records aren't set up before the certificate request.
**Why it happens:** Azure needs to validate domain ownership via CNAME or A record + TXT record before issuing the managed certificate. DigiCert must be able to reach the app from their IP addresses.
**How to avoid:** Set up DNS records (CNAME for subdomain pointing to the Container App's auto-generated domain) BEFORE requesting the managed certificate. The app must be publicly accessible at that point.
**Warning signs:** Certificate status shows "Failed" or remains in "Provisioning" state.

## Code Examples

### .dockerignore for Backend

```
# Virtual environment (built fresh in Docker)
.venv/
__pycache__/
*.pyc

# Environment files (secrets, local config)
.env
.env.*

# Testing (not needed in production image)
tests/
.pytest_cache/
.coverage
htmlcov/

# Development tools
.ruff_cache/
.git/
.gitignore

# IDE
.vscode/
.idea/

# OS
.DS_Store

# Documentation
*.md
```

### Container App Environment Variables Mapping

Based on the existing `.env.example` and `config.py`:

```
# Secrets -- fetched by Python lifespan via DefaultAzureCredential + Key Vault
# (no env vars needed for secrets -- existing code handles this)

# Non-secret config -- set as Container App environment variables
AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com/
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4o
COSMOS_ENDPOINT=https://<cosmos>.documents.azure.com:443/
KEY_VAULT_URL=https://<kv>.vault.azure.net/
ENABLE_INSTRUMENTATION=true
ENABLE_SENSITIVE_DATA=false
```

### Custom Domain Setup (for when user provides domain)

```bash
# For a subdomain like api.example.com:

# 1. Get the Container App's auto-generated FQDN
FQDN=$(az containerapp show --name second-brain-api --resource-group <RG> \
  --query properties.configuration.ingress.fqdn -o tsv)

# 2. Create CNAME record in DNS provider:
#    api.example.com -> $FQDN

# 3. Create TXT record for validation:
#    asuid.api.example.com -> <domain-verification-code from portal>

# 4. Add custom domain with managed certificate
az containerapp hostname add \
  --name second-brain-api \
  --resource-group <RG> \
  --hostname api.example.com

# 5. Bind managed certificate
az containerapp ssl auto \
  --name second-brain-api \
  --resource-group <RG> \
  --hostname api.example.com
```

**Source:** Microsoft Learn -- Custom domain names and free managed certificates in Azure Container Apps

## Discretion Recommendations

### Scaling Configuration

**Recommendation:** `--min-replicas 0 --max-replicas 3`

**Rationale:**
- `min-replicas 0`: Scales to zero when idle. No usage charges when not in use. This is a personal/hobby project -- no need to keep a replica running 24/7.
- `max-replicas 3`: Sufficient for a single-user UAT scenario. Default is 10 which is overkill.
- Default HTTP scaling rule (scale on concurrent requests) is appropriate -- no custom rules needed.
- Trade-off: First request after scale-to-zero has a cold start (container pull + app startup). For UAT this is acceptable. If cold start becomes annoying, set `min-replicas 1`.

**Source:** Microsoft Learn -- Set scaling rules in Azure Container Apps

### Health Check Configuration

**Recommendation:** Use the existing `/health` endpoint for HTTP health probes.

The app already has a `/health` endpoint (returns `{"status": "ok"}`) that bypasses API key auth (in `PUBLIC_PATHS`). Configure:

- **Startup probe:** HTTP GET `/health`, port 8000, `initialDelaySeconds: 5`, `failureThreshold: 30`, `periodSeconds: 2` (allows up to 65 seconds for startup)
- **Liveness probe:** HTTP GET `/health`, port 8000, `periodSeconds: 10`, `failureThreshold: 3`
- **Readiness probe:** HTTP GET `/health`, port 8000, `periodSeconds: 5`, `failureThreshold: 3`

Default TCP probes will also work (Container Apps auto-configures them if ingress is enabled), but HTTP probes on `/health` are richer -- they confirm the FastAPI app is actually responding, not just that the port is open.

**Source:** Microsoft Learn -- Health probes in Azure Container Apps, Architecture best practices

### Resource Allocation

**Recommendation:** 0.5 vCPU, 1 GiB memory

**Rationale:**
- This is a single-user FastAPI app with LLM API calls (Azure OpenAI) and Cosmos DB queries
- The app itself is lightweight -- the heavy compute happens on Azure OpenAI's side
- 0.5 vCPU / 1 GiB is the minimum for Container Apps on consumption plan and sufficient for this workload
- Can be increased later if monitoring shows resource pressure

### Dockerfile Optimization

**Recommendation:** Use the multi-stage pattern shown in Pattern 1 above with these specifics:

1. Pin the `uv` image version (e.g., `ghcr.io/astral-sh/uv:0.5.4`) for reproducibility
2. Use `python:3.12-slim-bookworm` as base for both stages (matches `.python-version`)
3. Two-step `uv sync` for optimal layer caching (deps first, then project)
4. Non-root user in runtime stage
5. `PYTHONDONTWRITEBYTECODE=1` and `PYTHONUNBUFFERED=1` for container best practices

### GitHub Actions Workflow Structure

**Recommendation:** Single workflow file, single job, 5 steps.

```
Workflow: deploy-backend.yml
  Trigger: push to main, paths: backend/**
  Job: build-and-deploy
    Steps:
      1. Checkout
      2. Azure Login (OIDC)
      3. ACR Login
      4. Docker Build + Push (tagged with commit SHA)
      5. Container App Update (new image tag)
```

Single job is simpler and sufficient for this project. No need for separate build/deploy jobs or environments for a hobby project with a solo developer.

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Service principal secrets in GitHub | OIDC Workload Identity Federation | GA since 2023 | No stored secrets, auto-rotating tokens |
| Gunicorn + UvicornWorker | Uvicorn `--workers N` (built-in supervisor) | Uvicorn ~0.30+ (2024) | Simpler setup, fewer dependencies. Single worker is fine for this use case |
| `pip install` in Dockerfile | `uv sync --frozen` | uv gained Docker support in 2024 | 10-100x faster installs, lockfile reproducibility |
| Key Vault access policies | Key Vault RBAC | Recommended since 2022 | Finer-grained control, consistent with Azure RBAC model |
| ACR admin credentials for pull | Managed identity image pull | GA since 2023 | No credentials to store, automatic rotation |

**Deprecated/outdated:**
- `az containerapp github-action add` (auto-generates workflow): Works but creates a service principal with stored secrets. OIDC is the current best practice.
- ACR admin user for image pull: Still works but managed identity is recommended.

## Open Questions

1. **Container Apps Environment**
   - What we know: An environment is needed to host the Container App. It may already be provisioned.
   - What's unclear: Whether an environment already exists in the user's Azure subscription.
   - Recommendation: Check during plan execution. If not, create one. The plan should include a "create environment if needed" step.

2. **Existing Azure Resources Inventory**
   - What we know: ACR and Key Vault are "already provisioned" per CONTEXT.md.
   - What's unclear: Exact names of the ACR, Key Vault, resource group, and whether a Container Apps environment exists.
   - Recommendation: First plan task should inventory existing resources and capture names. The planner should create a setup/bootstrap task.

3. **GitHub Repository Name for OIDC Subject**
   - What we know: The federated credential subject must include `repo:<owner>/<repo>:ref:refs/heads/main`.
   - What's unclear: The exact GitHub organization/username and repository name.
   - Recommendation: The planner should parameterize this in the one-time setup instructions.

4. **`load_dotenv()` Behavior in Container**
   - What we know: `load_dotenv()` is called at module load time in `main.py`. If no `.env` file exists, it's a no-op.
   - What's unclear: Whether this causes any issues or warnings in the container.
   - Recommendation: No change needed. `load_dotenv()` silently does nothing when `.env` is absent. Container App environment variables are read by `pydantic-settings` directly from `os.environ`.

## Sources

### Primary (HIGH confidence)
- Microsoft Learn -- Managed identities in Azure Container Apps: https://learn.microsoft.com/azure/container-apps/managed-identity
- Microsoft Learn -- Manage secrets in Azure Container Apps: https://learn.microsoft.com/azure/container-apps/manage-secrets
- Microsoft Learn -- Custom domains and free managed certificates: https://learn.microsoft.com/azure/container-apps/custom-domains-managed-certificates
- Microsoft Learn -- Health probes in Azure Container Apps: https://learn.microsoft.com/azure/container-apps/health-probes
- Microsoft Learn -- Set scaling rules: https://learn.microsoft.com/azure/container-apps/scale-app
- Microsoft Learn -- Deploy to Azure Container Apps with GitHub Actions: https://learn.microsoft.com/azure/container-apps/github-actions
- Microsoft Learn -- Workload Identity Federation concepts: https://learn.microsoft.com/entra/workload-id/workload-identity-federation
- Microsoft Learn -- Configure a federated identity credential: https://learn.microsoft.com/entra/workload-id/workload-identity-federation-create-trust

### Secondary (MEDIUM confidence)
- Astral uv Docker patterns (community-verified): https://medium.com/@benitomartin/deep-dive-into-uv-dockerfiles-by-astral-image-size-performance-best-practices
- Multi-stage Python Docker images with uv: https://digon.io/en/blog/2025_07_28_python_docker_images_with_uv
- FastAPI Docker production guide (2026): https://blog.greeden.me/en/2026/01/20/complete-guide-to-deploying-fastapi-in-production-reliable-operations-with-uvicorn-multi-workers-docker-and-a-reverse-proxy/

### Codebase (HIGH confidence -- direct inspection)
- `backend/pyproject.toml` -- Dependencies, build system, Python 3.12 requirement
- `backend/src/second_brain/main.py` -- Lifespan with Key Vault fetch, Cosmos DB init, port 8003
- `backend/src/second_brain/config.py` -- pydantic-settings configuration (reads from env vars)
- `backend/src/second_brain/auth.py` -- API key middleware, PUBLIC_PATHS includes `/health`
- `backend/src/second_brain/api/health.py` -- `/health` endpoint returning `{"status": "ok"}`
- `backend/src/second_brain/db/cosmos.py` -- DefaultAzureCredential for Cosmos auth
- `backend/.env.example` -- Documents all required environment variables
- `backend/.python-version` -- Python 3.12

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- Azure Container Apps, GitHub Actions OIDC, uv Docker patterns all well-documented with official sources
- Architecture: HIGH -- Patterns verified against Microsoft Learn documentation and existing codebase
- Pitfalls: HIGH -- Common issues documented in Microsoft troubleshooting guides; codebase-specific pitfalls identified through code inspection

**Research date:** 2026-02-23
**Valid until:** 2026-03-23 (stable infrastructure patterns, unlikely to change significantly)
