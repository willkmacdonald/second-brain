# Phase 09.1: Mobile UX Review and Refinements - Research

**Researched:** 2026-02-28
**Domain:** React Native / Expo mobile UI refactoring
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

#### Screen consolidation
- Delete `capture/text.tsx` entirely -- no separate text capture route
- One unified capture screen on the main Capture tab (`(tabs)/index.tsx`)
- Remove Photo and Video buttons (not implemented, were showing "Coming soon")
- Voice and Text buttons always visible at top as mode toggles
- Default to voice mode on app startup (voice pre-selected)

#### Voice/Text toggle buttons
- Equal width, side by side at the top of the screen
- Blue outline on the selected mode, no outline on unselected
- No outline on either button at startup until user selects (actually: default to voice, so voice has outline on startup)
- Taller buttons with proper padding so emoji icons sit fully inside (no clipping/bleeding)
- Same button size regardless of which is selected (both always the same dimensions)

#### Input area behavior
- When voice selected: record button centered in main area (current design)
- When text selected: text input box with "What's on your mind?" placeholder, Send button inside the text input at bottom-right
- No separate nav header or "Text Input" title bar -- everything lives on the one screen

#### Follow-up mode switching
- During follow-up (agent asks clarifying question), the Voice/Text toggles at top remain tappable
- Tapping Voice shows the record button for reply; tapping Text shows the text input for reply
- Remove all "Type instead" / "Record instead" toggle links -- the top buttons serve this purpose
- Agent question bubble appears between the toggles and the input area

#### Processing feedback
- Keep synchronous flow (screen stays blocked until classification completes)
- Replace single "Processing" indicator with granular stage progression: "Uploading..." -> "Transcribing..." -> "Classifying..." -> result
- Show processing feedback inline in the main content area (replace the record button / text input during processing)
- Input controls reappear when processing completes and screen resets

### Claude's Discretion
- Exact button height and padding values to contain emoji icons
- Processing stage text and animation details
- How the text input Send button is styled inside the input area
- Toast message positioning and timing on the unified screen
- How bucket selection buttons appear on the unified screen (low-confidence flow)

### Deferred Ideas (OUT OF SCOPE)
- Streaming audio upload during recording to reduce actual latency (upload chunks as user records, server stitches and transcribes immediately on stop) -- future phase
- Inbox screen UX improvements (detail cards, badge counts, item display) -- not discussed, potential future phase
</user_constraints>

## Summary

This phase is a pure mobile-side UI refactoring. No backend changes are needed -- the existing SSE event protocol, API endpoints, and ag-ui-client functions remain unchanged. The work consolidates two redundant capture screens (`(tabs)/index.tsx` and `capture/text.tsx`) into a single unified screen on the Capture tab, adds a Voice/Text toggle at the top, integrates the text capture flow that currently lives in `capture/text.tsx` directly into `(tabs)/index.tsx`, removes dead Photo/Video buttons, eliminates "Type instead" / "Record instead" links in favor of the persistent top toggles, and replaces the generic "Processing" step indicator with granular stage feedback.

The codebase is well-structured for this: both screens already share the same callback patterns (`StreamingCallbacks`), the same `sendCapture`/`sendFollowUp`/`sendFollowUpVoice` client functions, the same bucket selection UI, and the same HITL state management. The unification is primarily a layout refactor plus text capture integration -- not a rewrite.

**Primary recommendation:** Refactor `(tabs)/index.tsx` to be the single unified capture screen with mode toggle, merge the text capture logic from `capture/text.tsx`, then delete the text screen and clean up routing. No new libraries needed.

## Standard Stack

### Core (already installed, no changes)
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| expo-router | ~6.0.23 | Tab and stack navigation | Already the app's routing layer |
| expo-audio | ~1.1.1 | Voice recording | Already used for mic access and recording |
| expo-haptics | ~15.0.8 | Haptic feedback on actions | Already used for button presses |
| react-native-sse | ^1.2.1 | SSE EventSource client | Already used for AG-UI streaming |
| react-native-safe-area-context | ~5.6.0 | Safe area insets | Already used in capture screen |

### Supporting (no new libraries needed)
No new dependencies are required. All functionality exists in the current stack.

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Inline mode toggle | react-native-segmented-control | Adds a dep for something 2 Pressables do fine |
| Animated processing stages | react-native-reanimated | Over-engineered for text label transitions |

**Installation:** None required. Zero new dependencies.

## Architecture Patterns

### Current File Structure (before)
```
mobile/app/
‚îú‚îÄ‚îÄ (tabs)/
‚îÇ   ‚îú‚îÄ‚îÄ _layout.tsx          # Tab bar (Capture + Inbox)
‚îÇ   ‚îú‚îÄ‚îÄ index.tsx             # Main capture screen (voice + 4 buttons)
‚îÇ   ‚îî‚îÄ‚îÄ inbox.tsx             # Inbox list
‚îú‚îÄ‚îÄ capture/
‚îÇ   ‚îî‚îÄ‚îÄ text.tsx              # Separate text capture (TO BE DELETED)
‚îú‚îÄ‚îÄ conversation/
‚îÇ   ‚îî‚îÄ‚îÄ [threadId].tsx        # Conversation detail (unchanged)
‚îî‚îÄ‚îÄ _layout.tsx               # Root layout with Stack nav
```

### Target File Structure (after)
```
mobile/app/
‚îú‚îÄ‚îÄ (tabs)/
‚îÇ   ‚îú‚îÄ‚îÄ _layout.tsx          # Tab bar (Capture + Inbox) -- unchanged
‚îÇ   ‚îú‚îÄ‚îÄ index.tsx             # UNIFIED capture screen (voice + text via toggle)
‚îÇ   ‚îî‚îÄ‚îÄ inbox.tsx             # Inbox list -- unchanged
‚îú‚îÄ‚îÄ conversation/
‚îÇ   ‚îî‚îÄ‚îÄ [threadId].tsx        # Conversation detail -- unchanged
‚îî‚îÄ‚îÄ _layout.tsx               # Root layout: REMOVE capture/text route
```

### Pattern 1: Mode Toggle with Conditional Content Area

**What:** A `mode` state variable (`"voice" | "text"`) controls which input UI renders in the content area. The Voice/Text toggle buttons at the top always remain visible and switch the mode.

**When to use:** When two input modes share the same screen real estate and the same downstream processing pipeline.

**Current state analysis:**
- `(tabs)/index.tsx` already has `mode` state (`"text" | "voice"`) but it's used to show/hide the 4-button grid vs voice UI
- `capture/text.tsx` has its own standalone text input with `sendCapture`
- After refactoring, `mode` controls which input renders: voice record button OR text input box

**Key design:**
```tsx
const [mode, setMode] = useState<"voice" | "text">("voice"); // Default to voice per CONTEXT.md

// Toggle buttons at top (always visible)
<View style={styles.toggleRow}>
  <Pressable style={[styles.toggleButton, mode === "voice" && styles.toggleActive]} onPress={() => setMode("voice")}>
    <Text style={styles.toggleIcon}>üéôÔ∏è</Text>
    <Text style={styles.toggleLabel}>Voice</Text>
  </Pressable>
  <Pressable style={[styles.toggleButton, mode === "text" && styles.toggleActive]} onPress={() => setMode("text")}>
    <Text style={styles.toggleIcon}>‚úçÔ∏è</Text>
    <Text style={styles.toggleLabel}>Text</Text>
  </Pressable>
</View>

// Content area switches based on mode
{mode === "voice" && !processing && !agentQuestion && (
  <VoiceRecordArea ... />
)}
{mode === "text" && !processing && !agentQuestion && (
  <TextInputArea ... />
)}
```

### Pattern 2: Unified State Management

**What:** Merge the state from both screens into a single component. Both screens already manage nearly identical state (processing, toast, HITL, follow-up). The unified screen keeps one set of state variables.

**Key insight:** The current `(tabs)/index.tsx` already has 90% of the needed state for text capture. The missing pieces from `capture/text.tsx` are:
1. `thought` state (text input value) -- add as new state
2. `sending` state (text submission in progress) -- merge with existing `processing`
3. `handleSubmit` (text capture handler) -- merge into the unified screen
4. The `sendCapture` import -- already available, just not used in index.tsx currently

**State variables in unified screen:**
```tsx
// Mode
const [mode, setMode] = useState<"voice" | "text">("voice");

// Shared state (already in index.tsx)
const [processing, setProcessing] = useState(false);
const [toast, setToast] = useState<Toast | null>(null);
const [currentStep, setCurrentStep] = useState<string | null>(null);
const [completedSteps, setCompletedSteps] = useState<string[]>([]);
const [streamedText, setStreamedText] = useState("");
const [showSteps, setShowSteps] = useState(false);

// Text input (from text.tsx)
const [thought, setThought] = useState("");

// HITL state (already in index.tsx, identical to text.tsx)
const [hitlQuestion, setHitlQuestion] = useState<string | null>(null);
const [hitlThreadId, setHitlThreadId] = useState<string | null>(null);
const [hitlInboxItemId, setHitlInboxItemId] = useState<string | null>(null);
const [hitlTopBuckets, setHitlTopBuckets] = useState<string[]>([]);
const [isResolving, setIsResolving] = useState(false);
const [followUpRound, setFollowUpRound] = useState(0);
const [agentQuestion, setAgentQuestion] = useState<string | null>(null);
const [misunderstoodInboxItemId, setMisunderstoodInboxItemId] = useState<string | null>(null);
const [isReclassifying, setIsReclassifying] = useState(false);
const [followUpText, setFollowUpText] = useState("");
```

### Pattern 3: Processing Feedback Stages

**What:** Replace the single "Processing" step with granular stage progression displayed as text labels in the content area.

**Current backend event flow:**
- **Text capture:** `STEP_START("Classifying")` -> `STEP_END("Classifying")` -> result event
- **Voice capture:** `STEP_START("Processing")` -> `STEP_END("Processing")` -> result event

The backend emits a single step bracket. The granular client-side stages ("Uploading...", "Transcribing...", "Classifying...") need to be driven by the mobile client based on what it knows about the operation phase:

1. **Uploading...** -- shown immediately when voice recording stops and upload begins (before SSE connection opens). For text, skip this stage.
2. **Transcribing...** -- shown when SSE connection opens for voice capture (backend is transcribing + classifying). For text, skip this stage.
3. **Classifying...** -- shown when `STEP_START` arrives (for both text and voice).
4. **Result** -- shown when `CLASSIFIED`/`LOW_CONFIDENCE`/`MISUNDERSTOOD`/`UNRESOLVED` arrives.

**Implementation approach:** Use a `processingStage` state variable instead of the existing `currentStep`/`completedSteps` arrays:

```tsx
type ProcessingStage = "uploading" | "transcribing" | "classifying" | null;
const [processingStage, setProcessingStage] = useState<ProcessingStage>(null);

// Voice capture flow:
setProcessingStage("uploading");    // Immediately on record stop
// ...after FormData sent and SSE connected...
setProcessingStage("transcribing"); // We know backend is transcribing
// ...on STEP_START...
setProcessingStage("classifying");  // Agent is classifying
// ...on result event...
setProcessingStage(null);           // Done

// Text capture flow:
setProcessingStage("classifying");  // Immediately on send
// ...on result event...
setProcessingStage(null);           // Done
```

**Display:** Replace the record button / text input during processing with a centered stage indicator:
```tsx
{processingStage && (
  <View style={styles.processingArea}>
    <ActivityIndicator color="#4a90d9" />
    <Text style={styles.processingText}>
      {processingStage === "uploading" && "Uploading..."}
      {processingStage === "transcribing" && "Transcribing..."}
      {processingStage === "classifying" && "Classifying..."}
    </Text>
  </View>
)}
```

### Pattern 4: Follow-Up Mode via Top Toggles

**What:** During misunderstood follow-up, the Voice/Text toggle buttons remain tappable and control the reply input mode. This replaces the current "Type instead" / "Record instead" links.

**Current state:** `followUpMode` already exists in `(tabs)/index.tsx` and defaults to `"voice"`. The toggle links change it. The refactoring simply removes the toggle links and makes the top buttons control `followUpMode` during follow-up.

**Key consideration:** During follow-up, the mode toggle changes the follow-up input type, not the primary capture type. The `mode` state can serve double duty:
- When `agentQuestion` is null: `mode` controls primary capture input
- When `agentQuestion` is set: `mode` controls follow-up input

This eliminates the separate `followUpMode` state variable entirely -- `mode` handles both contexts.

### Anti-Patterns to Avoid

- **Separate follow-up mode state:** Do NOT keep a separate `followUpMode` alongside `mode`. One state variable controls the input type in all contexts (primary capture and follow-up reply).
- **Conditional navigation:** Do NOT route to a new screen for text input. The Text button switches mode in-place.
- **Backend changes for granular stages:** Do NOT modify the backend to emit separate "Uploading", "Transcribing" steps. The client knows the operation phase and can display stages locally. The backend's single step bracket is sufficient.
- **Recreating CaptureButton component for toggles:** The existing `CaptureButton` component is sized for the 4-button grid layout (`flex: 1` with no explicit height, `marginVertical: 6`). The new toggle buttons need explicit equal-width, taller dimensions with contained emoji icons. Either modify `CaptureButton` to accept a `variant` prop or (simpler) inline the toggle buttons directly in the screen since they have very different styling requirements.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| SSE event parsing | Custom parser | Existing `attachCallbacks` in ag-ui-client.ts | Already handles all v2 event types correctly |
| Voice recording | Custom audio API | expo-audio RecordingPresets.HIGH_QUALITY | Already working, well-tested |
| Toast notifications | Custom toast system | Keep existing inline toast pattern | Works fine for single-user MVP |
| Tab navigation | Custom tab bar | expo-router Tabs | Already configured |

**Key insight:** This phase is a UI refactoring, not a feature addition. The entire SSE pipeline, API client, and backend stay untouched. Resist adding new libraries or abstractions.

## Common Pitfalls

### Pitfall 1: Breaking Voice Permission Flow
**What goes wrong:** Voice recording stops working because permission request is lost during refactoring.
**Why it happens:** The `useEffect` that calls `AudioModule.requestRecordingPermissionsAsync()` is currently triggered by `mode !== "voice"`. If the default mode is "voice" (as specified), the effect must run on mount.
**How to avoid:** Run the permission request unconditionally on mount since voice is the default mode. The current pattern triggers on `mode` change, which means with default `"voice"`, it runs immediately on mount -- this is correct but verify it's preserved.
**Warning signs:** "Mic permission required" shows on first tap without any system permission dialog appearing.

### Pitfall 2: Text Capture Missing sendCapture Import
**What goes wrong:** Text submission silently fails or crashes because `sendCapture` wasn't imported into `(tabs)/index.tsx`.
**Why it happens:** The current `(tabs)/index.tsx` only imports `sendVoiceCapture`, `sendFollowUp`, and `sendFollowUpVoice`. It does not import `sendCapture` because text capture currently lives in `capture/text.tsx`.
**How to avoid:** Add `sendCapture` to the import from `../../lib/ag-ui-client` when merging text capture logic.
**Warning signs:** Tapping Send in text mode does nothing.

### Pitfall 3: Reset State Not Clearing Text Input
**What goes wrong:** After successful capture in text mode, the text input retains the previous thought.
**Why it happens:** `resetVoiceState` (renamed to `resetState`) doesn't include `setThought("")`.
**How to avoid:** Include `setThought("")` in the unified reset function.
**Warning signs:** Previous capture text still visible after "Filed" toast.

### Pitfall 4: Follow-Up Voice Permission Not Ready When Switching From Text
**What goes wrong:** User starts a text capture, gets MISUNDERSTOOD, switches to voice for follow-up, but mic permission hasn't been requested.
**Why it happens:** If the user has been in text mode the entire time, the mic permission `useEffect` may not have fired.
**How to avoid:** Request mic permission on mount regardless of initial mode (since voice is default, this should happen anyway, but ensure the permission request doesn't re-trigger and cause issues when toggling modes).
**Warning signs:** "Mic permission required" appears when switching to voice during follow-up.

### Pitfall 5: Expo Router Route Still Registered
**What goes wrong:** After deleting `capture/text.tsx`, expo-router still tries to render the route, or the `router.push("/capture/text")` call in index.tsx causes a crash.
**Why it happens:** The `Text` button's `onPress` currently calls `router.push("/capture/text")`. If the file is deleted but the button handler isn't updated, it crashes. Also, the root `_layout.tsx` has a `Stack.Screen` for `capture/text` that should be removed.
**How to avoid:**
1. Change Text button to `setMode("text")` instead of `router.push`
2. Remove the `capture/text.tsx` file
3. Remove the `capture/text` Stack.Screen from `_layout.tsx`
4. Delete the `capture/` directory if empty
**Warning signs:** App crashes on startup or when navigating.

### Pitfall 6: Emoji Icon Clipping in Toggle Buttons
**What goes wrong:** The emoji icons (microphone, writing hand) bleed outside the button boundaries.
**Why it happens:** The current `CaptureButton` component uses `fontSize: 48` for icons with no explicit button height constraint -- the icon can overflow the container depending on device.
**How to avoid:** Set explicit `height` on toggle buttons (suggest 72-80px), set `overflow: "hidden"` on the button, and reduce emoji `fontSize` to 32-36 to fit within the padded area.
**Warning signs:** Icons visually extend beyond the rounded rectangle boundaries.

### Pitfall 7: Processing Stage Timing for Voice
**What goes wrong:** "Uploading..." flashes too briefly or "Transcribing..." never shows because the SSE events arrive too quickly.
**Why it happens:** Voice upload uses EventSource with FormData body -- the SSE connection doesn't open until the upload completes. So "Uploading..." displays while the HTTP POST is in-flight, and "Transcribing..." might only show for a fraction of a second before `STEP_START("Processing")` arrives.
**How to avoid:** Accept that the timing is approximate and driven by actual network latency. For voice, show "Uploading..." immediately on record stop, then switch to "Transcribing..." when the EventSource `open` event fires (or immediately after creating the EventSource), then "Classifying..." on `STEP_START`. If "Transcribing..." duration is negligible, it will flash briefly -- this is acceptable since the user sees progress movement.
**Warning signs:** User sees "Uploading..." jump straight to classification result with nothing in between.

## Code Examples

### Text Capture Handler (merge from text.tsx)

The `handleSubmit` function from `capture/text.tsx` needs to be merged into `(tabs)/index.tsx`. The key pattern:

```tsx
// Source: capture/text.tsx lines 257-360 (current implementation)
const handleTextSubmit = useCallback(() => {
  if (!thought.trim() || processing) return;
  if (!API_KEY) {
    setToast({ message: "No API key configured", type: "error" });
    return;
  }

  setProcessing(true);
  setProcessingStage("classifying");
  Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Medium);

  const captureResult = sendCapture({
    message: thought.trim(),
    apiKey: API_KEY,
    callbacks: {
      onStepStart: (stepName: string) => {
        setProcessingStage("classifying");
      },
      onStepFinish: (_stepName: string) => {
        // Stage will transition on result event
      },
      onLowConfidence: (inboxItemId, bucket, _confidence) => {
        setHitlInboxItemId(inboxItemId);
        setHitlQuestion(`Best guess: ${bucket}. Which bucket?`);
        setShowSteps(true);
        setProcessing(false);
        setProcessingStage(null);
        setHitlTopBuckets([bucket]);
      },
      onMisunderstood: (_threadId, questionText, inboxItemId) => {
        setAgentQuestion(questionText);
        setMisunderstoodInboxItemId(inboxItemId);
        setFollowUpRound(1);
        setThought("");
        setProcessing(false);
        setProcessingStage(null);
        setShowSteps(false);
      },
      onUnresolved: (_inboxItemId) => {
        setProcessing(false);
        setProcessingStage(null);
        setToast({ message: "Couldn't classify. Check inbox later.", type: "error" });
        setTimeout(resetState, AUTO_RESET_MS);
      },
      onComplete: (result) => {
        setProcessing(false);
        setProcessingStage(null);
        Haptics.notificationAsync(Haptics.NotificationFeedbackType.Success);
        setToast({ message: result || "Captured", type: "success" });
        setTimeout(resetState, AUTO_RESET_MS);
      },
      onError: (error) => {
        setProcessing(false);
        setProcessingStage(null);
        setShowSteps(false);
        setToast({ message: error || "Couldn't file your capture. Try again.", type: "error" });
      },
    },
  });
  cleanupRef.current = captureResult.cleanup;
}, [thought, processing, resetState]);
```

### Toggle Button Styling

```tsx
// Inline toggle buttons (NOT reusing CaptureButton which has different layout needs)
const toggleStyles = StyleSheet.create({
  toggleRow: {
    flexDirection: "row",
    paddingHorizontal: 16,
    paddingTop: 12,
    paddingBottom: 8,
    gap: 12,
  },
  toggleButton: {
    flex: 1,
    height: 72,
    justifyContent: "center",
    alignItems: "center",
    backgroundColor: "#1a1a2e",
    borderRadius: 16,
    borderWidth: 2,
    borderColor: "transparent",
  },
  toggleActive: {
    borderColor: "#4a90d9",
  },
  toggleIcon: {
    fontSize: 32,     // Smaller than current 48 to prevent clipping
    marginBottom: 4,
  },
  toggleLabel: {
    fontSize: 14,
    fontWeight: "600",
    color: "#ffffff",
  },
});
```

### Text Input with Embedded Send Button

```tsx
// Text mode input area with Send button inside
<View style={styles.textInputContainer}>
  <TextInput
    style={styles.textInput}
    value={thought}
    onChangeText={setThought}
    placeholder="What's on your mind?"
    placeholderTextColor="#666"
    multiline
    textAlignVertical="top"
  />
  <Pressable
    onPress={handleTextSubmit}
    disabled={!thought.trim() || processing}
    style={({ pressed }) => [
      styles.sendButton,
      pressed && { opacity: 0.7 },
      (!thought.trim() || processing) && { opacity: 0.4 },
    ]}
  >
    <Text style={styles.sendButtonText}>Send</Text>
  </Pressable>
</View>

// Styles
textInputContainer: {
  backgroundColor: "#1a1a2e",
  borderRadius: 12,
  padding: 12,
  minHeight: 120,
  position: "relative",
},
textInput: {
  fontSize: 18,
  color: "#ffffff",
  paddingBottom: 40, // Space for Send button
  minHeight: 80,
},
sendButton: {
  position: "absolute",
  bottom: 10,
  right: 10,
  backgroundColor: "#4a90d9",
  paddingHorizontal: 16,
  paddingVertical: 8,
  borderRadius: 8,
},
sendButtonText: {
  color: "#ffffff",
  fontSize: 15,
  fontWeight: "600",
},
```

### Processing Stage Display

```tsx
// Replaces record button / text input during processing
{processingStage && (
  <View style={styles.processingArea}>
    <ActivityIndicator size="large" color="#4a90d9" />
    <Text style={styles.processingStageText}>
      {processingStage === "uploading" ? "Uploading..." :
       processingStage === "transcribing" ? "Transcribing..." :
       "Classifying..."}
    </Text>
  </View>
)}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| 4-button grid (Voice/Text/Photo/Video) | 2-button toggle (Voice/Text) | This phase | Removes dead "Coming soon" buttons |
| Separate text capture screen | Unified capture screen with mode toggle | This phase | One screen, no navigation for text |
| "Type instead" / "Record instead" links | Persistent top toggle buttons | This phase | Consistent mode switching everywhere |
| Single "Processing" indicator | Granular stage progression | This phase | Better perceived performance |

**Deprecated/outdated:**
- `capture/text.tsx`: Entire file deleted -- functionality merged into `(tabs)/index.tsx`
- `CaptureButton` component: May become unused after toggle refactoring. If no other screen uses it, delete it.
- `showComingSoon` function: Remove along with Photo/Video buttons
- `router.push("/capture/text")`: Remove -- Text button now calls `setMode("text")`

## Open Questions

1. **CaptureButton component reuse**
   - What we know: The current `CaptureButton` is used only in `(tabs)/index.tsx` for the 4-button grid. After refactoring to 2 inline toggle buttons with different dimensions, `CaptureButton` may be orphaned.
   - What's unclear: Whether any future phase will need the original 4-button grid layout.
   - Recommendation: Delete `CaptureButton` component if it's unused after refactoring. It's trivial to recreate if needed later.

2. **EventSource open event for stage transition**
   - What we know: react-native-sse v1.2.1 fires `message` events. The EventSource constructor accepts callbacks.
   - What's unclear: Whether react-native-sse exposes an `open` event that fires when the SSE connection establishes (to transition from "Uploading..." to "Transcribing..." for voice).
   - Recommendation: Use the first `message` event (typically `STEP_START`) as the transition trigger from "Uploading..." to a classifying stage. Don't rely on `open` event detection -- it would require researching react-native-sse internals. Keep it simple: "Uploading..." -> first SSE event -> "Classifying...". The "Transcribing..." stage can be shown during a brief fixed period after SSE connection opens, or skipped if the transition is too fast to perceive.

3. **AgentSteps component usage**
   - What we know: The `AgentSteps` component renders horizontal step indicators with pill-shaped dots and connector lines. The current processing uses a single "Processing" or "Classifying" step.
   - What's unclear: Whether the granular processing stages should use `AgentSteps` or a simpler text-only display.
   - Recommendation: Replace `AgentSteps` with a simpler inline text display (`ActivityIndicator` + stage text) for the processing feedback. The pill-dot UI was designed for multi-agent chains, which this app no longer uses (Foundry eliminated the Orchestrator). A centered spinner + "Classifying..." text is cleaner for a single-step flow with granular sub-stages.

## Sources

### Primary (HIGH confidence)
- Direct codebase analysis of `(tabs)/index.tsx` (1019 lines), `capture/text.tsx` (732 lines), `_layout.tsx`, `ag-ui-client.ts`, `CaptureButton.tsx`, `AgentSteps.tsx`
- Backend adapter analysis: `streaming/adapter.py`, `streaming/sse.py`, `api/capture.py` -- confirmed SSE event surface and step names
- CONTEXT.md user decisions (gathered 2026-02-28) -- locked implementation decisions

### Secondary (MEDIUM confidence)
- React Native StyleSheet patterns for button sizing and overflow handling -- standard patterns from training data, verified against existing project code

### Tertiary (LOW confidence)
- react-native-sse `open` event availability -- not verified, recommended to avoid relying on it

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - no new libraries, existing dependencies verified from package.json
- Architecture: HIGH - both source and target files thoroughly analyzed, merge points identified
- Pitfalls: HIGH - identified from actual code analysis (permission flows, import gaps, routing cleanup)
- Processing stages: MEDIUM - client-side staging approach is sound, but exact timing behavior for voice upload needs runtime validation

**Research date:** 2026-02-28
**Valid until:** 2026-03-28 (stable -- no external library changes expected)
