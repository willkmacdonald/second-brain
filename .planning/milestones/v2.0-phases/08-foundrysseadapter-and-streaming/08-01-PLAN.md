---
phase: 08-foundrysseadapter-and-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/second_brain/streaming/__init__.py
  - backend/src/second_brain/streaming/adapter.py
  - backend/src/second_brain/streaming/sse.py
  - backend/src/second_brain/api/capture.py
  - backend/src/second_brain/main.py
  - backend/tests/test_streaming_adapter.py
autonomous: true
requirements: [STRM-01, STRM-02, STRM-03]

must_haves:
  truths:
    - "POST /api/capture with text field produces STEP_START, STEP_END, CLASSIFIED, and COMPLETE SSE events"
    - "POST /api/capture with audio file produces STEP_START, STEP_END, CLASSIFIED, and COMPLETE SSE events"
    - "Chain-of-thought reasoning text is suppressed from SSE stream and logged to Application Insights"
    - "ERROR event followed by COMPLETE is emitted on agent timeout or tool failure"
    - "SSE events use new top-level type names (STEP_START, STEP_END, CLASSIFIED, MISUNDERSTOOD, UNRESOLVED, COMPLETE, ERROR)"
  artifacts:
    - path: "backend/src/second_brain/streaming/adapter.py"
      provides: "FoundrySSEAdapter async generator functions for text and voice capture streaming"
      min_lines: 80
    - path: "backend/src/second_brain/streaming/sse.py"
      provides: "SSE encoding helper and AG-UI event constructors"
      min_lines: 20
    - path: "backend/src/second_brain/api/capture.py"
      provides: "POST /api/capture endpoint handling both text and voice multipart uploads"
      min_lines: 40
    - path: "backend/tests/test_streaming_adapter.py"
      provides: "Unit tests for SSE encoding and adapter event mapping"
      min_lines: 30
  key_links:
    - from: "backend/src/second_brain/api/capture.py"
      to: "backend/src/second_brain/streaming/adapter.py"
      via: "import stream_text_capture, stream_voice_capture"
      pattern: "from second_brain\\.streaming\\.adapter import"
    - from: "backend/src/second_brain/api/capture.py"
      to: "app.state.classifier_client"
      via: "request.app.state"
      pattern: "request\\.app\\.state\\.classifier_client"
    - from: "backend/src/second_brain/main.py"
      to: "backend/src/second_brain/api/capture.py"
      via: "include_router"
      pattern: "app\\.include_router\\(capture_router\\)"
---

<objective>
Build the FoundrySSEAdapter streaming module and POST /api/capture endpoint that streams Foundry agent classification results as AG-UI-compatible SSE events for both text and voice captures.

Purpose: This is the bridge between the Foundry-backed Classifier agent (Phase 7) and the mobile Expo app. Without this, no captures flow through the v2 pipeline.

Output: streaming/ module (adapter.py, sse.py), api/capture.py router, main.py wiring, unit tests.
</objective>

<execution_context>
@/Users/willmacdonald/.claude/get-shit-done/workflows/execute-plan.md
@/Users/willmacdonald/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-foundrysseadapter-and-streaming/08-CONTEXT.md
@.planning/phases/08-foundrysseadapter-and-streaming/08-RESEARCH.md
@.planning/phases/07-classifier-agent-baseline/07-02-SUMMARY.md
@backend/src/second_brain/main.py
@backend/src/second_brain/api/inbox.py
@backend/src/second_brain/tools/classification.py
@mobile/lib/ag-ui-client.ts
@mobile/lib/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create streaming module (sse.py + adapter.py) and unit tests</name>
  <files>
    backend/src/second_brain/streaming/__init__.py
    backend/src/second_brain/streaming/sse.py
    backend/src/second_brain/streaming/adapter.py
    backend/tests/test_streaming_adapter.py
  </files>
  <action>
Create the `backend/src/second_brain/streaming/` package with three files:

**`__init__.py`**: Empty init file.

**`sse.py`**: SSE encoding helper and event constructors.
- `encode_sse(data: dict) -> str`: Format a dict as `f"data: {json.dumps(data)}\n\n"`. No `event:` field (react-native-sse defaults to "message" event type).
- Event constructor functions that return dicts (NOT SSE strings -- the adapter calls `encode_sse` on the result):
  - `step_start_event(step_name: str) -> dict` returning `{"type": "STEP_START", "stepName": step_name}`
  - `step_end_event(step_name: str) -> dict` returning `{"type": "STEP_END", "stepName": step_name}`
  - `classified_event(inbox_item_id: str, bucket: str, confidence: float) -> dict` returning `{"type": "CLASSIFIED", "value": {"inboxItemId": ..., "bucket": ..., "confidence": ...}}`
  - `misunderstood_event(thread_id: str, inbox_item_id: str, question_text: str) -> dict` returning `{"type": "MISUNDERSTOOD", "value": {"threadId": ..., "inboxItemId": ..., "questionText": ...}}`
  - `unresolved_event(inbox_item_id: str) -> dict` returning `{"type": "UNRESOLVED", "value": {"inboxItemId": ...}}`
  - `complete_event(thread_id: str, run_id: str) -> dict` returning `{"type": "COMPLETE", "threadId": ..., "runId": ...}`
  - `error_event(message: str) -> dict` returning `{"type": "ERROR", "message": message}`

Per CONTEXT decisions: event names are STEP_START/STEP_END (not STEP_STARTED/STEP_FINISHED), CLASSIFIED/MISUNDERSTOOD/UNRESOLVED are top-level types (not wrapped in CUSTOM), COMPLETE (not RUN_FINISHED), ERROR for failures. No TEXT_MESSAGE_CONTENT event.

**`adapter.py`**: Async generator functions (NOT a class -- per CONTEXT discretion and RESEARCH recommendation).
- `stream_text_capture(client: AzureAIAgentClient, user_text: str, tools: list, thread_id: str, run_id: str) -> AsyncGenerator[str, None]`:
  1. Create `messages = [Message(role="user", text=user_text)]` and `options: ChatOptions = {"tools": tools}`
  2. Yield `encode_sse(step_start_event("Classifying"))` -- step name is "Classifying" per CONTEXT decisions
  3. Initialize tracking: `detected_tool: str | None = None`, `detected_tool_args: dict = {}`, `tool_result: dict | None = None`, `reasoning_buffer: str = ""`
  4. Call `stream = client.get_response(messages=messages, stream=True, options=options)`
  5. `async for update in stream:` iterate `update.contents or []`:
     - `content.type == "text"`: Buffer reasoning text. Log each chunk at INFO to `logging.getLogger("second_brain.streaming.reasoning")` with `extra={"reasoning_text": content.text, "agent_run_id": run_id, "chunk_index": chunk_idx}` where chunk_idx increments. Do NOT yield to SSE.
     - `content.type == "function_call"` and `content.name == "file_capture"`: Set `detected_tool = "file_capture"`. Parse `content.arguments` defensively -- if str, `json.loads()`; if dict/Mapping, `dict(args)`. Store in `detected_tool_args`.
     - `content.type == "function_result"`: Parse `content.result` defensively (same str/dict handling). Store in `tool_result`.
  6. Yield `encode_sse(step_end_event("Classifying"))`
  7. Emit result event based on `detected_tool_args.get("status")`:
     - `"classified"` or `"pending"`: Yield `encode_sse(classified_event(item_id, bucket, confidence))` where values come from `tool_result` (preferred) falling back to `detected_tool_args`
     - `"misunderstood"`: Yield `encode_sse(misunderstood_event(thread_id, item_id, question_text))` where question_text is `detected_tool_args.get("title", "Could you clarify?")`
     - Otherwise (no tool call detected): Yield `encode_sse(unresolved_event(""))` as fallback
  8. Yield `encode_sse(complete_event(thread_id, run_id))`
  9. Wrap the entire streaming section (steps 4-8) in `try/except` with `asyncio.timeout(60)`. On `TimeoutError` or `Exception`: yield `encode_sse(error_event(str(error)))` then `encode_sse(complete_event(thread_id, run_id))`.

- `stream_voice_capture(client: AzureAIAgentClient, blob_url: str, tools: list, thread_id: str, run_id: str) -> AsyncGenerator[str, None]`:
  Per CONTEXT decisions: voice captures use a SINGLE step with name "Processing" (no synthetic transcription step).
  1. Create `messages = [Message(role="user", text=f"Transcribe and classify this voice recording: {blob_url}")]`
  2. Yield `encode_sse(step_start_event("Processing"))` -- one step bracket for the whole run
  3. Same streaming loop as text capture (detect function_call for file_capture, parse results)
  4. Also detect `content.type == "function_call"` with `content.name == "transcribe_audio"` -- log it but do NOT emit a separate step event (per CONTEXT: single step for voice)
  5. After stream: yield `encode_sse(step_end_event("Processing"))`
  6. Same result event emission logic as text capture
  7. Same error handling with 60-second timeout

**`test_streaming_adapter.py`**: Unit tests.
- Test `encode_sse` produces correct `data: {json}\n\n` format
- Test each event constructor returns the expected dict structure
- Test that event type names match the new contract (STEP_START not STEP_STARTED, COMPLETE not RUN_FINISHED, etc.)
- Do NOT test the async generator functions with mocked Foundry clients (that is integration test territory -- Phase 8 is validated by mobile E2E)
  </action>
  <verify>
Run: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/test_streaming_adapter.py -v` -- all tests pass.
Run: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff check src/second_brain/streaming/ tests/test_streaming_adapter.py && ruff format --check src/second_brain/streaming/ tests/test_streaming_adapter.py` -- no lint or format errors.
Run: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.streaming.adapter import stream_text_capture, stream_voice_capture; from second_brain.streaming.sse import encode_sse; print('imports ok')"` -- prints "imports ok".
  </verify>
  <done>
streaming/ package exists with sse.py (encode_sse + 7 event constructors) and adapter.py (stream_text_capture + stream_voice_capture async generators). All unit tests pass. Event names match new contract: STEP_START, STEP_END, CLASSIFIED, MISUNDERSTOOD, UNRESOLVED, COMPLETE, ERROR.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create POST /api/capture endpoint and wire into main.py</name>
  <files>
    backend/src/second_brain/api/capture.py
    backend/src/second_brain/main.py
  </files>
  <action>
**`capture.py`**: New FastAPI router module for the unified capture endpoint.

Per CONTEXT decisions: POST /api/capture is a single endpoint for both text and voice. Request body indicates type (text field or audio file). Voice uses direct multipart upload.

```python
from fastapi import APIRouter, File, Form, Request, UploadFile
from fastapi.responses import StreamingResponse
```

- `router = APIRouter(tags=["Capture"])`
- Pydantic model `TextCaptureBody` with fields: `text: str`, `thread_id: str | None = None`, `run_id: str | None = None`

**Text capture path** -- `POST /api/capture` with JSON body:
```python
@router.post("/api/capture")
async def capture(request: Request, body: TextCaptureBody) -> StreamingResponse:
```
1. Get `client = request.app.state.classifier_client`, `tools = request.app.state.classifier_agent_tools`
2. Generate `thread_id = body.thread_id or f"thread-{uuid4()}"`, `run_id = body.run_id or f"run-{uuid4()}"`
3. Call `stream_text_capture(client, body.text, tools, thread_id, run_id)`
4. Return `StreamingResponse(generator, media_type="text/event-stream", headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"})`

**Voice capture path** -- `POST /api/capture/voice` with multipart form:
```python
@router.post("/api/capture/voice")
async def capture_voice(request: Request, file: UploadFile = File(...)) -> StreamingResponse:
```
1. Get `blob_manager = request.app.state.blob_manager`, check it exists (503 if not)
2. Upload audio to blob storage: read file bytes, generate blob name `f"voice-captures/{uuid4()}.m4a"`, upload via `blob_manager`
3. Generate thread_id and run_id
4. Create the voice capture generator, but wrap it to handle blob cleanup:
   ```python
   async def stream_with_cleanup():
       try:
           async for event in stream_voice_capture(client, blob_url, tools, thread_id, run_id):
               yield event
       finally:
           # Delete blob after stream completes (per RESEARCH recommendation)
           try:
               await blob_manager.delete_blob(blob_name)
           except Exception:
               logger.warning("Failed to delete voice blob: %s", blob_name)
   ```
5. Return `StreamingResponse(stream_with_cleanup(), media_type="text/event-stream", headers={...})`

IMPORTANT: The endpoint URL is /api/capture (text) and /api/capture/voice (voice). Per CONTEXT, both are in the same capture.py router. The mobile currently sends text to /api/ag-ui and voice to /api/voice-capture -- Plan 02 will update the mobile URLs.

**`main.py`** changes:
1. Add import: `from second_brain.api.capture import router as capture_router`
2. Add: `app.include_router(capture_router)` alongside the existing health_router and inbox_router includes.
No other changes to main.py.

Check if BlobStorageManager has a `delete_blob` method. If not, check for an equivalent. If BlobStorageManager only has `upload_blob` and no delete, add a simple `delete_blob` method that calls `blob_container_client.delete_blob(blob_name)`. Look at the existing blob_storage.py to understand the interface.
  </action>
  <verify>
Run: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.main import app; routes = [r.path for r in app.routes]; print(routes); assert '/api/capture' in routes; assert '/api/capture/voice' in routes; print('routes ok')"` -- prints routes including /api/capture and /api/capture/voice.
Run: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff check src/second_brain/api/capture.py src/second_brain/main.py && ruff format --check src/second_brain/api/capture.py src/second_brain/main.py` -- no lint or format errors.
Run: `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v --ignore=tests/test_classifier_integration.py` -- all existing tests still pass plus new streaming tests.
  </verify>
  <done>
POST /api/capture (text) and POST /api/capture/voice (voice) endpoints are wired into the FastAPI app. Text capture streams through stream_text_capture, voice capture uploads to blob storage then streams through stream_voice_capture with blob cleanup. main.py includes the capture router. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -m pytest tests/ -v --ignore=tests/test_classifier_integration.py` -- all tests pass
2. `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && ruff check . && ruff format --check .` -- no lint/format issues
3. `cd /Users/willmacdonald/Documents/Code/claude/second-brain/backend && python3 -c "from second_brain.main import app; print('app starts')"` -- app imports cleanly
4. Verify new event type names in test assertions: STEP_START, STEP_END, CLASSIFIED, MISUNDERSTOOD, UNRESOLVED, COMPLETE, ERROR
5. Verify no TEXT_MESSAGE_CONTENT event is emitted anywhere in adapter.py
6. Verify no CUSTOM wrapper event is used anywhere (events are top-level)
</verification>

<success_criteria>
- streaming/ package with adapter.py and sse.py exists and imports cleanly
- POST /api/capture endpoint accepts text JSON body and returns SSE StreamingResponse
- POST /api/capture/voice endpoint accepts multipart audio file and returns SSE StreamingResponse
- SSE events use new type names (STEP_START, STEP_END, CLASSIFIED, MISUNDERSTOOD, UNRESOLVED, COMPLETE, ERROR)
- Chain-of-thought text is suppressed from SSE stream
- 60-second timeout with ERROR + COMPLETE events on failure
- All unit tests pass
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-foundrysseadapter-and-streaming/08-01-SUMMARY.md`
</output>
